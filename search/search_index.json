{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This is the corner of my personal life where I share my passions. we have:</p> <ul> <li>Software Engineering Blog<ul> <li>Developer Experience</li> <li>DevOps</li> <li>Spring Boot</li> <li>System Design</li> </ul> </li> </ul>"},{"location":"#about-me","title":"About Me","text":"<p>I'm a Java Server-Side-Engineer\ud83d\ude80, focusing on Java, Spring Boot, System Design (Kafka, Event Sourcing, etc.), DevOps (CI/CD, Toggle System, etc.), and Developer Experience (Backstage). When I'm not coding, you\u2019ll find me playing my guitar or piano and singing Japanese songs.</p>"},{"location":"#technical-spotlight","title":"Technical Spotlight","text":"<ul> <li>How to design an efficient Idempotent API</li> <li>Easier, Flexible, and Lower Resource Cost Deployment Strategies by Feature Toggle</li> <li>Safely Deliver Large Scale Feature Migration With Feature Toggle</li> </ul>"},{"location":"#supporting","title":"Supporting","text":"<p>If you find my articles are valuable, consider supporting my work. You can show your appreciation by buying me a beer \ud83c\udf7a.</p> <p> </p> <p>Your generosity fuels my passion for creating more content and sharing with you.</p> <p>Thanks for your support, and I hope you enjoy exploring everything this site has to offer!</p>"},{"location":"Software%20Engineering%20Blog/","title":"Software Engineering Blog","text":""},{"location":"Software%20Engineering%20Blog/2024/02/15/3-times-performance-improvement-for-generative-ai-within-a-kafka-pipeline-system/","title":"3 Times Performance Improvement for Generative AI within a Kafka Pipeline System","text":"<p>Generative AI went viral in the last year, many use cases show the ability of generative AI, such as text, image, video, music generation, and more. It helps people to create more content with less effort. However, unlike conventional APIs, the Gen-AI API often has a relatively longer latency and higher costs due to the need for better (more expensive) GPU resources to enhance performance.</p> <p>To ensure a better user experience, it's common to build an asynchronous system using the Kafka pipeline and bot messaging mechanism between the client device (e.g. mobile apps, web apps) and the generative API. This design helps prevent system crashes during peak request times and prevents client timeouts while waiting for responses. However, building such a system requires careful attention to various details, which we will cover in this article. Here is the outline:</p> <ol> <li>Asynchronous system integrating generative AI</li> <li>Problems related to high-volume requests<ul> <li>The producers unevenly distributed events among Kafka partitions</li> <li>The consumers frequently experienced unnecessary rebalancing and event double-consumption</li> </ul> </li> <li>How to solve the problems</li> </ol>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2024/02/15/3-times-performance-improvement-for-generative-ai-within-a-kafka-pipeline-system/#asynchronous-system-integrating-generative-ai","title":"Asynchronous System Integrating Generative AI","text":"<p>Let's say we are building a system that provides an AI model for generating slides according to users' original images, style selection, and extra descriptions.</p> <p>We can draw a system architecture like this:</p> <p></p> <p>Through the architecture above, we can visualize the entire user journey starting from uploading images, selecting a style, and receiving a message containing the final result, potentially in the form of a URL link to the slide.</p> <p>For each request, the Checking AI requires 0.5 seconds for processing, while the Generative AI needs 3 seconds. Both can concurrently handle up to 64 requests at maximum capacity.</p> <p>Several design considerations are worth mentioning:</p> <ol> <li>We utilize the object storage service's published API to have users directly upload to them instead of an Application built on our own.     =&gt; Let the professionals do it, and they often provide the notification mechanism, we only need to build an API to receive the <code>objectID</code> and other request parameters such as style and description.</li> <li>All APIs (except upload API) won't bring the image in the request body.    =&gt; This optimization reduces network traffic.</li> <li>The obvious bottlenecks are the Generative AI and the Checking AI, so we place them behind a Kafka queue.    =&gt; This design ensures that the number of concurrent requests won't cause crashes. Additionally, users receive a response such as 'request received, please wait' before the result is generated.</li> </ol>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2024/02/15/3-times-performance-improvement-for-generative-ai-within-a-kafka-pipeline-system/#problems-related-to-high-volume-of-requests","title":"Problems related to High-Volume of Requests","text":"<p>At first, we constructed a system based on the above architecture, primarily using default settings for both Kafka producer and consumer. The feature test results were satisfactory; however, during the load test, we encountered two significant issues that severely impacted the system's requests per second (RPS) performance:</p> <ol> <li>The producers unevenly distributed events among Kafka partitions.</li> <li>The consumers frequently experienced unnecessary rebalancing and event double-consumption.</li> </ol>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2024/02/15/3-times-performance-improvement-for-generative-ai-within-a-kafka-pipeline-system/#1-the-producers-unevenly-distributed-events-among-kafka-partitions","title":"1. The Producers Unevenly Distributed Events among Kafka Partitions","text":"<p>This situation is related to the producer's Kafka client partitioner, have a glance at its class document below first:</p> <p>The default partitioning strategy: <ul> <li>If a partition is specified in the record, use it <li>If no partition is specified but a key is present choose a partition based on a hash of the key <li>If no partition or key is present choose the sticky partition that changes when the batch is full. See KIP-480 for details about sticky partitioning. <p>When the batch-send mechanism is enabled using <code>batch.size</code> and <code>linger.ms</code>, and no key is provided for each event, the default partitioner will assign all the events in the same batch to a single partition (For more details, please refer to KIP-480 and sticky partitioning).</p> <p></p> <p>This issue becomes particularly prominent during peak requests, as numerous requests arrive within a short period. When the message size is too small to reach the batch size, all events within the linger time become jammed in the same partition</p> <p>This behavior causes a serious uneven workload problem among consumers. We control the number of consumers based on the maximum acceptable concurrent requests of the AI API. Therefore, we cannot afford for some consumers to remain inactive during peak times, as this worsens the bottleneck in the AI API.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2024/02/15/3-times-performance-improvement-for-generative-ai-within-a-kafka-pipeline-system/#2-the-consumers-frequently-experienced-unnecessary-rebalancing-and-event-double-consumption","title":"2. The Consumers Frequently Experienced Unnecessary Rebalancing and Event Double-consumption","text":"<p>This situation is primarily related to the long processing time of the Generative AI API and also involves the heartbeat, polling, and rebalancing mechanism of Kafka consumers. Let's begin by understanding the definitions of heartbeat and rebalance:</p> <p>Heartbeats are sent when the consumer polls (i.e., retrieves records) and when it commits records it has consumed.</p> <p>If the consumer stops sending heartbeats for long enough, its session will time out and the group coordinator will consider it dead and trigger a rebalance.</p> <p>(After the KIP-62, they decouple the heartbeat from the <code>poll</code> and <code>commit offset</code>, the client will keep sending heartbeats before exceeding the <code>max.poll.interval.ms</code>.)</p> <p>In our scenario, with the default setting of <code>session.timeout.ms</code>, <code>max.poll.interval.ms</code>, <code>max.poll.records</code>, and consumer <code>ack-mode</code> set to <code>Manual</code> (or <code>Batch</code> in default). The flow related to them will look like below:</p> <p></p> <p>After the session timeout, the entire consumer group will start to rebalance (for more details, refer to this article), causing consumption to pause temporarily (for a few seconds or even minutes). Moreover, the offsets of handled events cannot be committed to the broker, leading to the re-consumption of these events after rebalancing.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2024/02/15/3-times-performance-improvement-for-generative-ai-within-a-kafka-pipeline-system/#how-to-solve-the-problems","title":"How to solve the problems","text":"<p>While the issues mentioned above may not directly lead to serious business logic problems (an idempotent consumer can also prevent errors caused by double-consuming events), they do slow down the entire pipeline. This slowdown poses a significant challenge when our goal is to deliver more value and provide a better experience to our users.</p> <p>In the following sections, we'll explore the options available to address these issues.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2024/02/15/3-times-performance-improvement-for-generative-ai-within-a-kafka-pipeline-system/#kafka-producer-tuning","title":"Kafka Producer Tuning","text":"<p>We aim to solve the issue of uneven distribution of events to Kafka partitions by adjusting the producer settings. After analyzing the problem, we have identified two potential solutions:</p> <ol> <li>Reducing the <code>linger.ms</code> and <code>batch.size</code>: This makes the producer reduce the number of events sent to a single partition in one batch.</li> <li>Assigning a unique event key to each event: This approach allows the partitioner to distribute events across partitions based on the hash of the key.</li> </ol> <p>While option 1 may result in fewer events per batch and increased network traffic, it's important to note that the sticky partitioning strategy still may push events unevenly since it only ensures that the new batch is not sent to the same partition as the previous one. </p> <p>Therefore, we opt for option 2\u2014assigning every event an event key. This ensures that events are aggregated in batches for partitions with the same leader broker.</p> <p></p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2024/02/15/3-times-performance-improvement-for-generative-ai-within-a-kafka-pipeline-system/#kafka-consumer-tuning","title":"Kafka Consumer Tuning","text":"<p>We aim to resolve the issue of consumers frequently rebalancing and double-consuming events by adjusting consumer settings. Upon analyzing the problem, we have listed several solutions to consider:</p> <ol> <li>Increase the Heartbeat: We can set the consumer <code>ack-mode</code> to <code>manual-immediate</code>, <code>count</code>, or <code>time</code> to commit offsets more frequently for processed events.</li> <li>Limit the Number of Records in Each Poll: We can set a small number for the <code>max.poll.records</code> to ensure that we can process all records within the <code>max.poll.interval.ms</code>.</li> <li>Extend Timeout Tolerance: We can set a high number for both <code>max.poll.interval.ms</code> and <code>session.timeout.ms</code> to allow sufficient time for processing all records from one poll.</li> </ol> <p>First, we can remove the option 3. This is a risky adjustment since it can not make sure whether the consumer is actively processing events or is stuck somehow. A longer timeout tolerance could delay recovery when failure did happen.</p> <p>When it comes to options 1 and 2, I would say they are both acceptable. They both increase the network traffic between the consumer and the broker.</p> <p>Option 1 involves fetching a large number of records at once and committing offsets frequently. While this may help to slightly lower network traffic, it requires more memory in each consumer, depending on the parameter we set.</p> <p>On the other hand, option 2 involves fetching a smaller number of records per poll and committing offsets after processing them all. This approach maintains relatively stable network traffic and memory requirements. Therefore, I favor this solution.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2024/02/15/3-times-performance-improvement-for-generative-ai-within-a-kafka-pipeline-system/#summary","title":"Summary","text":"<p>In this article, we propose a pipeline system architecture, which integrates some high-latency Generative AI APIs using Kafka to enhance user experience and stability. Moreover, we bring out the critical point of how to improve the performance of the entire pipeline.</p> <p>The article focuses on two critical challenges: achieving even event distribution across partitions and preventing unnecessary rebalancing and event double consumption. To address the former issue, we ensure that every event has its own key, such as a UUID. For the latter problem, we adjust the <code>max.poll.records</code>, parameter to a small value, ensuring that records can be processed within the <code>max.poll.interval.ms</code>. These adjustments result in a significant improvement in pipeline efficiency, estimated to be around 3 times.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2024/02/15/3-times-performance-improvement-for-generative-ai-within-a-kafka-pipeline-system/#reference","title":"Reference","text":"<p>Producer:</p> <ul> <li>DefaultPartitioner.java, from Apache, in GitHub</li> <li>KIP-480 Sticky Partitioning</li> <li>StickyPartitionCache.java, from Apache, in GitHub</li> </ul> <p>Consumer:</p> <ul> <li>Chapter 4. Kafka Consumers: Reading Data from Kafka, in O'REILLY</li> <li>difference between max.poll.interval.ms and session.timeout.ms for Kafka, in stackoverflow.com</li> <li>Understanding Kafka\u2019s Consumer Group Rebalancing, By Verica, in www.verica.io</li> </ul>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2024/07/12/5-steps-to-make-gradle-configuration-extreme-clean-in-a-multi-module-project/","title":"5 Steps to Make Gradle Configuration Extreme Clean in a Multi-Module Project","text":"<p>Multi-module Gradle projects involve numerous tasks during the build process. Managing dependency version control, plugin usage, build logic, and more with Gradle proves to be a popular and effective approach. But, achieving these tasks requires a lot of configuration scripts, which can make the file more complicated, and more difficult for development. These steps in the article will guide you through a clean and efficient way to manage configuration files:</p> <ol> <li>extract version declaring in <code>gradle.properties</code>.</li> <li>define all plugins and repositories in <code>settings.gradle</code>.</li> <li>define all libraries in the <code>allprojects.dependencyManagement</code> in <code>./build.gradle</code>.</li> <li>declaring dependency and plugin directly instead of using <code>subproject</code> in submodule.</li> <li>extract complex and common task config to extra files and apply wherever needed.</li> </ol> <p>Take a look at this repository or refactor PR, if you can't wait to find out how it looks.</p>","tags":["Gradle","Java","Refactoring"]},{"location":"Software%20Engineering%20Blog/2024/07/12/5-steps-to-make-gradle-configuration-extreme-clean-in-a-multi-module-project/#step-by-step-demonstration","title":"Step by Step Demonstration","text":"","tags":["Gradle","Java","Refactoring"]},{"location":"Software%20Engineering%20Blog/2024/07/12/5-steps-to-make-gradle-configuration-extreme-clean-in-a-multi-module-project/#step-1-extract-version-declaration","title":"Step 1: Extract Version Declaration","text":"<p>Version declarations can be extracted into a <code>gradle.properties</code> file. Additionally, Gradle arguments can be defined as shown below:</p> ./gradle.properties<pre><code>group='org.example'\nversion=0.0.1.SNAPSHOT\n\n# Plugin Version\njibVersion=3.4.3\n\n# Spring Version\nspringBootVersion=3.1.5\nspringDependencyVersion=1.1.4\nspringCloudVersion=2022.0.1\n\n# Dependency Version\nspringdocVersion=2.1.0\nfeignMicrometerVersion=12.1\nwiremockVersion=3.7.0\nlogbackAppenderVersion=1.4.0-rc2\nlombokVersion=1.18.20\n\n# Gradle Argument\norg.gradle.parallel=true\n</code></pre>","tags":["Gradle","Java","Refactoring"]},{"location":"Software%20Engineering%20Blog/2024/07/12/5-steps-to-make-gradle-configuration-extreme-clean-in-a-multi-module-project/#step-2-define-used-plugins-and-maven-source","title":"Step 2: Define Used Plugins and Maven Source","text":"<p>All used plugins and the source Maven repository can be defined in a <code>settings.gradle</code>:</p> ./settings.gradle<pre><code>import org.gradle.api.initialization.resolve.RepositoriesMode\n\npluginManagement {\n    plugins {\n        id 'org.springframework.boot' version \"${springBootVersion}\"\n        id 'io.spring.dependency-management' version \"${springDependencyVersion}\"\n        id 'com.google.cloud.tools.jib' version \"${jibVersion}\"\n    }\n}\n\ndependencyResolutionManagement {\n    repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS)\n    repositories {\n        mavenCentral()\n        mavenLocal()\n        maven {\n            url '.m2/local/'\n        }\n    }\n}\n\nrootProject.name = 'event-sourcing-order-poc'\n\ninclude 'modules'\ninclude 'modules:common'\nfindProject(':modules:common')?.name = 'common'\n// ... and other modules settings\n\ninclude 'order'\ninclude 'order:command-side'\nfindProject(':order:command-side')?.name = 'order-command-side'\ninclude 'order:event-handler'\nfindProject(':order:event-handler')?.name = 'order-event-handler'\ninclude 'order:query-side'\nfindProject(':order:query-side')?.name = 'order-query-side'\n// ... and other sub-project settings\n</code></pre>","tags":["Gradle","Java","Refactoring"]},{"location":"Software%20Engineering%20Blog/2024/07/12/5-steps-to-make-gradle-configuration-extreme-clean-in-a-multi-module-project/#step-3-define-allprojects-dependencymanagement","title":"Step 3: Define Allprojects DependencyManagement","text":"<p>All the used libraries should be defined in a <code>allprojects.dependencyManagement</code> closure in <code>build.gradle</code> of the root module:</p> ./build.gradle<pre><code>import org.springframework.boot.gradle.plugin.SpringBootPlugin\n\nplugins {\n    id 'java'\n    id 'java-library'\n    id 'io.spring.dependency-management'\n    id 'org.springframework.boot' apply false\n    id 'com.google.cloud.tools.jib' apply false\n}\n\nallprojects {\n\n    java {\n        sourceCompatibility = JavaVersion.VERSION_17\n        targetCompatibility = JavaVersion.VERSION_17\n    }\n\n    apply plugin: 'java'\n    apply plugin: 'io.spring.dependency-management'\n    apply plugin: 'java-library'\n\n    dependencyManagement {\n        imports {\n            mavenBom SpringBootPlugin.BOM_COORDINATES\n            mavenBom \"org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\"\n        }\n        dependencies {\n            dependency \"org.springdoc:springdoc-openapi-starter-webmvc-ui:${springdocVersion}\"\n            dependency \"io.github.openfeign:feign-micrometer:${feignMicrometerVersion}\"\n            dependency \"org.projectlombok:lombok:${lombokVersion}\"\n            dependency \"org.wiremock:wiremock:${wiremockVersion}\"\n            dependency \"com.github.loki4j:loki-logback-appender:${logbackAppenderVersion}\"\n        }\n    }\n\n    dependencies {\n        // only declare all-needed dependencies\n        compileOnly \"org.projectlombok:lombok:${lombokVersion}\"\n        annotationProcessor \"org.projectlombok:lombok:${lombokVersion}\"\n    }\n\n    test {\n        useJUnitPlatform()\n    }\n\n}\n\ntasks.named(\"jar\") {\n    enabled = false\n}\n</code></pre> <p>in the <code>dependencyManagement</code> closure, we can first import the BOM of other dependencies project like spring-boot-dependencies and spring-cloud-dependencies. Then, we can declare the version of other used libraries.</p>","tags":["Gradle","Java","Refactoring"]},{"location":"Software%20Engineering%20Blog/2024/07/12/5-steps-to-make-gradle-configuration-extreme-clean-in-a-multi-module-project/#step4-avoid-using-subprojects","title":"Step4: Avoid Using <code>subprojects {}</code>","text":"<p>Declaring dependency and plugin directly instead of using <code>subproject</code> in <code>build.gradle</code> for sub-modules like:</p> ./order/command-side/build.gradle<pre><code>plugins {\n    id 'org.springframework.boot'\n    id 'com.google.cloud.tools.jib'\n}\n\napply from: \"$rootDir/gradle/jib.gradle\"\n\ndependencies {\n    implementation project(\":modules:common\")\n    implementation project(\":modules:event\")\n    implementation project(\":modules:client\")\n    implementation project(\":modules:observation\")\n    implementation project(\":modules:idempotency\")\n\n    implementation 'org.springframework.boot:spring-boot-starter-web'\n    implementation 'org.springframework.boot:spring-boot-starter'\n    implementation 'org.springframework.boot:spring-boot-starter-actuator'\n    implementation 'org.springframework.kafka:spring-kafka'\n    implementation 'org.springdoc:springdoc-openapi-starter-webmvc-ui'\n\n    testImplementation 'org.springframework.boot:spring-boot-starter-test'\n    testImplementation 'org.junit.jupiter:junit-jupiter-api'\n    testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine'\n}\n</code></pre> <p>It can be more intuitive to declare the used plugin and dependencies in each project. Thanks to the <code>dependencyManagement</code> in the root module, we can use a simple form of the declaration here in the subproject.</p>","tags":["Gradle","Java","Refactoring"]},{"location":"Software%20Engineering%20Blog/2024/07/12/5-steps-to-make-gradle-configuration-extreme-clean-in-a-multi-module-project/#step-5-extract-related-configuration","title":"Step 5: Extract Related Configuration","text":"<p>Extract complex and common task config to extra files and apply them wherever needed.</p> <p>In the above file <code>./order/command-side/build.gradle</code>, the important script snippet </p> <pre><code>...\napply from: \"$rootDir/gradle/jib.gradle\"\n...\n</code></pre> <p>will include an extra <code>.gradle</code> file, which we can group related config into one file. Let's take the <code>./gradle/jib.gradle</code> for example:</p> ./gradle/jib.gradle<pre><code>jib {\n    from {\n        image = \"openjdk:17-slim\"\n    }\n\n    to.image = \"noahhsu/${project.name}\"\n    to.tags = [\"latest\"]\n\n    container {\n        creationTime = 'USE_CURRENT_TIMESTAMP'\n    }\n\n}\n</code></pre> <p>In this way, we can make the <code>.gradle</code> file in the submodules/subprojects is very clean and more readable. Moreover, we can reuse these configurations in different places (e.g. <code>order/query-side</code>, <code>payment/command-side</code>, etc.).</p>","tags":["Gradle","Java","Refactoring"]},{"location":"Software%20Engineering%20Blog/2024/07/12/5-steps-to-make-gradle-configuration-extreme-clean-in-a-multi-module-project/#summary","title":"Summary","text":"<p>In conclusion, managing a multi-module Gradle project can be streamlined and elegant by adopting a structured approach to configuration. In this article, we propose a five-step method to centralize plugin and dependency version declarations and extract configurations into independent .gradle files. Besides, be cautious when using special methods to ensure the project-building logic straightforward and easy to manage. By following these steps, you can enhance the readability and maintainability of your multi-module Gradle projects.</p>","tags":["Gradle","Java","Refactoring"]},{"location":"Software%20Engineering%20Blog/2024/07/12/5-steps-to-make-gradle-configuration-extreme-clean-in-a-multi-module-project/#reference","title":"Reference","text":"<ul> <li>Why Avoid <code>subprojects {}</code></li> <li>Pull Request</li> </ul>","tags":["Gradle","Java","Refactoring"]},{"location":"Software%20Engineering%20Blog/2023/07/31/centralize-all-needed-knowledge-in-one-developer-portals-through-spotify-backstage/","title":"Centralize All Needed Knowledge in One Developer Portals Through Spotify Backstage","text":"<p>A developer portal is designed to enhance the developer experience by uniting all the essential knowledge required for development, maintenance, monitoring, and more, into a single platform. Backstage fulfills this objective through its core features, which include:</p> <ul> <li>Software catalog: This feature allows users to define relationships between systems, components, and APIs, while also providing API definitions.</li> <li>Kubernetes: Backstage enables developers to check the health of target services within the Kubernetes cluster.</li> <li>Software template: Backstage offers a variety of templates that empower developers to initiate new projects swiftly, incorporating all necessary implementations such as CI/CD templates, company policies, and guidelines.</li> <li>TechDoc and Searching: Backstage integrates all relevant markdown documents, effectively centralizing them and eliminating scattering across GitHub README, company Wiki, company blog, etc.</li> </ul> <p>Doesn\u2019t the prospect of such a portal sound promising and exciting for developers? In this article, we will demonstrate how to integrate a Spring Boot application into Backstage using my personal GitHub repository as an example. Here is the related PR and Repository:</p> <p>project to onboard backstage: My PR on project My Custom Backstage App: main branch</p>","tags":["Backstage","Developer Portal"]},{"location":"Software%20Engineering%20Blog/2023/07/31/centralize-all-needed-knowledge-in-one-developer-portals-through-spotify-backstage/#outline","title":"Outline","text":"<ul> <li>prerequisites</li> <li>Add Software Catalog (System, Component, API)</li> <li>Add TechDoc</li> <li>Summary</li> </ul> <p>Note: Kubernetes and Software template is not included in this article since integrating Kubernetes is not so convenient for me to run on my laptop, and the latter is way more complex, I might write another article to focus on it.</p>","tags":["Backstage","Developer Portal"]},{"location":"Software%20Engineering%20Blog/2023/07/31/centralize-all-needed-knowledge-in-one-developer-portals-through-spotify-backstage/#prerequisites","title":"Prerequisites","text":"<ul> <li>A local backstage App Running: please refer to the Get Started and Configure backstage in backstage.io. (I use node-18.16.1 in my case)</li> <li>Other repositories/ projects to onboard in Backstage: in my case is my event-sourcing-order-poc project.</li> </ul> <p>Following the tutorial in the backstage.io, we should able to run the backstage app by running <code>yarn dev</code> and login with a GitHub account(we can run <code>yarn install</code> after you install some plugins and face an error when running <code>yarn dev</code>).</p> <p></p> <p>Then, is time to add an existing project into your backstage App.</p>","tags":["Backstage","Developer Portal"]},{"location":"Software%20Engineering%20Blog/2023/07/31/centralize-all-needed-knowledge-in-one-developer-portals-through-spotify-backstage/#add-software-catalog","title":"Add Software Catalog","text":"<p>First, we have to add the <code>.yaml</code> file to our project. Since my project is kind of a mono-repo (contains multiple components, and APIs), I refer to the file structure for the backstage demo site. Here is a quick view of the whole config file in my project for onboarding Backstage:</p> <p></p> <p>In this structure, we can register all the components by only importing the <code>all.yaml</code> into backstage app. Here are three ways to do so depending on where is the <code>all.yaml</code>:</p> <ul> <li>On the GitHub repository</li> <li>import the <code>all.yaml</code> file URL through UI, refer to the tutorial.</li> <li> <p>modify the <code>catalog.locations</code> part in <code>app-config.yaml</code> for the backstage project and restart the App, the config is like the below:</p> <p>app-config.yaml<pre><code>...\ncatalog:\n  import:\n    ...\n  rules:\n    ...\n  locations:\n    - type: url\n      target: https://github.com/NoahHsu/event-sourcing-order-poc/blob/master/backstage/all.yaml\n...\n</code></pre> - In the local file system   3. you can only modify the <code>catalog.locations</code> part in <code>app-config.yaml</code> for the backstage project and restart the App, the config is like the one below ( in my case, the projects of <code>event-sourcing-order-poc</code> and <code>backstage</code> are in the same folder):</p> app-config.yaml<pre><code>...\ncatalog:\n  import:\n    ...\n  rules:\n    ...\n  locations:\n    - type: file\n      target: ../../../event-sourcing-order-poc/backstage/all.yaml\n      # Local example data, file locations are relative to the backend process, typically `packages/backend`\n...\n</code></pre> </li> </ul>","tags":["Backstage","Developer Portal"]},{"location":"Software%20Engineering%20Blog/2023/07/31/centralize-all-needed-knowledge-in-one-developer-portals-through-spotify-backstage/#config-component","title":"Config Component","text":"<p>Config in each component\u2019s file is similar, here provide the <code>order-command-side-component.yaml</code> as an example and comments on some notable settings.</p> order-command-side-component.yaml<pre><code>apiVersion: backstage.io/v1alpha1\nkind: Component\nmetadata:\n  name: order-command\n  description: command side of order aggregate\n  annotations:\n    # control the link of \"View Source\" button in \"About\" block\n    backstage.io/source-location: url:https://github.com/NoahHsu/event-sourcing-order-poc\n  links: \n    # each link will be list in Links block\n    - url: https://github.com/NoahHsu/event-sourcing-order-poc/tree/master/order/command-side\n      title: Server Root\n      # value refer to https://github.com/backstage/backstage/blob/master/packages/app-defaults/src/defaults/icons.tsx#L39\n      icon: github\nspec:\n  type: service\n  lifecycle: experimental\n  # refer to the name in System .yaml, affect the relations-graph.\n  system: event-sourcing-poc\n  owner: guest\n  providesApis:\n    # refer to the name in API .yaml, affect the relations-graph.\n    - order-command\n</code></pre> <p>The page ends up like the below image:</p> <p></p>","tags":["Backstage","Developer Portal"]},{"location":"Software%20Engineering%20Blog/2023/07/31/centralize-all-needed-knowledge-in-one-developer-portals-through-spotify-backstage/#config-api-doc","title":"Config API Doc","text":"<p>For the API Doc, all the points are to provide the API definition (plain text or URL ). In my example, I run my <code>event-sourcing-order-poc project</code> by docker-compose, and the <code>spring-doc</code> will auto-generate the OpenAPI Specification and host on the server (i.g. http://localhost:8083/v3/api-docs.yaml). We only need to provide it in the <code>spec.definition</code>.</p> order-query-side-api.yaml<pre><code>apiVersion: backstage.io/v1alpha1\nkind: API\nmetadata:\n  name: order-query\n  description: The order-query API\n  tags:\n    - order\n    - query\n    - rest\n  links:\n    - url: https://github.com/NoahHsu/event-sourcing-order-poc/tree/master/order/query-side\n      title: Server Root\n      icon: github\nspec:\n  type: openapi\n  lifecycle: experimental\n  owner: guests\n  definition:\n    $text: http://localhost:8083/v3/api-docs.yaml\n</code></pre> <p>If we want to provide it in a static json text, we can change the definition like this:</p> <pre><code>definition: |\n    {\"openapi\":\"3.0.1\",\"info\":{\"title\":\"OpenAPI definition\",\"version\":\"v0\"},\"servers\":[{\"url\":\"http://localhost:7007/api/proxy\",\"description\":\"Generated server url\"}],\"paths\":{\"/api/v1/orders\":{\"post\":{\"tags\":[\"order-controller\"],\"operationId\":\"createOrder\",\"requestBody\":{\"content\":{\"application/json\":{\"schema\":{\"$ref\":\"#/components/schemas/Order\"}}},\"required\":true},\"responses\":{\"200\":{\"description\":\"OK\",\"content\":{\"application/json\":{\"schema\":{\"$ref\":\"#/components/schemas/Order\"}}}}}}},\"/api/v1/orders/complete\":{\"post\":{\"tags\":[\"order-controller\"],\"operationId\":\"completeOrder\",\"requestBody\":{\"content\":{\"application/json\":{\"schema\":{\"$ref\":\"#/components/schemas/Order\"}}},\"required\":true},\"responses\":{\"200\":{\"description\":\"OK\",\"content\":{\"application/json\":{\"schema\":{\"type\":\"object\",\"additionalProperties\":{\"type\":\"string\"}}}}}}}}},\"components\":{\"schemas\":{\"Order\":{\"type\":\"object\",\"properties\":{\"id\":{\"type\":\"string\"}}}}}}\n</code></pre> <p>For now, if we run the Backstage App, we will encounter the URL not allowed to read error. So we have to add the <code>reading.allow.host</code> into the <code>app-config.yaml</code> in the Backstage project like this as the tutorial says:</p> app-config.yaml<pre><code>backend:\n  ...\n  reading:\n    allow:\n      - host: localhost:8081\n      - host: localhost:8083\n  ...\n</code></pre> <p>As a result, after restarting the Backstage App no longer complain about the issue, and we can browse the API-Doc on Backstage smoothly.</p> <p></p> <p></p> <p></p> <p>It looks nice, but I'm facing the CORS issue when I try to use the \u201cTry it out\u201d function on the Swagger UI. Then I found three satisfying solutions by using the Backstage proxy or just enable <code>CrossOrigin</code> on my server. Please check it out on my another article Three Ways to Solve CORS Issue in the Embed Swagger Page in Backstage</p>","tags":["Backstage","Developer Portal"]},{"location":"Software%20Engineering%20Blog/2023/07/31/centralize-all-needed-knowledge-in-one-developer-portals-through-spotify-backstage/#add-thchdoc","title":"Add ThchDoc","text":"<p>To add TechDoc (<code>.md</code> file) with our project into Backstage App we have to provide more settings to tell Backstage where to find the documents. Here is a quick view of the whole config file in my project for adding techDoc in the Backstage App.</p> <p></p>","tags":["Backstage","Developer Portal"]},{"location":"Software%20Engineering%20Blog/2023/07/31/centralize-all-needed-knowledge-in-one-developer-portals-through-spotify-backstage/#basic-setting","title":"Basic Setting","text":"<p>First, we have to prepare a <code>mkdocs.yml</code> file like this: mkdocs.yml<pre><code>site_name: event-sourcing-poc\n\n# telling backstage how to render the navigator in sidebar\nnav: \n  - Overview: index.md\n  - Quick Start: run.md\n  - Business Logic:\n      - Event-Stream: event-stream.md\n      - Order: order.md\n      - Payment: payment.md\n      - Shipment: shipment.md\n  - System Architecture : system-architecture.md\n  - Code Structure: code-structure.md\n\nplugins:\n  - techdocs-core\n  # kroki is optional for support markdown with mermaid diagram\n  - kroki\n</code></pre></p> <p>Then add an annotation in the <code>.yaml</code> file for any kind (in my example, I put it in the System kind) with value and point to the directory that contains the <code>mkdocs.yml</code>(my kind System is declared in <code>all.yaml</code> in the same folder with the <code>mkdocs.yml</code>).</p> all.yaml<pre><code>...\n---\napiVersion: backstage.io/v1alpha1\nkind: System\nmetadata:\n  name: event-sourcing-poc\n  annotations:\n    backstage.io/techdocs-ref: dir:.\n    github.com/project-slug: NoahHsu/event-sourcing-order-poc\nspec:\n  owner: guests\n</code></pre> <p>The final step is to add two lines to the <code>packages/backend/Dockerfile</code> in the Backstage App like below:</p> packages/backend/Dockerfile<pre><code>...\nRUN apt-get update &amp;&amp; apt-get install -y python3 python3-pip\nRUN pip3 install mkdocs-techdocs-core==1.1.7\n\nUSER node # put the above two lines before this existing line\n...\n</code></pre>","tags":["Backstage","Developer Portal"]},{"location":"Software%20Engineering%20Blog/2023/07/31/centralize-all-needed-knowledge-in-one-developer-portals-through-spotify-backstage/#support-mermaid","title":"Support Mermaid","text":"<p>If our <code>.md</code> file contains mermaid diagrams, we have to add a plugin, kroki to help generate it. As the tutorial said, we should build a Docker image on our own with a Dockerfile like this:</p> kroki/Dockerfile<pre><code>FROM python:3.10-alpine\n\nRUN apk update &amp;&amp; apk --no-cache add gcc musl-dev openjdk11-jdk curl graphviz ttf-dejavu fontconfig\n\nRUN pip install --upgrade pip &amp;&amp; pip install mkdocs-techdocs-core==1.2.0\n\nRUN pip install mkdocs-kroki-plugin\n\nENTRYPOINT [ \"mkdocs\" ]\n</code></pre> <p>with a command like:</p> <pre><code>docker build . ${the-image-name}\n</code></pre> <p>Then tell the Backstage to use this image to generate techDoc in the <code>app-config.yaml</code>.</p> app-config.yaml<pre><code>...\ntechdocs:\n  builder: 'local'\n  generator:\n    runIn: 'docker'\n    dockerImage: {the-image-name}\n    pullImage: false\n  publisher:\n    type: 'local' # Alternatives - 'googleGcs' or 'awsS3'. Read documentation for using alternatives.\n...\n</code></pre> <p>Then we should modify the .md file itself to use \u201ckroki-mermaid\u201d instead of \u201cmermaid\u201d ( it\u2019s a little inconvenient though) like this:</p> <pre><code>```kroki-mermaid\n{kind of diagram i.e. C4Context}\n{the original contents}\n```\n</code></pre> <p>Finally, we can restart the Backstage App and see the result:</p> <p></p> <p>We can access the techDocs either by the link in a resource (i.e. System, Component\u2026) with the annotation (above picture) or the Documentation list (below picture).</p> <p></p> <p>After we access the page, we can see the Backstage start to generate the documents. After a moment, we can see the result:</p> <p></p> <p>With a navigator in the left sidebar, and a table of contents in the right side.</p> <p></p> <p>it can also show the mermaid-diagram.</p> <p></p> <p>Sweet!</p>","tags":["Backstage","Developer Portal"]},{"location":"Software%20Engineering%20Blog/2023/07/31/centralize-all-needed-knowledge-in-one-developer-portals-through-spotify-backstage/#summary","title":"Summary","text":"<p>This article show how to enhance the developer experience by integrating Our Apps into Backstage, which is designed to centralize all essential knowledge required for development, maintenance, and monitoring.</p> <p>Here provides a step-by-step guide focus on how to add software catalog to configure components and APIs. Then also include TechDocs to keep all things together.</p> <p>Here is the related PR and Repository:</p> <ul> <li>project to onboard backstage: My PR on project</li> <li>My Custom Backstage App: main branch</li> </ul>","tags":["Backstage","Developer Portal"]},{"location":"Software%20Engineering%20Blog/2023/07/31/centralize-all-needed-knowledge-in-one-developer-portals-through-spotify-backstage/#reference","title":"Reference","text":"<ul> <li>Backstage: https://backstage.io/docs/overview/what-is-backstage</li> <li>Backstage demo: https://demo.backstage.io/catalog?filters%5Bkind%5D=component&amp;filters%5Buser%5D=owned</li> <li>catalog example: https://github.com/backstage/backstage/blob/master/packages/catalog-model/examples/apis/petstore-webhook.oas.yaml</li> <li>default icon: https://github.com/backstage/backstage/blob/master/packages/app-defaults/src/defaults/icons.tsx#L39</li> </ul>","tags":["Backstage","Developer Portal"]},{"location":"Software%20Engineering%20Blog/2024/03/31/design-an-easy-to-test-flexible-application-with-config-server-toggle-system-and-mock-server/","title":"Design an Easy-to-Test, Flexible Application with Config Server, Toggle System, and Mock Server","text":"<p>In the modern software's fast-paced environment, the ability to swiftly adapt to changing requirements and market demands is the key to success. Achieving this needs the adoption of robust development practices. In this article, we will dive into how to design an easy-to-test, flexible application using a combination of powerful tools: Config Server, Toggle System, and Mock Server. When well implemented and applying a decent workflow, these components can significantly enhance the development lifecycle, enabling teams to build resilient and adaptable software systems.</p>","tags":["System Design","Devops","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2024/03/31/design-an-easy-to-test-flexible-application-with-config-server-toggle-system-and-mock-server/#introduction-of-tools","title":"Introduction of Tools","text":"<p>In this section, we will first brief the function of each component</p>","tags":["System Design","Devops","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2024/03/31/design-an-easy-to-test-flexible-application-with-config-server-toggle-system-and-mock-server/#config-server","title":"Config Server","text":"<p>Config Server is an extra server providing config (in the format of .xml, .json, etc.) to the applications. So we can change the config and then restart the application without committing code and compile. The point is to save time by skipping the PR check and the merge-pipeline.</p>","tags":["System Design","Devops","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2024/03/31/design-an-easy-to-test-flexible-application-with-config-server-toggle-system-and-mock-server/#toggle-system","title":"Toggle System","text":"<p>Toggle System is usually a bunch of servers that provide both management and evaluation features. We can change the toggle setting via the management server and when the application calls the evaluation API, it will get the new setting in the runtime. The main reason to use it is to change the app's behavior without code change, compile, and restart.</p>","tags":["System Design","Devops","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2024/03/31/design-an-easy-to-test-flexible-application-with-config-server-toggle-system-and-mock-server/#mock-server","title":"Mock Server","text":"<p>Mock Server can be configured to return specific responses for different requests (by path, headers, body...etc.), or be a proxy server to relay the request to the actual server and return the real response. The intention to use it is to mock some special case (error, timeout...etc.) or replace some API that does not allow you to put a loading test on it ( due to cost or other reasons).</p>","tags":["System Design","Devops","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2024/03/31/design-an-easy-to-test-flexible-application-with-config-server-toggle-system-and-mock-server/#design-for-different-situations","title":"Design for Different Situations","text":"<p>With the three components above, we can design the system architecture to demonstrate how to use them to help our work during the development of new features:</p>","tags":["System Design","Devops","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2024/03/31/design-an-easy-to-test-flexible-application-with-config-server-toggle-system-and-mock-server/#local-pr","title":"Local/ PR","text":"<p>When we develop in the local environment or run build in a PR-check runner, we can leverage the mock server to help us integrate 3rd-party API, help us make sure we can parse the response correctly, and make sure some mechanism works like our imagination.  <pre><code>    C4Context\n    title System Architect when Developing in Local\n\n    Boundary(b0, \"Local\") {\n        System(Backend1, \"Backend Server\", \"The Project's Application\")\n        System(Mock, \"Mock Server\", \"Set up responses for different requests\")\n    }\n\n    BiRel(Backend1, Mock, \"Send Request &lt;br&gt;&lt;br&gt;&lt;br&gt; Receive &lt;br&gt;Response\")\n    UpdateRelStyle(Backend1, Mock, $offsetX=\"-37\")</code></pre></p>","tags":["System Design","Devops","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2024/03/31/design-an-easy-to-test-flexible-application-with-config-server-toggle-system-and-mock-server/#beta","title":"Beta","text":"<p>After we merge the code and deploy it to the beta environment, we can add the config server and toggle evaluator to make our testing easier.</p> <pre><code>    C4Context\n    title System Architect when Testing in Beta\n\n    Boundary(b0, \"Cluster\") {\n        System(Backend1, \"Backend Server\", \"The Project's Application\")\n        Boundary(b1, \"Other components\") {\n            System(Config, \"Config Server\", \"Set up the config for Application start-up\")\n            System(Mock, \"Mock Server\", \"Set up responses for different requests\")\n            System(Toggle, \"Toggle Evaluator\", \"Set up the toggle for Application runtime\")\n        }\n    }\n\n    BiRel(Backend1, Config, \"1. Fetch config to enable &lt;br&gt;most use case to send request to &lt;br&gt; Mock server and Toggle Evaluator\")\n    UpdateRelStyle(Backend1, Config, $offsetX=\"-100\", $offsetY=\"-20\")\n\n    BiRel(Backend1, Mock, \"2. Send/Receive &lt;br&gt; Mock Response &lt;br&gt; or proxy for actual API\")\n    UpdateRelStyle(Backend1, Mock, $offsetX=\"-50\", $offsetY=\"40\")\n\n    BiRel(Backend1, Toggle, \"3. Decide whether to run &lt;br&gt;some features\")\n    UpdateRelStyle(Backend1, Toggle, $offsetX=\"-150\", $offsetY=\"-40\")</code></pre> <p>First, we can set some configs as <code>on</code> to make the app always call the mock server and the toggle evaluator (which would slightly slow down the performance, so we need a config to switch this behavior for different situations).</p> <p>Second, we can easily manipulate the mock server to return every edge-case response (temporary error, timeout, data conflict...) as the spec from 3rd-party API. Also, we could use the conditional proxy mode from the mock server to make some requests called the actual 3rd-party API.</p> <p>Finally, we could use the feature toggle to decide what version of the feature our Application will execute (e.g. switch two different algorithms to solve parsing document). Or, when testing some frequency limitation logic (e.g. we can only trigger notify 100 times in real product), we can use the toggle to set the limitation to 2 for testers, which can save their time to reach that limitation.</p> <p>Imagine how much time will be saved for both developers and testers. All of the above flexibility for testing can be done without any code change, PR checking, compilation, or deployment. Only to change the setting via UI and sometimes restart our application to fetch a new config from the config server.</p>","tags":["System Design","Devops","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2024/03/31/design-an-easy-to-test-flexible-application-with-config-server-toggle-system-and-mock-server/#load-test","title":"Load test","text":"<p>Sometimes, we are working on some low-latency-first system. We have to do a load test to show the real performance of our Application. In this case, we are likely facing some issues such as: 1. Some external APIs are not suitable for receiving too many requests due to cost or not only serving us. 2. Some hard-to-produce data need to be prepared (e.g. user data needs to be inserted into auth-server) 3. Some frequency limitations need to be ignored</p> <pre><code>    C4Context\n    title System Architect in a Load Test\n\n    Boundary(b0, \"Cluster\") {\n        System(Backend1, \"Backend Server\", \"The Project's Application\")\n        Boundary(b1, \"Other components\") {\n            System(Config, \"Config Server\", \"Set up the config for Application start-up\")\n            System(Mock, \"Mock Server\", \"Set up responses for different requests\")\n            System(Toggle, \"Toggle Evaluator\", \"Set up the toggle for Application runtime\")\n        }\n    }\n\n    BiRel(Backend1, Config, \"1. Fetch config to enable &lt;br&gt;needed use case to send request to &lt;br&gt; Mock server and Toggle Evaluator\")\n    UpdateRelStyle(Backend1, Config, $offsetX=\"-100\", $offsetY=\"-20\")\n\n    BiRel(Backend1, Mock, \"2. Replace some API that &lt;br&gt;can not put load test on\")\n    UpdateRelStyle(Backend1, Mock, $offsetX=\"0\", $offsetY=\"30\")\n\n    BiRel(Backend1, Toggle, \"3. loosen limitation or threshold in load test\")\n    UpdateRelStyle(Backend1, Toggle, $offsetX=\"-150\", $offsetY=\"-40\")</code></pre> <p>Fortunately, We have some tools to solve this issue, we definitely can use the mock server to replace the external API that can not put a load test on. Then we can also mock the response dynamically in most mock servers, including custom field values and response latency.</p> <p>Still, for the limitation logic problem, we use the feature toggle to set a big enough threshold to solve it. But we need to make sure that, the app will not send an extra request to the toggle evaluator. In some cases we only call toggle as a flexible way to make testing easier, it won't happen in the real situation, so we need to use the config server to disable this part to reduce the effect on performance.</p>","tags":["System Design","Devops","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2024/03/31/design-an-easy-to-test-flexible-application-with-config-server-toggle-system-and-mock-server/#prod","title":"Prod","text":"<p>When everything goes well in the test, we can build the production environment. Basically, we won't need and shouldn't use the Mock server in production, to prevent any unintentional mock results for real user requests.</p> <pre><code>    C4Context\n    title System Architect when Deploy to Production\n\n    Boundary(b0, \"Cluster\") {\n        System(Backend1, \"Backend Server\", \"The Project's Application\")\n        Boundary(b1, \"Other components\") {\n            System(Config, \"Config Server\", \"Set up the config for Application start-up\")\n            System(Toggle, \"Toggle Evaluator\", \"Set up the toggle for Application runtime\")\n        }\n    }\n\n    BiRel(Backend1, Config, \"1. Fetch config to disable &lt;br&gt;most use case to send request to &lt;br&gt; Mock server and feature toggle &lt;br&gt; to optimize the performance\")\n    UpdateRelStyle(Backend1, Config, $offsetX=\"-150\", $offsetY=\"20\")\n\n    BiRel(Backend1, Toggle, \"2. Decide whether to run &lt;br&gt;some features\")\n    UpdateRelStyle(Backend1, Toggle, $offsetX=\"-150\", $offsetY=\"-40\")</code></pre> <p>However, the toggle evaluator is still an essential part of the DevOps process, especially when we adopt trunk-based management. We should use it to hide some code and features that are not ready to go live from being executed. Besides, It can also be used to do some A/B tests and canary releases for some uncertainty in our new feature.</p>","tags":["System Design","Devops","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2024/03/31/design-an-easy-to-test-flexible-application-with-config-server-toggle-system-and-mock-server/#pilot-test","title":"Pilot Test","text":"<p>Sometimes, we will need to test the full user journey in the real system integrated with 3rd-party API vendors or even some real-world service. It might be a large new feature, e.g. the first time integrating with a new logistic vendor for an e-commerce platform so our tester might need to send a shipment via the system with the new vendor. </p> <pre><code>    C4Context\n    title System Architect when Pilot Test in Prod \n\n    Boundary(b0, \"Cluster\") {\n        System(Backend1, \"Backend Server\", \"The Project's Application\")\n        Boundary(b1, \"Other components\") {\n            System(Config, \"Config Server\", \"Set up the config for Application start-up\")\n            System(Toggle, \"Toggle Evaluator\", \"Set up the toggle for Application runtime\")\n        }\n    }\n\n    BiRel(Backend1, Config, \"1. Fetch config to disable &lt;br&gt;most use case to send request to &lt;br&gt; Mock server and feature toggle &lt;br&gt; to optimize the performance\")\n    UpdateRelStyle(Backend1, Config, $offsetX=\"-150\", $offsetY=\"20\")\n\n    BiRel(Backend1, Toggle, \"2. Decide whether to run &lt;br&gt;some features\")\n    UpdateRelStyle(Backend1, Toggle, $offsetX=\"-150\", $offsetY=\"-40\")</code></pre> <p>In this case, we would set up a whitelist toggle to allow only our testing accounts to use the new feature while normal users can not use or even see it. </p>","tags":["System Design","Devops","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2024/03/31/design-an-easy-to-test-flexible-application-with-config-server-toggle-system-and-mock-server/#summary","title":"Summary","text":"<p>In this article, we introduce three components (Config Server, Toggle Evaluator, and Mock Server) to make our development, testing, and maintenance easier and more flexible. Use config server to save the commit and compile time while we still have some flexibility to control the application's behavior, including whether to call the Mock server and Toggle Evaluator (is related to the tradeoff between performance and flexibility). Use Toggle System and Mock Server to dynamically decide the behavior of our application or to test some special branch of our code. Moreover, we gave the use case and system architecture for different situations from local development, PR check, Beta test, Load test, pilot test, and go live on the Production environment.  </p>","tags":["System Design","Devops","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2024/03/31/design-an-easy-to-test-flexible-application-with-config-server-toggle-system-and-mock-server/#reference","title":"Reference","text":"<ul> <li>Feature Toggles, by Pete Hodgson, in MartinFowler.com</li> <li>https://www.mock-server.com/</li> </ul>","tags":["System Design","Devops","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/","title":"Easier, Flexible, and Lower Resource Cost Deployment Strategies by Feature Toggle","text":"<p>Having different deployment strategies is essential to ensure that the new version of the software is delivered to users in an efficient and robust way. After reading other articles, we can organize the following insight of deployment-strategies( If you are not familiar with deployment strategies, please see deployment-strategies or deployment-strategies-6-explained-in-depth to get a comprehensive explanation).</p> <p>The easiest Recreate Deployment might cause service downtime and expose potential bugs to all users. Others (Blue/Green, Rolling, A/B Testing, Shadow, Canary\u2026 ) guarantee zero downtime, and some of them use more resources (hardware like memory, CPU\u2026) to achieve running both versions of the applications at the same time to provide more confidence in a release or easier to rollback to the old version.</p> <p>But we should not treat the hardware resources like they are free or unlimited, especially in this harsh time for the whole software industry. So as Pete Hodgson says in his article (Feature Toggle), we can use a feature toggle system to perform the strategy (i.e. Canary, A/B Testing\u2026) which can save some resources. Moreover, we can eliminate the difficult jobs (for some developers not familiar with DevOps or SRE knowledge) of setting the continuous delivery tool or network components (i.e. load balancer) for the strategy. The only works remaining are setting toggles and writing some code (easy if/else or switch).</p> <p>In this article, we will introduce:</p> <ol> <li>What features are required for a toggle system to do so?</li> <li>How to use a toggle system to perform different deployment strategies (Blue/Green, A/B Testing, Canary, Shadow\u2026 )?</li> <li>How to minimize the toggle maintenance effort?</li> </ol> <p>Here is the GitHub Repository open-feature-openflagr-example for this article, feel free to visit and leave any comments.</p>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/#requirements-of-a-feature-toggle-system","title":"Requirements of a Feature Toggle System","text":"<p>When considering the adoption of a toggle system instead of complex release strategies, it\u2019s essential to explore different open-source and enterprise-level toggle systems available on the internet(Unleash, Flagsmith, Flagr, LanuchDarkly, etc.) and choose a toggle system with the following features and traits in minimal requirements:</p> <ol> <li>Dynamic Evaluation can afford High RPS calling: The toggle system should handle high RPS loads efficiently when evaluating toggle states via its API (get toggle is on/off via API) since the toggle should affect the core business performance at a minimal level.</li> <li>Dynamic Configuration and Persistence: The toggle system should offer the flexibility to adjust settings dynamically, allowing changes to be made either through a UI or via an API. Furthermore, it should ensure that these configuration changes persist even in a server shutdown, ensuring consistent behavior across system restarts.</li> <li>Toggle evaluation API should provide features:</li> <li>Targeting Key supported: Distribution toggle result based on an identifier in request (e.g. a hash algorithm, so that the same ID will always get the same result)</li> <li>Evaluation Context Supported: Can set constraints to decide the result (e.g. when the region in request payload = Asia then toggle on; = Europe then toggle off)</li> </ol> <p>The above are the minimal requirements for replacing deployment strategies by integrating our application with a toggle system. We can transfer the traffic configuration work into our development job on the codebase to be reviewed with the feature pull request (PR).</p>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/#deployment-strategies-with-toggle","title":"Deployment Strategies with Toggle","text":"<p>In this part, we will show how to config the toggle (take Flagr as an example), and demonstrate how would the code snippet be like in a simple way ( I will use simple <code>if/else</code> or <code>switch</code> for the demo, so it can be implemented as a strategy pattern or other elegant way in a real project). Started from the easiest toggle on/off to perform Blue/Green or Shadow deployment. Then apply the percentage-based rollouts setting on the toggle to achieve Canary Release. Finally, add constraints to evaluate the context (fields in request payload) to implement A/B Testing.</p> <p>Given this shared code snippet for the following demos:</p> service.java<pre><code>public static String v1Feature() {\n    return BLUE + \"o\" + RESET;\n}\n\npublic static String v2Feature() {\n    return GREEN + \"x\" + RESET;\n}\n</code></pre>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/#bluegreen-onoff","title":"Blue/Green (on/off)","text":"<p>The configuration of the toggle is very simple in these two scenarios.</p> <p> </p> <p>and the code is like the below for a Blue/Green Deployment:</p> <pre><code>...\nboolean toggleOn = client.getBooleanValue(FLAG_KEY, false, ctx);\n\nString message;\nif (toggleOn) {\n    message = v2Feature();\n    v2++;\n} else {\n    message = v1Feature();\n    v1++;\n}\nSystem.out.print(message);\n...\n</code></pre> <p>I set the toggle off at first and then turned it on during the iteration execution. We can see that the app switches between the two features smoothly exactly as we expected.</p> <p></p> <p>In this way, we can save a lot of hardware resources since we don\u2019t need complete two (blue and green) distinguish environments to run the different versions of apps.</p>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/#shadow-release-onoff","title":"Shadow Release (on/off)","text":"<p>in this example, we can share the same flag config with the blue/green deployment but set the toggle on in the first place. The code is like the below for a Shadow Deployment: <pre><code>...\nString version = client.getStringValue(FLAG_KEY, \"off\", ctx);\n\nString message = \"\";\nmessage = v1Feature();\nv1++;\nif (version.equalsIgnoreCase(\"on\")) {\n    Thread newThread = new Thread(() -&gt; {\n        atomicString.accumulateAndGet(v2Feature(), String::concat);\n        v2.getAndIncrement();\n    });\n    newThread.start();\n}\nSystem.out.print(message);\n</code></pre> at first, we call both the v1 and the v2 features, suppose we find something went wrong in the v2 feature then we turn off the toggle during the iteration. Then we can see that the v2 is no longer been called.</p> <p></p> <p>Using a toggle system to perform Shadow Release is a highly flexible and efficient way. As long as we put some more complexity into code and put a little bit of effort into asynchronous.</p>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/#canary-release-percentage-based-rollouts","title":"Canary Release (percentage-based rollouts)","text":"<p>Let\u2019s introduce the distribution feature into the toggle\u2019s configuration for Canary Release.</p> <p> </p> <p>and the code is like the below for a Canary Release:</p> <pre><code>...\nUUID userId = UUID.randomUUID();\nMutableContext ctx = new MutableContext(userId.toString());\n\nString version = client.getStringValue(FLAG_KEY, \"v1\", ctx);\n\nString message = \"\";\nswitch (version) {\n    case \"v1\" -&gt; {\n        message = v1Feature();\n        v1++;\n    }\n    case \"v2\" -&gt; {\n        message = v2Feature();\n        v2++;\n    }\n}\nSystem.out.print(message);\n...\n</code></pre> <p>Given the distribution is like 3:1 (v1=75%; v2=25%), and since we gave different <code>targetKey</code> to every request, we will get a result that is very close to the given distribution.  what if we gave the same <code>targetKey</code> as <code>\"tester\"</code>,</p> <p></p> <p>The result will stay the same since the same <code>targetKey</code> is hashed to the same result (in this example, v2).</p> <p>So we can say that using a toggle system for canary release is quite easy and straightforward. we can change the percentage any time we like, as long as we think the new feature is steady enough to move on next level.</p>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/#ab-testing-constraints-on-context","title":"A/B Testing (constraints on context)","text":"<p>Finally, let\u2019s implement A/B testing. We could add the final piece of a toggle system, constraints on the context like below.</p> <p>  and the code is like below for A/B Testing:</p> <p><pre><code>...\nUUID userId = UUID.randomUUID();\nMutableContext ctx = new MutableContext(userId.toString());\nctx.add(\"region\", region);\n\nString version = client.getStringValue(FLAG_KEY, \"v1\", ctx);\n\nString message = \"\";\nswitch (version) {\n    case \"v1\" -&gt; {\n        message = v1Feature();\n        v1++;\n    }\n    case \"v2\" -&gt; {\n        message = v2Feature();\n        v2++;\n    }\n}\nSystem.out.print(message);\n...\n</code></pre> Given the constraint that all users from Asia should use the v1 feature while users from Europe use the v2, and users from other regions should use the feature like a fifty-fifty distribution. As we can see in the report, the distribution is as our expectation.</p> <p></p> <p>Since we can adjust the constraint dynamically, it makes it extremely flexible and easy to control the feature experiment, pilot feature in a production environment, etc.</p>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/#minimize-toggle-maintainance-effort","title":"Minimize Toggle Maintainance Effort","text":"<p>As the development cycle progresses, the toggle-related snippet will spread all over the codebase, or even worse, across multiple repositories. Then, the code will look like a mess, and developers will easily get lost in toggle logic and core business logic. Furthermore, we might also find that the chosen toggle system didn\u2019t meet our expectations or raised security concerns, leading to the need for transfer to a different toggle system.</p> <p>To address these complexities, it becomes imperative to introduce an additional layer of abstraction toggle logic to help the app perform toggle evaluation elegantly. Thus, the OpenFeature specification is born.</p> <p>We won\u2019t cover too much about OpenFeature, but here is the basic key concept that we should know:</p>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/#implementation-client","title":"Implementation Client:","text":"<ul> <li>Develop an XxxClient (Xxx represent the toggle system we choose, i.e. flagrClient), or use the SDK provided by the toggle system to be an API Client to send requests to the toggle system.</li> </ul> <p>OpenFlagrClient.java<pre><code>public interface OpenFlagrClient {\n\n  String BASE_PATH = \"/api/v1/\";\n\n  @RequestLine(\"POST \" + BASE_PATH + \"evaluation\")\n  @Headers(\"Content-Type: application/json\")\n  V1EvaluationResponse evaluate(V1EvaluationRequest request);\n\n}\n</code></pre> - Develop an XxxFeatureProvider, which lists all the common (or maybe the more reasonable) use cases for a real-time toggle evaluation logic.</p> OpenFlagrProvider.java<pre><code>public class OpenFlagrProvider implements FeatureProvider {\n...\n  public ProviderEvaluation&lt;Boolean&gt; getBooleanEvaluation(String key, \n           Boolean defaultValue, EvaluationContext ctx) {\n\n      V1EvaluationRequest request = buildRequest(key, ctx);\n\n      V1EvaluationResponse response = flagrClient.evaluate(request);\n      String answerVariant = response.variantKey() == null\n              ? \"\"\n              : response.variantKey().toLowerCase();\n      boolean isOn = defaultOnToggleKeys.contains(answerVariant);\n\n      return ProviderEvaluation.&lt;Boolean&gt;builder()\n              .value(isOn)\n              .variant(response.variantKey())\n              .build();\n  }\n\n  @Override\n  public ProviderEvaluation&lt;String&gt; getStringEvaluation(String key, \n           String defaultValue, EvaluationContext ctx) {\n      V1EvaluationRequest request = buildRequest(key, ctx);\n      V1EvaluationResponse response = flagrClient.evaluate(request);\n      String answerVariant = response.variantKey() == null\n              ? \"\"\n              : response.variantKey();\n\n      return ProviderEvaluation.&lt;String&gt;builder()\n              .value(answerVariant)\n              .build();\n  }\n... there are a lot of other methods\n\n}\n</code></pre>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/#configuration-client-and-openfeature","title":"Configuration Client and OpenFeature","text":"<p>Then, configure the XxxFeatureProvider to the OpenFeatureAPI instance, which is designed to have multiple different FeatureProvider (can set/get with name). Here, since I am working on a spring boot, I build a class to contain the OpenFeatureAPI instance.</p> FeatureToggleApiProvider.java<pre><code>public class FeatureToggleApiProvider implements InitializingBean {\n    @Autowired\n    FlagrClient flagrClient;\n\n    OpenFeatureAPI api = OpenFeatureAPI.getInstance();\n\n    @Override\n    public void afterPropertiesSet() throws Exception {\n        OpenFlagrProvider openFlagrProvider = new OpenFlagrProvider(flagrClient);\n        api.setProvider(openFlagrProvider);\n    }\n\n    public Client getFlagrApiClient() {\n        return api.getClient();\n    }\n\n}\n</code></pre>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/#make-use-of-openfeature","title":"Make Use of OpenFeature","text":"<p>Finally, other modules can make use of this <code>OpenFlagrProvider</code> to perform toggle evaluation by getting a Client interface ( not implemented by the XxxClient, but is by OpenFeatureClient which will make use of the given XxxFeatureProvider):</p> <pre><code>Client client = featureToggleApiProvider.getFlagrApiClient();\n\nString version = client.getStringValue(FLAG_KEY, \"v1\", ctx);\n// or \nboolean toggleOn = client.getBooleanValue(FLAG_KEY, false, ctx);\n</code></pre>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/#whats-the-benefits","title":"What's the Benefits?","text":"<p>Here is a rough introduction of how to integrate a toggle system via OpenFeature specification (please check my GitHub Repo for more details and complete code). The toggle logic is extracted into another abstract layer and the main application remains focused on core business and deployment strategies. Even one day we need to change the toggle system, the application won\u2019t need any change, since we only need to develop the new XxxClient and XxxFeatureProvider (maybe there is an existing one so no development work is needed, check out the OpenFeature Ecosystem).</p>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/#summary","title":"Summary","text":"<p>In this article, we go through three points that we should know when we want to perform the deployment strategies in a more flexible, easier, and lower-cost way with a feature toggle. First, our toggle system should be capable of dynamic configuration with persistence, provide a high-efficiency dynamic evaluation method, and the evaluation should support targeting-key and constraint on request context (payload). Then, we show the toggle configuration and code snippet for different kinds of deployment strategies. Finally, we introduce the OpenFeature abstraction layer to make the codebase stay clean and be more maintainable and flexible.</p>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/09/28/easier-flexible-and-lower-resource-cost-deployment-strategies-by-feature-toggle/#reference","title":"Reference","text":"<p>Deployment strategies and toggle knowledge</p> <ul> <li>https://www.baeldung.com/ops/deployment-strategies</li> <li>https://www.plutora.com/blog/deployment-strategies-6-explained-in-depth</li> <li>https://martinfowler.com/articles/feature-toggles.html</li> <li>https://en.wikipedia.org/wiki/Service_provider_interface</li> </ul>","tags":["DevOps","Feature Toggle","Deployment Strategy"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/","title":"Get Kafka in prod-ready, 2 decisions to make and 3 implementation details","text":"<p>Kafka is a well-known event-streaming solution that provides events publish/subscribe stream and persistence. Besides the main strength, very high throughput (approximate million RPS), Kafka also does well on scalability and availability and provides guarantees on event broadcasting.</p> <p>But Kafka can\u2019t be just plugged into our system and bring out 100% of its advantages. So in this article, we will go through 2 decisions and 3 implementation details that should consider when using Kafka in Production. The Decisions to make include:</p> <ol> <li>number of partitions and replicas </li> <li>semantic level.</li> </ol> <p>Besides, we should be careful in the following issues:</p> <ol> <li>eventually consistency </li> <li>event order </li> <li>handling retry and recovery.</li> </ol> <p>Before this, let\u2019s have a basic recap of Kafka!</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#basic-recap","title":"Basic recap","text":"<p>Kafka mainly provides an asynchronous decouple way for different web applications. We Can divide it into the server and client sides, the server side is usually a cluster that receives and stores events from the client. The client side\u2019s responsibility is to send/consume events from the server and do the business logic among events. See the official document for a detailed introduction.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#basic-terminology","title":"Basic Terminology","text":"<p>For the Kafka server-side, we have:</p> <ul> <li>Broker    Broker is a single Kafka server to receive/reply to event messages. We can connect multiple brokers to form a Kafka cluster.</li> <li>Topic    The Topic is for distinguishing all event messages sent into Kafka (like the table in RDB)</li> <li>Partition    One Topic will be divided into multiple partitions to store the event message of the same topic.</li> <li>Replica    One Topic can have multiple data replicas on different brokers.</li> </ul> <p>Given there are 3 partitions and 3 replicas for one topic in a 3 brokers Kafka cluster. Then there will be 3 partitions for each broker for this topic, and one of the same partitions will be selected as the Leader while others are replicas (as in the image below).</p> <p> (image copied from https://learnbyinsight.com/2020/07/26/beginner-guide-to-understand-kafka/)</p> <p>Here we skip the Zookeeper since the Kafka team seems to discard it and use the Quorum controller instead.</p> <p>The Kafka client-side includes:</p> <ul> <li>Producer    The producer sends data to the broker that has the leader partition of the topic.</li> <li>Consumer    The consumer pulls data from the broker that has the leader partition of the topic by default (may read from replicas partition, see this article).</li> <li>Consumer group    We can assign multiple consumers to one consumer group, then one of them will pull it, while others do not when one event message is sent, but the consumer outside the same consumer group will consume the event message.</li> </ul>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#decision-1-number-of-partitions-and-replicas","title":"Decision 1: number of partitions and replicas","text":"<p>Before we start to use Kafka in our system, the first thing is to create the topic in the Kafka cluster with parameters like the number of partitions and replicas, which affect the performance and reliability respectively. Since the partition represent the parallelism</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#number-of-partitions","title":"number of partitions","text":"<p>For partitions, there are two aspects to consider, the desired throughput and the number of brokers and consumers. Before that, we should know that 1 partition can only be consumed by 1 consumer in a consumer group. The exceed consumers will be idle, but two partitions can be listened to by the same consumer. First, the number of partitions must be greater than the number of consumers.</p> <p>Second, we should do a simple test about the speed for consuming from a single partition, and use the speed to measure the number with the desired throughput.</p> <pre><code>number of partitions = Desired Throughput / single partition speed\n</code></pre> <p>Finally, we should modify it according to the number of brokers, because partitions should be evenly distributed to each broker in order to get a balanced loading on each broker. So here is some advice, when there are fewer than 6 brokers, then we should create the partitions with 2 or 3 times the number of brokers in the cluster. The exceeded partitions are for expanding cluster size (which can be done with help of the partition reassignment tool). And if there are more than 6 brokers, we can use the same number of brokers.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#number-of-replicas","title":"number of replicas","text":"<p>Before we start, there are two properties that need to be clarified here, <code>min.insync.replicas</code> and <code>acks</code>. The former restricts the minimum number of replicas that have the same message as the leader partition; the latter denotes the number of replicas \u201creceive\u201d the produced message. this article describes these in every detail.</p> <p>For replication, it is relatively simple since it is just a trade-off between performance and fault tolerance. In almost every distributed system, 3 replication is a best practice in common. Since it provides strong reliability, for example, when one of the partitions is dead, then we still have two <code>insync-replicas</code> and plenty of time to recover the failed replica or start a new one. The key point is that one more fail is tolerable when recovering the third replica, the Kafka remains available except there are three fails before recovering one of them. Moreover, the more replicas the more acknowledgment the producer needs when publishing an event (if the <code>acks</code> is set to <code>all</code>), which is a performance killer. So I believe that 3 is the best number for replicas.</p> <p>But for the <code>min.insync.replicas</code>, I think it can remain at 1 for higher availability as Cloudkarafka recommended. Since if set to 2, then we will have a situation where only one broker is down, but the whole cluster is not able to receive new messages, which I think is not reasonable.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#decision-2-semantic-level","title":"Decision 2: Semantic level","text":"<p>Below are the message semantic levels in the official document:</p> <ul> <li>At most once \u2014 Messages may be lost but are never redelivered.</li> <li>At least once \u2014 Messages are never lost but may be redelivered.</li> <li>Exactly once \u2014 this is what people actually want, each message is delivered once and only once.</li> </ul> <p>This issue is all about the retry/acknowledge/idempotent mechanism of the producer and consumer and once again, is a trade-off between performance and completeness guarantee. Below, I use a simple table to display how to achieve the semantic level:</p> <p></p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#at-most-once","title":"At most once","text":"<p>When the producer is set to no retry and ack is needed, then it is obvious that there might be no message sent to Kafka. Besides, the consumer <code>enable.auto.commit</code> is <code>true</code> (as default) when it pulls the message from a broker before really handling it successfully, so maybe all the consumption process fails which leads to \u201cAt most one\u201d message being processed.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#at-least-once","title":"At least once","text":"<p>When the producer starts to retry until receives at least one ack from the broker, it might send 1 or more times for the same message (&gt;1 due to a network error). Since the target is that the message is processed \u201cat least once\u201d, we should no longer use auto-commit ack, but manually commit it after the process successfully instead.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#exactly-once","title":"Exactly once","text":"<p>Based on settings of \u201cAt least once\u201d, we can add idempotent mechanics to achieve exactly once. want to do so, we can add a property like <code>enable.idempotence</code> to <code>true</code> in producer (note: need <code>acks=all</code>), which will append a <code>PID</code> and <code>sequenceId</code> in the sent and retry event message to make the broker can identify which message is already written into the partition. As a result, the producer will only make one record in Kafka.</p> <p>In the consumer, we must implement our business logic in an idempotent way, in order that the manual return ack is not received by the broker due to a network error. An easy implement way is to add a <code>eventId</code> in the event message, then use it as an <code>idempotentKey</code> or <code>dataVersion</code> to prevent reprocessing of the same event.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#implement-detail-1-eventually-consistency","title":"Implement Detail 1: Eventually consistency","text":"<p>There are two layers of eventually-consistency for using Kafka,</p> <ol> <li>Produce event eventually received by all replicas.</li> <li>All consumer groups eventually pull and processed the event</li> </ol> <p>In the producer, this is a trade-off of performance since we can set the number of <code>acks</code> that the producer needs. The options include 0, 1, and all, the higher number causes lower RPS of the whole system.</p> <p>We should always be aware that the consumer is not guaranteed to finish consuming the event after servers received a request and success sent an event to Kafka and then respond with an \u201cok\u201d back to the client. Especially when the second request comes, and we need to check the data which should be updated by the consumer when handling the first event, like an event-sourcing system. We can add the version checks by adding an <code>eventId</code> in the event message when we encounter this situation.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#implement-detail-2-event-order","title":"Implement Detail 2: Event Order","text":"<p>In Kafka, only the events from the same partition are guaranteed to be consumed in order, but it\u2019s impossible to use only one partition that will completely lose the parallelism ability (if really need to do so, I would say that the scenario is not suitable for using Kafka).</p> <p>So, the problem becomes how to let the order-sensitive events can be in the same partition. There are three ways to decide which event would be sent to which partition:</p> <ol> <li>Round robin (Default)    In this way, the event will be sent to all partitions in balance.</li> <li>Hash key    In this way, all events should contain a key, which will be hashed and distributed into each partition. But we need to be aware of the loading of each partition. As we can see the default hash method takes modulus by the number of partitions, so we basically need to make sure that the event needs orders must have the same key; and irrelevant events should have a different key to make a balanced loading.</li> <li>Custom partitioner    If we need a more complex logic than the hash key method to satisfy our business, then we should implement a custom partitioner and config it in the consumer. Here is an easy example of doing this.</li> </ol> <p>This section gives a brief concept about event ordering, we can learn more and check the implementation example in this article.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#implement-detail-3-handle-retry-and-recovery","title":"Implement Detail 3: Handle retry and recovery","text":"<p>As an asynchronous messaging system, error handling is an important part of providing a reliable service. Especially for the consumer of Kafka, imagine that if the consumer encounters an exception (from DB or third-party API), the behavior will differ depending on the settings of ack commitment.</p> <ol> <li>auto-commit = true    The consumed event is committed before catching the exception, so the event actually is considered finished. No consumer will try to consume that event again, which will lead to data inconsistency.</li> <li>auto-commit =false    The consumed event is committed manually, so our code will decide whether the exception is thrown before or after catching an exception, in most cases we commit the event after all processes are done. Therefore, it will cause an infinite loop of consuming events and catch exceptions (if it\u2019s not a temporary issue).</li> </ol> <p>It seems to get a bad result under both settings when an exception occurs. So it\u2019s important to implement the retry and recovery mechanism. The concepts and steps are like following:</p> <ol> <li>consume event </li> <li>send to retry-queue (delay-queue) if a recoverable exception is thrown </li> <li>send to fail-queue (dead letter queue) if a non-recoverable exception is thrown </li> <li>consume from retry-queue </li> <li>send to fail-queue if it fails again </li> <li>consume from fail-queue and log/alert</li> </ol> <p>First, our work is to define whether an exception is recoverable or not. we can reference this article which defines many error scenarios. Second, we should decide that the retry-queue and fail-queue would be implemented by another Kafka topic or DB table\u2026etc. I think there is no correct answer, we should choose the service which provides the highest availability. Finally, it\u2019s time to implement the mechanism, we can take a look at my article about how to do that in a Spring Boot 3 application.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#conclusion","title":"Conclusion","text":"<p>There are plenty of articles that dive deep into the topic in this article, but this article tries to summarize them and organize the key points we need to decide and design when we want to use Kafka in our system in the real environment. Using Kafka in a real environment means it needs to have high reliability and availability, and also provide high throughput (achieve an acceptable balancing performance, at least).</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/01/28/get-kafka-in-prod-ready-2-decisions-to-make-and-3-implementation-details/#reference","title":"Reference","text":"<p>Basic introduction document</p> <ol> <li>https://kafka.apache.org/documentation/</li> <li>https://learnbyinsight.com/2020/07/26/beginner-guide-to-understand-kafka/</li> </ol> <p>Consume from replicas</p> <ol> <li>https://developers.redhat.com/blog/2020/04/29/consuming-messages-from-closest-replicas-in-apache-kafka-2-4-0-and-amq-streams</li> </ol> <p>Kafka cluster without Zookeeper</p> <ol> <li>https://cwiki.apache.org/confluence/display/KAFKA/KIP-631%3A+The+Quorum-based+Kafka+Controller</li> </ol> <p>The number of partitions and replicas</p> <ol> <li>https://www.conduktor.io/kafka/kafka-topics-choosing-the-replication-factor-and-partitions-count</li> <li>https://www.linkedin.com/pulse/choosing-right-partition-count-replication-factor-apache-ul-hasan/</li> </ol> <p>Semantic level</p> <ol> <li>https://medium.com/@andy.bryant/processing-guarantees-in-kafka-12dd2e30be0e</li> <li>https://medium.com/lydtech-consulting/kafka-idempotent-producer-6980a3e28698</li> </ol> <p>Event order</p> <ol> <li>https://medium.com/latentview-data-services/how-to-use-apache-kafka-to-guarantee-message-ordering-ac2d00da6c22</li> </ol> <p>Error Handling</p> <ol> <li>https://blogs.perficient.com/2021/02/15/kafka-consumer-error-handling-retry-and-recovery/</li> <li>https://blog.pragmatists.com/retrying-consumer-architecture-in-the-apache-kafka-939ac4cb851a</li> </ol>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2024/04/21/high-availability-deployment-by-pod-topology-spread-constraints-in-k8s-cluster/","title":"High Availability Deployment by Pod Topology Spread Constraints in K8s Cluster","text":"<p>In the modern world, running a high-availability service is the most important thing for the users. As Kubernetes is getting more common, it's essential to know how to achieve a robust deployment across all the Applications. Assuming the network unreliability is handled by the Application retry and idempotency mechanism, what's left is to make sure the Applications are running well. The only threat is some \"real-world\" damage to the server. so we are always told to spread our application across different server-rack, data center zones, or geography regions.</p> <p>In this article, we will share:</p> <ol> <li>Related K8s labels to be used</li> <li>How to use the Pod Topology Spread Constraints</li> <li>How would it work</li> </ol>","tags":["Kubernetes","Devops","High-Availability"]},{"location":"Software%20Engineering%20Blog/2024/04/21/high-availability-deployment-by-pod-topology-spread-constraints-in-k8s-cluster/#related-k8s-labels-to-be-used","title":"Related K8s Labels to Be Used","text":"<p>There are some native labels that we can make use of. Since our goal is to distribute the applications among different servers, zones, and regions. The following labels denote the essential properties:</p> <ul> <li> <p>kubernetes.io/hostname   As the name means, this label shows the hostname of the node, every node will have a different name.</p> </li> <li> <p>topology.kubernetes.io/region   The nodes in different region will get a different value for this label.</p> </li> <li> <p>topology.kubernetes.io/zone   The nodes in different zone will get a different value for this label.</p> </li> </ul>","tags":["Kubernetes","Devops","High-Availability"]},{"location":"Software%20Engineering%20Blog/2024/04/21/high-availability-deployment-by-pod-topology-spread-constraints-in-k8s-cluster/#set-up-the-deploymentyaml","title":"Set up the deployment.yaml","text":"<p>In the deployment setting file for Kubernetes, We can use the topology-spread-constraints to control how the Pods are assigned in the cluster. If we want our applications can be run on different nodes, zones, and regions to avoid simultaneous failures (no matter if an accident or a scheduled maintenance causes it). The naive <code>.yaml</code> file would like below: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  # ... other properties like replicas, selector...\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n        # - ... the container to deploy\n      topologySpreadConstraints:\n        - maxSkew: 1\n          topologyKey: topology.kubernetes.io/zone # or use topology.kubernetes.io/region\n          whenUnsatisfiable: ScheduleAnyway\n          labelSelector:\n            matchLabels:\n                app: myapp\n        - maxSkew: 1\n          topologyKey: kubernetes.io/hostname\n          whenUnsatisfiable: ScheduleAnyway\n          labelSelector:\n            matchLabels:\n              app: myapp\n      # ... other properties like affinity, tolerations...\n</code></pre></p> <p>In this way, our pods will not concentrate on the same hardware (node, zone, or region) due to the <code>maxSkew</code> setting to one (means the difference of replicas number in different zones ( or between nodes and regions) would not be bigger than 1). The ideal process when the Pods number grows under the former Topology Spread Constraints would act like below: </p> <p></p> <p>The Scheduler would try its best to assign the pods to satisfy these constraints. But if there is no room for a new pod in a specific node, zone, or region to satisfy that, it would still assign it to others to prevent overloading for existing pods (because we set the <code>whenUnsatisfiable</code> as <code>ScheduleAnyway</code>).</p>","tags":["Kubernetes","Devops","High-Availability"]},{"location":"Software%20Engineering%20Blog/2024/04/21/high-availability-deployment-by-pod-topology-spread-constraints-in-k8s-cluster/#summary","title":"Summary","text":"<p>In this article, we introduce a simple example of using <code>topologySpreadConstraints</code> to make sure our deployment can be high-availability. In this way, our service is run on different nodes, zones, and regions to prevent simultaneous failures like node broken, data center accidents, regional disasters, etc. If you need more detail or explanation for each attribute for <code>topologySpreadConstraints</code>, please visit the official document here. Thanks for reading, helps this article can help you out in any way.</p>","tags":["Kubernetes","Devops","High-Availability"]},{"location":"Software%20Engineering%20Blog/2024/04/21/high-availability-deployment-by-pod-topology-spread-constraints-in-k8s-cluster/#reference","title":"Reference","text":"<ul> <li>well known labels: https://kubernetes.io/docs/reference/labels-annotations-taints/</li> <li>topology-spread-constraints: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</li> <li>deployment.yaml example: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</li> </ul>","tags":["Kubernetes","Devops","High-Availability"]},{"location":"Software%20Engineering%20Blog/2023/11/23/how-to-design-an-efficient-idempotent-api/","title":"How to design an efficient Idempotent API","text":"<p>Idempotency API means that the data/ system state will be the same no matter how many times the API is successfully called with the same request body/parameter.</p> <p>For a well-follow-principles RESTful API, besides the POST and PATCH, all other methods are idempotent. But in some particular circumstances (e.g. create a payment, upload files, microservice communication...), it is essential to implement idempotent if we want to make an API more robust.</p> <p>In This article, we will propose an idempotency mechanism that is also focus on the API performance. The topics we will cover are as follows:</p> <ul> <li>Why do we need an API to be idempotent</li> <li>How to improve performance</li> <li>Propose an efficient idempotency mechanism</li> </ul>","tags":["Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2023/11/23/how-to-design-an-efficient-idempotent-api/#why-need-idempotent-api","title":"Why need Idempotent API","text":"<p>In the software engineering world, we were told that the network is not always reliable. The naive solution is to retry when a request fails. But here arises the question: it's not just sending requests that's unreliable; receiving responses is also unstable.</p> <pre><code>sequenceDiagram\ntitle How Network Unreliable Fails a Request    \nalt sending requests fail\nServer1--xNetwork Unreliable:send request\nelse receiving responses fail\nServer1-&gt;&gt;+Server2:send request\nServer2--x-Network Unreliable: send response\nend</code></pre> <p>As seen in the above picture, two distinct scenarios can result in server1 failing to receive a response. However, the scenarios are entirely different for the data state in server2. When the request fails to send, server2 doesn't receive any data, so the resources aren't created yet. In this case, it's safe to retry. Conversely, if the issue lies in the response not being correctly sent back to server1, server2 has already generated the resources. In such a scenario, retrying without adequate protection is inappropriate.</p> <p>Engaging in a retry without protection could potentially lead to critical issues, such as double-charging customers, generating duplicate files on the server, or causing data inconsistencies, etc.</p> <p>Implementing an idempotent API stands as one of the solutions to establish a safe-retry mechanism, ensuring the provision of a robust API and a resilient system.</p>","tags":["Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2023/11/23/how-to-design-an-efficient-idempotent-api/#how-to-improve-performance","title":"How to improve performance","text":"<p>A traditional idempotency API solely relies on the database (DB) to verify whether a request has been previously processed, which is an extra workload for the DB that could cause a bad performance for our API. </p> <p>Considering the common retry scenarios, they are mostly retries following a request timeout or retries initiated by an asynchronous message consumer, etc. Most retries tend to occur within a short timeframe.</p> <p>Hence, we propose to put an extra memory cache server (such as Redis) to help optimize the mechanism by caching this data, thereby enhancing its efficiency.</p>","tags":["Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2023/11/23/how-to-design-an-efficient-idempotent-api/#efficient-idempotent-mechanism","title":"Efficient Idempotent mechanism","text":"<p>Here is the flow of my proposed mechanism:</p> <pre><code>---\ntitle: Efficient Idempotent mechanism\n---\nflowchart LR\nP1[Request &lt;br&gt; with `rid`] --&gt; D1{Is `rid` exist &lt;br&gt;in cache?}\nD1 --&gt;|Yes| D2{Is Done?}\nD1 --&gt;|No| P2[Init cache &lt;Br&gt;with `rid`]\nsubgraph cache exist\n    D2 --&gt;|Yes| P3[Return cached&lt;Br&gt; response]\n    D2 --&gt;|No| P4[Return in-progress&lt;Br&gt; error]\nend\nsubgraph no cached exist\n    P2 --&gt;D3{Is `rid` exist &lt;br&gt; in DB}\n    D3 --&gt;|Yes| P6[Update &lt;Br&gt; cache]\n    D3 --&gt;|No| P5[Do original &lt;br&gt;API process]\n    P5 --&gt; D4{Is process &lt;Br&gt;success}\n    D4 --&gt;|Yes| P6[Update cache]\n    D4 --&gt;|No| D5{Is error &lt;Br&gt;retryable}\n    D5 --&gt;|No| P6\n    D5 --&gt;|Yes| P7[Delete &lt;Br&gt; cache]\n    P6 &amp; P7 --&gt; P8[Return &lt;Br&gt; response] \nend</code></pre> <p>In the above mechanism, We have 8 processes and 5 decision points. Note that this whole flow should be placed after the auth-filter to prevent an unauth request from getting the cached data. Now, let's delve into each of them individually (P for process; D for Decision):</p>","tags":["Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2023/11/23/how-to-design-an-efficient-idempotent-api/#main-flow","title":"Main Flow","text":"<ul> <li>P1: Request with <code>rid</code>    When invoking the idempotency API, it's essential to supply a unique <code>request-id</code> (or <code>rid</code>), often a combination of timestamp, user ID, and a random number.</li> <li>D1: Is <code>rid</code> exist in cache    The initial server action involves checking for the existence of <code>rid</code> in the cache to determine the subsequent flow based on its presence or absence. </li> </ul>","tags":["Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2023/11/23/how-to-design-an-efficient-idempotent-api/#subflow1-when-the-cache-does-not-exist","title":"Subflow1: When the cache does not exist","text":"<ul> <li>P4: init cache with <code>rid</code>    If no cache exists, the first step is to create one with an <code>is-done</code> property set to <code>false</code>. This operation should occur within the same transaction as D1 to prevent race conditions, possibly using methods like <code>setIfAbsent</code> in the Java Redis client or <code>setnx</code> in Redis.</li> <li>D3: Is <code>rid</code> exist in DB    Then before we really handle the request, we still need to check whether the request has proceeded previously, in order to prevent duplication. </li> <li>P5: Do the original API process    In the absence of cache or DB records, the original process should be executed to handle the request.</li> <li>D4: Is process success    Upon handling the request, different reactions are triggered based on the process result. Successful execution leads to cache update and response to the caller. Otherwise, detailed error checking is necessary to decide what to do next.</li> <li>D5: Is error retryable    Proper exception handling is crucial in categorizing errors as retryable or non-retryable. Retryable errors include network errors to the 3rd-party API or other data sources, file systems, etc. Non-retryable errors, such as validation errors, internal-server errors, etc. require code modification or settings adjustment for successful retries.  </li> <li>P6: Update cache    Upon obtaining a definitive result (success or non-retryable error), the cache is updated with the actual response and setting <code>is-done</code> to <code>true</code>.</li> <li>P7: Delete cache    For retryable errors, the cache should be deleted, anticipating the handling of retry requests.</li> <li>P8: Return response    Regardless of the process outcome, responding to the caller is essential.</li> </ul>","tags":["Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2023/11/23/how-to-design-an-efficient-idempotent-api/#subflow2-when-cache-exists","title":"Subflow2: When cache exists","text":"<ul> <li>D2: Is Done    Retrieve the cache value and inspect the <code>is-done</code> property, which was set or updated during the Subflow1.</li> <li>P2: Return cached response    If the <code>is-done</code> properties in the cache is <code>ture</code>, indicating a previously completed request, and promptly returning the cached response.</li> <li>P3: return in-process error    If the <code>is-done</code> properties in the cache is <code>false</code>, means the request is now being executed, in order to prevent race conditions or some other multi-thread issue, we should return an error code to inform the caller that the request is in progress and advise trying again later.</li> </ul>","tags":["Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2023/11/23/how-to-design-an-efficient-idempotent-api/#summary","title":"Summary","text":"<p>This article introduces an idempotency mechanism that collaborates with a cache server to enhance performance. Here are some crucial points to consider during implementation:</p> <ol> <li>Unique Request Identifier (<code>rid</code>):   Clients must include a rid in their requests, ensuring it's distinctive enough to identify each request uniquely.</li> <li>Placement after Auth-Filter:    The Idempotency flow should be positioned after the authentication filter for security concerns.</li> <li>Atomic Cache Operations:    Checking for cache existence and creating cache entries should occur atomically to ensure consistency.</li> <li>DB Integration for Non-existent Cache:    Despite leveraging caching, the mechanism requires a database for existence checks when the cache is absent.</li> <li>Utilization of Additional Properties:    Incorporate supplementary properties like is-done within cache values to denote the processing status of a request.</li> <li>Exception Handling Retryability:    Define the retryability of each exception encountered during request handling, aiding in decisions regarding cache updates or deletions.</li> </ol> <p>I will then try to implement this mechanism on my event-sourcing poc project, see you next time.</p>","tags":["Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2023/11/23/how-to-design-an-efficient-idempotent-api/#reference","title":"Reference","text":"<ul> <li>https://restfulapi.net/idempotent-rest-apis/</li> <li>https://dev.to/karishmashukla/building-resilient-systems-with-idempotent-apis-5e5p</li> </ul>","tags":["Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2024/05/31/mysql-trap-when-db-read-write-separate-and-parent-child-relationship-schema-without-transaction/","title":"Mysql Trap when DB Read-Write Separate and Parent-Child Relationship Schema without Transaction","text":"<p>DB replicas are essential for data-intensive systems. First, we can accelerate our performance by writing to the master and reading from replicas, which can have many instances or be geographically close to users. Second, the replica DB can serve as a backup for the system. No matter what happens to the main DB, it can be a data recovery source or even become the new main DB to reduce downtime.</p> <p>It's no secret that the trade-off for using a read-write separation mechanism is so-called \"data inconsistency.\" The solutions to avoid this situation depend on the use cases and the architecture. In this article, we will go through the MySQL binlog mechanism and what would happen if we don't use transaction when insert/update parent-child relationship data. Let's start!</p>","tags":["Database","Distributed Systems","Read-Write Separate"]},{"location":"Software%20Engineering%20Blog/2024/05/31/mysql-trap-when-db-read-write-separate-and-parent-child-relationship-schema-without-transaction/#how-replica-sync-data-from-main-db","title":"How Replica Sync Data from Main DB","text":"<p>In MySQL 8.x, the replica DB syncs data from the main DB by reading the \"binary log,\" which we call a \"binlog.\" The content of the \"binlog\" can be set in two formats: row-based and statement-based. Since some nondeterministic statements (those that depend on a loadable function or stored program, LIMIT without an ORDER BY, etc.; see more details on (statement-based replication disadvantages) are not suitable for statement-based replication, the default format is row-based (ref.).</p> <p>In the row-based replication, it will acquire a table lock first, and then apply the affected row changes (update or insert) in batch mode (more detail in row-based replication process). So all the changes within one statement or one transaction will be applied together. </p> <p>As a result, if we don't use transaction on multiple insert statements to the same table, the \"binlog\" will add multiple separate logs, one for each record. The replica DB then reads the logs one by one, which would be a trap that some change is applied but some are not. </p> <p>After we understand how MySQL replicas sync data, let's take a look on which use cases might fail under this mechanism.</p>","tags":["Database","Distributed Systems","Read-Write Separate"]},{"location":"Software%20Engineering%20Blog/2024/05/31/mysql-trap-when-db-read-write-separate-and-parent-child-relationship-schema-without-transaction/#use-case-that-fall-into-trap","title":"Use Case that Fall into Trap","text":"<p>Given that we are an e-commerce platform providing a create order feature to the user, one order may contain many items, which we call order items. This data should be saved into the DB and used to calculate for data analysis and financial requirements. To handle the peak loading during campaigns, we designed the system with an asynchronous update mechanism as shown in the diagram below:  </p> <pre><code>sequenceDiagram\n    participant User\n    participant OrderServer\n    participant CreateOrderConsumer\n    participant OrderDB\n    participant OrderAccumulateConsumer\n    User-&gt;&gt;OrderServer: Create Order (API Request)\n    activate OrderServer\n    OrderServer --&gt; CreateOrderConsumer: Send/Receive &lt;Br&gt; Create Order Event\n    OrderServer -&gt;&gt; User: Create Success (API Response)\n    activate CreateOrderConsumer\n    deactivate OrderServer\n    CreateOrderConsumer-&gt;&gt;OrderDB: Save Order Record\n    CreateOrderConsumer-&gt;&gt;OrderDB: Save Order Item 1 Record\n    CreateOrderConsumer-&gt;&gt;OrderDB: Save Order Item 2 Record\n    CreateOrderConsumer--&gt;OrderAccumulateConsumer: Send/Receive &lt;Br&gt; Order Created Event\n    deactivate CreateOrderConsumer\n    activate OrderAccumulateConsumer\n    OrderAccumulateConsumer-&gt;&gt;OrderDB: Query all order Item by order-id &lt;br&gt;(retryable)\n    OrderAccumulateConsumer-&gt;&gt;OrderAccumulateConsumer: Calculate value of all order items\n    deactivate OrderAccumulateConsumer</code></pre> <p>When we apply DB replication to increase the performance and robustness of this process, we direct the OrderAccumulateConsumer queries to the replica DB while inserting data into the main DB from the CreateOrderConsumer. We are aware that the data might not be consistent at some points in time, so we add a retry mechanism for the query step if the order doesn't exist.</p> <p>We thought this would solve all problems perfectly. Unfortunately, there is still one case that might go wrong. Let's zoom in on the interaction between the two consumers and DBs:</p> <pre><code>sequenceDiagram\n    participant CreateOrderConsumer\n    participant OrderDB Main\n    participant OrderDB Replica\n    participant OrderAccumulateConsumer\n    activate CreateOrderConsumer\n    CreateOrderConsumer-&gt;&gt;OrderDB Main: Save Order Record\n    CreateOrderConsumer-&gt;&gt;OrderDB Main: Save Order Item 1 Record\n    CreateOrderConsumer-&gt;&gt;OrderDB Main: Save Order Item 2 Record\n    CreateOrderConsumer--&gt;OrderAccumulateConsumer: Send/Receive &lt;Br&gt; Order Created Event\n    deactivate CreateOrderConsumer\n    activate OrderAccumulateConsumer\n\n    alt success case\n        OrderAccumulateConsumer-&gt;&gt;OrderDB Replica: Query all order Item by order-id &lt;br&gt;(fail)\n        OrderDB Main-&gt;&gt;OrderDB Replica: load binlog (the order)\n        OrderDB Main-&gt;&gt;OrderDB Replica: load binlog (the order item1)\n        OrderDB Main-&gt;&gt;OrderDB Replica: load binlog (the order item2)\n        OrderAccumulateConsumer-&gt;&gt;OrderDB Replica: Query all order Item by order-id &lt;br&gt;(retry success)\n    end\n\n    alt fail case\n        OrderDB Main-&gt;&gt;OrderDB Replica: load binlog (the order)\n        OrderDB Main-&gt;&gt;OrderDB Replica: load binlog (the order item1)\n        OrderAccumulateConsumer-&gt;&gt;OrderDB Replica: Query all order Item by order-id &lt;br&gt;(success)\n        OrderDB Main-&gt;&gt;OrderDB Replica: load binlog (the order item2)\n    end\n\n    OrderAccumulateConsumer-&gt;&gt;OrderAccumulateConsumer: Calculate value of all order items\n    deactivate OrderAccumulateConsumer</code></pre> <p>As we can see, a problematic situation may occur when the replica DB loads the binlog halfway (if we insert data without a transaction initially). The OrderAccumulateConsumer might only retrieve one order item, which will not trigger a retry query, resulting in an incorrect calculation.</p>","tags":["Database","Distributed Systems","Read-Write Separate"]},{"location":"Software%20Engineering%20Blog/2024/05/31/mysql-trap-when-db-read-write-separate-and-parent-child-relationship-schema-without-transaction/#how-to-avoid-incomplete-read-from-replicas","title":"How to Avoid Incomplete Read from Replicas","text":"<p>The reason we fell into this trap is that we initially wrote the insert child table record one by one without using a transaction. When we faced increased loading, we decided to use a Read-Write separate architecture, aware that it might cause data inconsistency in the read part. However, we didn't realize that our coding practices for inserting data also affect how the replica loads data.</p> <p>To step out of the trap, insert all child records within a transaction or in a single statement, ensuring they are not saved separately. If this is difficult, there are still some workarounds, such as adding an extra field in the parent table to denote the expected number of child records and then modifying the retry logic to check if the query result matches this value. or send all the needed data to the second consumer instead of querying from a replica DB.</p>","tags":["Database","Distributed Systems","Read-Write Separate"]},{"location":"Software%20Engineering%20Blog/2024/05/31/mysql-trap-when-db-read-write-separate-and-parent-child-relationship-schema-without-transaction/#summarry","title":"SumMarry","text":"<p>In this article, we first provide a brief overview of how the MySQL replica DB syncs data from the main DB. Second, we introduce a scenario that could cause an incorrect result if a query occurs during partial replication due to inserting data separately. We then identify that the cause is historical technical debt and a lack of comprehensive knowledge of DB replication. Finally, we discuss how to avoid this error by merging multiple inserts into a single statement or using transactions. Additionally, we explore some workarounds to avoid this trap if a single statement or transaction is not feasible in certain cases.</p> <p>Thank you for reading!</p>","tags":["Database","Distributed Systems","Read-Write Separate"]},{"location":"Software%20Engineering%20Blog/2024/05/31/mysql-trap-when-db-read-write-separate-and-parent-child-relationship-schema-without-transaction/#reference","title":"Reference","text":"<p>read-after-write inconsistency: https://avikdas.com/2020/04/13/scalability-concepts-read-after-write-consistency.html replicas binlog format: https://dev.mysql.com/doc/refman/8.0/en/replication-formats.html pros-and-cons of each format: https://dev.mysql.com/doc/refman/8.0/en/replication-sbr-rbr.html row-based replication process: https://dev.mysql.com/doc/refman/8.4/en/replication-rbr-usage.html</p>","tags":["Database","Distributed Systems","Read-Write Separate"]},{"location":"Software%20Engineering%20Blog/2023/06/17/robust-kafka-consumer-error-handling-on-a-spring-boot-3-application/","title":"Robust Kafka Consumer Error Handling on a Spring Boot 3 Application","text":"","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/06/17/robust-kafka-consumer-error-handling-on-a-spring-boot-3-application/#achieving-dead-letter-queue-blocking-and-non-blocking-retry-mechanisms-by-using-retryabletopic-annotation","title":"Achieving dead letter queue, blocking and non-blocking retry mechanisms by using RetryableTopic annotation","text":"<p>In the previous article I shared before, I didn\u2019t show how to implement the error handling in the Kafka consumer for our spring boot application. Since the missing piece is so essential, here I wrote a new article to show how to do the following stuff:</p> <ol> <li>Blocking retry  Do retry when retryable exceptions occur during consuming a message, and block the next message.</li> <li>Non-blocking retry  Send the message to another retry topic, when the message exceeds the blocking retry max attempts limit.</li> <li>Dead letter queue and handler  Send the message to another dead letter topic, when the message exceeds the non-blocking retry max attempts limit or the exception is not a retryable exception.</li> </ol> <p>Before we start, If you want to learn the basic components and concepts of Kafka, How to achieve the desired performance and message guarantee level, please visit my previous article: Get Kafka in prod-ready, 2 decisions to make and 3 implementation details.</p> <p>If you are interested in the coding detail, please refer to the PR in my POC project .</p> <p>Let\u2019s start!</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/06/17/robust-kafka-consumer-error-handling-on-a-spring-boot-3-application/#default-behavior","title":"Default Behavior","text":"<p>Given a simple <code>KafkaListener</code> method (setting as manual commit acknowledge):</p> <pre><code>@KafkaListener(topics = ORDER_TOPIC, groupId = ORDER_STATUS_GROUP_ID_PREFIX + \"#{ T(java.util.UUID).randomUUID().toString() }\")\n@Transactional\npublic void orderEventListener(OrderEvent orderEvent, Acknowledgment ack) {\n    log.info(\"ORDER_TOPIC handler receive data = {}\", orderEvent);\n    try {\n        orderEventRecordHandler.onEvent(orderEvent);\n        orderRecordHandler.onEvent(orderEvent);\n        ack.acknowledge();\n    } catch (Exception e) {\n        log.warn(\"Fail to handle event {}.\", orderEvent);\n        throw e;\n    }\n}\n</code></pre> <p>The default behavior is attempting to consume one massage at most 10 times, then consume the next message and print an error log if it still fails. Please see the <code>org.springframework.kafka.listener.DefaultErrorHandler</code> for details.</p> DefaultErrorHandler.java<pre><code>public class DefaultErrorHandler extends FailedBatchProcessor implements CommonErrorHandler {\n\n   private boolean ackAfterHandle = true;\n\n   /**\n    * Construct an instance with the default recoverer which simply logs the record after\n    * {@value SeekUtils#DEFAULT_MAX_FAILURES} (maxFailures) have occurred for a\n    * topic/partition/offset, with the default back off (9 retries, no delay).\n    */\n   public DefaultErrorHandler() {\n      this(null, SeekUtils.DEFAULT_BACK_OFF);\n   }\n   ...\n}\n</code></pre> <p>and the log is like:</p> <pre><code>2023-06-03T08:57:16.573Z ERROR [order-query-side,,] 1 --- [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] \no.s.kafka.listener.DefaultErrorHandler   : Backoff FixedBackOff {interval=0, currentAttempts=10, maxAttempts=9} exhausted for ORDER-0@0\n</code></pre> <p>After the message is skipped, then the consumer will never process it again. But not skipping this error will make the service stuck at this message which could be unprocessable. So we need to add some non-blocking retry mechanism to get our application more robust under this eventual consistency concept.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/06/17/robust-kafka-consumer-error-handling-on-a-spring-boot-3-application/#non-blocking-retry","title":"Non-Blocking Retry","text":"<p>The easier way to do so is to use the <code>@RetryableTopic</code> (avaliable after springframework.kafka 2.7), comparing to building the retry topic by ourselves and sending messages to it when catch an exception (refer to this commit).</p> <p>With <code>@RetryableTopic</code>, it will build the retry topics for you with the broker default setting. It might create multiple topics if we retry many times and every time will send to a different topic (can be configured with <code>fixedDelayTopicStrategy</code> property), like <code>origintopic-retry-1</code>, <code>origintopic-retry-2</code>\u2026. The whole setting will look like this:</p> <pre><code>@RetryableTopic(kafkaTemplate = \"kafkaTemplate\",\n        attempts = \"4\",\n        backoff = @Backoff(delay = 3000, multiplier = 1.5, maxDelay = 15000)\n)\n@KafkaListener(topics = ORDER_TOPIC, groupId = ORDER_STATUS_GROUP_ID_PREFIX + \"#{ T(java.util.UUID).randomUUID().toString() }\")\n@Transactional\npublic void orderEventListener(@Header(KafkaHeaders.RECEIVED_TOPIC) String receivedTopic,\n                               OrderEvent orderEvent, Acknowledgment ack) throws SocketException {\n    log.info(\"Topic({}) handler receive data = {}\", receivedTopic, orderEvent);\n    try {\n        orderEventRecordHandler.onEvent(orderEvent);\n        if (receivedTopic.contains(\"retry\")) {\n            orderRecordHandler.onRequeueEvent(orderEvent);\n        } else {\n            orderRecordHandler.onEvent(orderEvent);\n        }\n        ack.acknowledge();\n    } catch (Exception e) {\n        log.warn(\"Fail to handle event {}.\", orderEvent);\n        throw e;\n    }\n}\n</code></pre> <p>There are plenty of properties we can set to control the behavior of retry like max attempts, retry interval, retryable exception, retry topic naming strategy, etc. Please refer to the document for features of <code>org.springframework.kafka.annotation.RetryableTopic</code></p> <p>In this way, this KafkaListener method will consume messages from both the original topic and the retry topic. If you really want to distinguish the different logic of the original and retry one, we can get this information from <code>@Header(KafkaHeaders.RECEIVED_TOPIC) String receivedTopic</code>. Using other KafkaHeader can also achieve other use cases.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/06/17/robust-kafka-consumer-error-handling-on-a-spring-boot-3-application/#dead-letter-queue-and-handler","title":"Dead letter queue and handler","text":"<p>In some cases, the message is definitely unprocessable (like parsing error, or invalid properties\u2026). Then we should not waste our resources trying to consume it.</p> <p>we can use the include and exclude properties to control which exception should/should not be retried like:</p> <pre><code>@RetryableTopic(kafkaTemplate = \"kafkaTemplate\",\n        exclude = {DeserializationException.class,\n                MessageConversionException.class,\n                ConversionException.class,\n                MethodArgumentResolutionException.class,\n                NoSuchMethodException.class,\n                ClassCastException.class},\n        attempts = \"4\",\n        backoff = @Backoff(delay = 3000, multiplier = 1.5, maxDelay = 15000)\n)\n</code></pre> <p>And we should write a dead letter handler in the same class of the KafkaListener method like:</p> <pre><code>@DltHandler\npublic void processMessage(OrderEvent message) {\n    log.error(\"DltHandler processMessage = {}\", message);\n}\n</code></pre> <p>then them will work as expected.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/06/17/robust-kafka-consumer-error-handling-on-a-spring-boot-3-application/#blocking-retry","title":"Blocking Retry","text":"<p>Before we send the fail-processed message to the retry topic, we might want to retry a couple of times to save some network round trip. There are plenty of ways to change the default behavior likes:</p> <ol> <li>provide your own <code>@Bean</code> of <code>KafkaListenerErrorHandler</code></li> <li>provide your own <code>@Bean</code> of <code>DefaultErrorHandler</code>    with different <code>ConsumerRecordRecoverer</code> (instead of just printing error logs) and different <code>BackOff</code> settings to customize attempts and retry intervals.</li> <li>When Using <code>@RetryableTopic</code> for methods annotated with KafkaListener, provide a <code>@Configuration</code> class extends <code>RetryTopicConfigurationSupport</code>.</li> </ol> <p>The former 2 ways are not well integrated with a non-blocking retry mechanism, so I recommend the third way to do so.</p> <p>When we have a <code>@RetryableTopic</code> on our KafkaListener like the sample code of the above section, then we just add a configuration class like:</p> KafkaConfig.java<pre><code>@Configuration\n@RequiredArgsConstructor\n@EnableScheduling\n@Slf4j\npublic class KafkaConfig extends RetryTopicConfigurationSupport {\n\n    @Override\n    protected void configureBlockingRetries(BlockingRetriesConfigurer blockingRetries) {\n        blockingRetries\n                .retryOn(IOException.class)\n                .backOff(new FixedBackOff(5000, 3));\n    }\n\n}\n</code></pre> <p>Note that I encounter an error when I first try without <code>@EnableScheduling</code> like the below:</p> <pre><code>Caused by: java.lang.IllegalArgumentException: \nEither a RetryTopicSchedulerWrapper or TaskScheduler bean is required\n</code></pre> <p>And I found this issue in StackOverflow, but I think the better solution is to delegate this implementation detail to the spring framework. So the <code>@EnableScheduling</code> is essential.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/06/17/robust-kafka-consumer-error-handling-on-a-spring-boot-3-application/#summary","title":"Summary","text":"<p>In this article, we address the need for blocking retry, non-blocking retry, and dead letter queue mechanisms. Exploring the implementation of error handling in a Kafka consumer for a Spring Boot application and introducing the RetryableTopic annotation as a solution.</p> <p>I\u2019ve opened the related Pull Request (PR) in my personal repository, feel free to get more details and the complete code here.</p>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/06/17/robust-kafka-consumer-error-handling-on-a-spring-boot-3-application/#reference","title":"Reference","text":"<ol> <li>https://docs.spring.io/spring-kafka/docs/current/reference/html/</li> </ol>","tags":["Kafka","Distributed Systems"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/","title":"Safely Deliver Large Scale Feature Migration With Feature Toggle","text":"<p>Large-scale Change (LSC) within software development often presents massive challenges. It's a scenario that needs careful planning to meet the need for stability. As DevOps philosophy evolves at a rapid pace, seamless implementation, testing, and release of these changes becomes necessary. This is why feature toggles emerge as a valuable tool, offering a variety of strategies to manage, test, and deploy large-scale changes effectively.</p> <p>In this article, we will mainly describe how to develop, test, and release an LSC safely with feature toggles. Starting from the definition of LSC, why it needs help from feature toggle, and what kind of LSC can be covered. Next, we will introduce what kind of toggle we will use, and brief the migration schedule. Finally, we will show what to do with the code and toggle settings during the development, testing, and release stages with an easy demo. Let's begin! </p>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#large-scale-change","title":"Large Scale Change","text":"<p>What's a Large-Scale Change (LSC)? the definition in the book Software Engineering at Google is like, a change that should be a single commit logically, due to some constraints (merge conflict, testing resources, etc.) that turn out to be many separate commits to the codebase. During the project life cycle, we often encounter some of them, which might be the migration from an old API vendor to a new one, upgrade of the used Library, deprecating old paradigms and adopt new ones, this is usually a so-called Large Scale Change.</p> <p>Most of these updates have a large impact, and may also include the Critical User Journey (CUJ) of the system. Since there might be a certain degree of uncertainty in the new version's logic, performance, and implementation details, it can lead to the need to spend more time doing research and testing before gaining enough confidence to deploy to the production environment. Or in the worst case, no one dares to make any changes. So, it's the time for feature toggle to help.</p>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#what-kind-of-lsc-can-be-covered","title":"What kind of LSC can be covered?","text":"<p>First, we should briefly divide the LSC into two categories: logic-level and compiler-level. The main difference is whether we can include both versions of code in a single deployable artifact. For example, a dependency version upgrade in a Java Maven Project is a compiler-level change (i.e. upgrade spring boot2.6 to spring boot 3.1). On the other hand, migrating usage in the codebase from the Google Map API to the other API vendor, or refactoring all String concat to StringBuilder, are both logic-level changes. Therefore, for logic-level change, we could apply some mechanisms using Feature Toggle to help the process easier and with more confidence. For the compiler-level change, we now could only use some hardware-level deployment strategies(blue-green, shadowing) to make it safer.</p>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#how-can-feature-toggle-help-lsc","title":"How can Feature Toggle help LSC?","text":"<p>According to the article, Feature Toggles in MartinFowler.com, we have four kinds of toggle, Release, Permission, Ops, and Experiment. Assuming we need to migrate all features from integrating the API vendor A to integrating the API vendor B. Then, we will use three kinds of toggles to optimize the process of switching API vendors.</p> <p>Release Toggle: Use Release Toggle to ensure that programs using vendor B's ARI will not be executed for all real cases even if the code is merged into the main branch and deployed to production.</p> <pre><code>---\ntitle: Release Toggle\n---\nsequenceDiagram\n    actor A as Client\n    participant B as Server\n    participant C as VendorA\n    participant D as VendorB\n    A-&gt;&gt;B: request\n    activate B\n    alt toggle is off\n        B-&gt;&gt;C: request\n    else toggle is on\n        B-&gt;&gt;D: request\n    end    \n    B-&gt;&gt;A: response\n    deactivate B</code></pre> <p>Permission Toggle: At the same time, with a Permission Toggle. Testers can test by a specific user (i.e. test account) whether the features integrated with vendor B's API work well.</p> <pre><code>---\ntitle: Permission Toggle\n---\nsequenceDiagram\n    actor A as Client\n    participant B as Server\n    participant C as VendorA\n    participant D as VendorB\n    A-&gt;&gt;B: request &lt;br&gt;(with a property&lt;br&gt; named \"userId\")\n    activate B\n    alt userId != \"tester\"\n        B-&gt;&gt;C: request\n    else userId = \"tester\"\n        B-&gt;&gt;D: request\n    end    \n    B-&gt;&gt;A: response\n    deactivate B</code></pre> <p>Ops Toggle: Use Ops Toggle to implement Canary Release. Ensuring that after all functions are completed, it will be available to a low percentage of real users at first. If there are no problems, increase the percentage of users using the new function.</p> <pre><code>---\ntitle: Ops Toggle\n---\nsequenceDiagram\n    actor A as Client\n    participant B as Server\n    participant C as VendorA\n    participant D as VendorB\n    A-&gt;&gt;B: request &lt;br&gt;(with a property&lt;br&gt; named \"userId\")\n    activate B\n    alt false = isRollout(userId)\n        B-&gt;&gt;C: request\n    else true = isRollout(userId)\n        B-&gt;&gt;D: request\n    end    \n    B-&gt;&gt;A: response\n    deactivate B</code></pre>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#migration-and-toggle-schedule","title":"Migration and Toggle Schedule","text":"<p>With the three toggles above, we can say that the flow of an API vendor migration contains the following stages: First PR Merged, Start Testing, Dev Completed, Production Test Complete, Production Stable, and Remove Toggle Code. Then we can activate/deactivate the toggles to safely release the feature migration. The schedule will be like below:</p> <p></p> <p>Here are some notable key points (given we have a <code>fooService</code> that will use both<code>vendorAStrategy</code> integrated with vendor A API, and <code>vendorBStrategy</code> integrated with vendor B\u2019s API.):</p> <ol> <li> <p>After merging the first PR containing vendor B strategy, we should use a release toggle to ensure all real traffic goes to <code>vendorAStrategy</code> and no real user is affected.</p> </li> <li> <p>When testers want to test the feature integrated with vendor B\u2019s API, we should set up a permissions toggle to make a specific user will trigger the <code>vendorBStrategy</code>.</p> </li> <li> <p>For the regression or automatic test, we should add a permissions toggle to let a second user which will always trigger the vendor A strategy. That can make sure the vendor A strategy is working fine.</p> </li> <li> <p>After all features in <code>vendorAStrategy</code> are completed and tested. We will change the release toggle to an Ops toggle to perform a canary release.</p> </li> <li> <p>After the production runs stably, we should clean up the code containing the toggle logic to keep the codebase simple.</p> </li> </ol>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#implementation-and-demo","title":"Implementation and Demo","text":"<p>In this section, we will show how I implement the strategy pattern to solve the multi-vendors logic, and with toggles to help the strategies switch in runtime to carry out the above migration plan. (You can get more details by checking my GitHub repository commit for this article)</p>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#codes-before-change","title":"Codes Before Change","text":"<p>First, we have the base service named <code>FooService</code>, which would call a <code>VendorServiceFactory</code> to get the correct strategy to execute each step's method.</p> FooService.java<pre><code>@Service\n@RequiredArgsConstructor\npublic class FooService {\n\n  private final VendorServiceFactory vendorServiceFactory;\n\n  public String step1(RequestModel request) {\n    VendorService vendorService = vendorServiceFactory.findTargetVendor(request);\n    try {\n      return vendorService.step1() + \" -&gt; \";\n    } catch (Exception e) {\n      return \"X -&gt; \";\n    }\n  }\n  ... other method (e.g. step2, step3).\n}\n</code></pre> <p>Then, we need to introduce the interface <code>VendorService</code>, which has three methods, step1, step2, and step3.</p> VendorService.java<pre><code>public interface VendorService {\n\n    boolean accept(RequestModel request);\n\n    String step1();\n\n    String step2();\n\n    String step3();\n\n}\n</code></pre> <p>For, the <code>VendorServiceFactory</code> we will inject all class that implements the VendorService interface. When every time we need to find the target vendor, we loop all of them to see whether each VendorStrategy implementation can handle the request. (Noted: the order in the list becomes important when there is any overlay in the <code>accept</code> logic between different strategies. please refer to this article to handle the order issue).</p> VendorServiceFactory.java<pre><code>@Service\n@RequiredArgsConstructor\npublic class VendorServiceFactory {\n\n    private final List&lt;VendorService&gt; vendorServices;\n\n    VendorService findTargetVendor(RequestModel request) {\n        for (VendorService strategy : vendorServices) {\n            if (strategy.accept(request)) {\n                return strategy;\n            }\n        }\n        throw new RuntimeException(\"no match strategy\");\n    }\n\n}\n</code></pre> <p>For the implementation of VendorService, we take <code>VendorSV1ServiceImpl</code> (given this is to handle the VIP users) for example,</p> VendorSV1ServiceImpl.java<pre><code>@Service\npublic class VendorSV1ServiceImpl implements VendorService{\n    @Override\n    public boolean accept(RequestModel request) {\n        return request.vendor().isVip();\n    }\n\n    @Override\n    public String step1() {\n        return PURPLE + \"S1\" + RESET;\n    }\n\n    @Override\n    public String step2() {\n        return PURPLE + \"S2\" + RESET;\n    }\n\n    @Override\n    public String step3() {\n        return PURPLE + \"S3\" + RESET;\n    }\n}\n</code></pre> <p>And for the main character of this article, the <code>VendorAV1ServiceImpl</code> looked like this before making any change, and will take care of normal users for the <code>FooService</code>.</p> VendorAV1ServiceImpl.java<pre><code>@Service\n@RequiredArgsConstructor\npublic class VendorAV1ServiceImpl implements VendorService {\n\n    private final FeatureToggleApiProvider featureToggleApiProvider;\n\n    @Override\n    public boolean accept(RequestModel request) {\n        return request.vendor().isNormal();\n    }\n\n    @Override\n    public String step1() {\n        return BLUE + \"A1\" + RESET;\n    }\n\n    @Override\n    public String step2() {\n        return BLUE + \"A2\" + RESET;\n    }\n\n    @Override\n    public String step3() {\n        return BLUE + \"A3\" + RESET;\n    }\n\n}\n</code></pre>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#code-change-toggle-settings","title":"Code Change &amp; Toggle Settings","text":"<p>Here, we will show the code changes in the old <code>VendorAV1ServiceImpl</code> and the new strategy <code>VendorBV1ServiceImpl</code>  during the migration process. They both serve the \"Normal\" user for <code>FooService</code>. Let's start!</p>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#first-pr-merged","title":"First PR Merged","text":"<p>In this stage, We first modify the <code>accept</code> method of <code>VendorAV1ServiceImpl</code> to below:</p> VendorAV1ServiceImpl.java<pre><code>public class VendorAV1ServiceImpl implements VendorService {\n\n  private final FeatureToggleApiProvider featureToggleApiProvider;\n\n  @Override\n  public boolean accept(RequestModel request) {\n    Client client = featureToggleApiProvider.getFlagrApiClient();\n    MutableContext evaluationContext = new MutableContext(request.userId());\n    evaluationContext.add(USER_ID_KEY, request.userId());\n\n    boolean isToggleOn = client.getBooleanValue(FLAG_KEY, false, evaluationContext);\n\n    return !isToggleOn &amp;&amp; request.vendor().isNormal();\n  }\n  ... // other methods\n}\n</code></pre> <p>also, we implement the VendorBV1ServiceImpl similar to VendorAV1ServiceImpl except the <code>accept</code> logic is only when isToggleOn is true and the user is normal. Here, given the logic of each step is complex in this strategy, we can just implement the step1 and release this version to production like the below:</p> VendorBV1ServiceImpl.java<pre><code>@Service\n@RequiredArgsConstructor\npublic class VendorBV1ServiceImpl implements VendorService {\n\n    private final FeatureToggleApiProvider featureToggleApiProvider;\n\n    @Override\n    public boolean accept(RequestModel request) {\n        Client client = featureToggleApiProvider.getFlagrApiClient();\n        MutableContext evaluationContext = new MutableContext(request.userId());\n        evaluationContext.add(USER_ID_KEY, request.userId());\n\n        boolean isToggleOn = client.getBooleanValue(FLAG_KEY, false, evaluationContext);\n\n        return isToggleOn &amp;&amp; request.vendor().isNormal();\n    }\n\n    @Override\n    public String step1() {\n        return GREEN + \"B1\" + RESET;\n    }\n\n    @Override\n    public String step2() {\n        throw new UnsupportedOperationException(\"not implements yes\");\n    }\n\n    @Override\n    public String step3() {\n        throw new UnsupportedOperationException(\"not implements yes\");\n    }\n\n}\n</code></pre> <p>Meanwhile, we use the toggle system (here we use OpenFlagr) to set up a toggle that always returns off. </p> <p></p> <p>Then, we can make 50 users (<code>id % 10 == 0</code> is the VIP users, which would print a purple string) call all the steps in <code>fooService</code> and all the normal users would go to use <code>vendorAV1Strategy</code>, which prints a blue string in the console. </p>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#start-testing","title":"Start Testing","text":"<p>When the Testers want to test if the <code>VendorBV1ServiceImpl</code> is working well, we won't need to change any code but only add a permission toggle that enables <code>VendorBV1ServiceImpl</code> for some specific pilot users (use userId = 1 for example). Noted this toggle should be evaluated before the release toggle.</p> <p></p> <p>Then we run the demo program, we can see that it prints a green <code>B1</code> followed by two white <code>X</code> (only step1 is implemented) on the console for the user1 while others remain the same.</p> <p></p>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#dev-completed","title":"Dev Completed","text":"<p>During this stage, we will implement the step2 and step3 in the <code>VendorBV1ServiceImpl</code>, so we can see a complete green string for user1.</p> <p></p>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#production-test-complete","title":"Production Test Complete","text":"<p>The same toggle setting can be set on the toggle system in production, for testers to perform a pilot test. In this stage, it's important to make sure that real users won't execute <code>VendorBV1ServiceImpl</code> methods. For the automation test and regression test, we might also specific users to always run <code>VendorAV1ServiceImpl</code>.</p> <p></p> <p>After the test is complete and the bugs are fixed, we can start the canary release by removing the release toggle and turning it to an ops Toggle (it should still be evaluated after all permission toggles).</p>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#production-stable","title":"Production Stable","text":"<p>First, we set the rollout percentage to 25%, </p> <p>if there is no problem, then increase to 50%, 75%, 100%. (get more detail about performing canary release by feature toggles in my previous article) </p> <p></p> <p>After keeping the canary ops toggle as 100% rollout for a while, we can say that the <code>VendorBV1ServiceImpl</code> is stable enough.</p>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#remove-toggle-code","title":"Remove Toggle Code","text":"<p>After the <code>VendorBV1ServiceImpl</code> is stable enough, we can start to remove the related code to keep the codebase clean. First, we can remove the whole <code>VendorAV1ServiceImpl.java</code> file, and then modify the <code>VendorBV1ServiceImpl</code> like this: </p> <p></p> <p>After this version is deployed in production, we can also remove the toggle setting in the toggle system.</p>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#summary","title":"Summary","text":"<p>In this article, we propose a migration schedule with three kinds of toggles to manage from development to test and release. Finally, we can see the power of the feature toggle in making an LSC easier and more flexible. One notable thing is that once the feature development work is complete, we won't need to change any code but still can manage the feature execution for test and canary release.</p>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/12/21/safely-deliver-large-scale-feature-migration-with-feature-toggle/#reference","title":"Reference","text":"<ul> <li>Large-Scale Changes from Software Engineering at Google, by Lisa Carey, in O'REILLY</li> <li>Feature Toggles, by Pete Hodgson, in MartinFowler.com</li> <li>Spring ordered list of beans, in stackoverflow</li> </ul>","tags":["System Design","Devops","Feature Toggle","Deployment Strategy","Trunk-Based"]},{"location":"Software%20Engineering%20Blog/2023/04/04/say-goodbye-to-meaningless-code-for-controller-and-service-with-spring-data-rest/","title":"Say Goodbye to meaningless code for Controller and Service with Spring-Data-Rest","text":"<p>In this article, we will introduce a convenient library, Spring-Data-Rest to eliminate your meaningless code in a Spring Boot Application. As we all know, a DB-access API will be implemented as a Controller-Service-Repository stack in a Spring Boot application coding convention. Often, there is no business logic in controllers and services, but only call the next component and return the result. It\u2019s exactly the meaningless code. The Spring-Data-Rest can help us to eliminate it elegantly.</p> <p>After applying the Spring-Data-Rest, we can find that the API response is formatted under the constraint of Hypermedia as the Engine of Application State (HATEOAS). There are a lot of properties that are returned with the original DB entity.</p> <p>So we have to integrate Spring-HATEOAS to wrap the API-client component, then we can provide an SDK module that can be used to call the Spring-Data-Rest API endpoint with minimum effort.</p> <p>The followings are what we will cover:</p> <ol> <li>Integrate Spring-Data-Rest to replace DB access APIs in a Spring Boot App</li> <li>Customize the exposed endpoint as a CQRS query-side server</li> <li>Integrate Spring-HATEOAS to build an API client module for the DB-access APIs</li> </ol> <p>Let\u2019s begin!</p>","tags":["Java","OpenFeign","Refactoring"]},{"location":"Software%20Engineering%20Blog/2023/04/04/say-goodbye-to-meaningless-code-for-controller-and-service-with-spring-data-rest/#1-integrate-spring-data-rest","title":"1. Integrate Spring-Data-Rest","text":"<p>Integrating Spring-Data-Rest in a Spring Boot application is a very easy thing to do as the official spring-boot-starter-data-rest exists.</p> <p>so we can add the following dependency into the <code>gradle.build</code>:</p> build.gradle<pre><code>dependencies {\n    ...\n    implementation 'org.springframework.boot:spring-boot-starter-data-rest'\n    ...\n}\n</code></pre> <p>then provide an entity class and the corresponding JPA-repository interface as <code>OrderRecord.java</code>:</p> OrderRecord.java<pre><code>@Entity\n@Table(name = \"ORDER_RECORD\")\n\n@Data\n@AllArgsConstructor\n@NoArgsConstructor\n@Builder\npublic class OrderRecord {\n\n    @Id\n    private String orderId;\n\n    private OrderStatus status;\n\n    private Instant createdDate;\n\n    private Instant updatedDate;\n\n}\n</code></pre> <p>and <code>OrderRepository.java</code>:</p> <p>OrderRepository.java<pre><code>public interface OrderRepository extends JpaRepository&lt;OrderRecord, String&gt; {\n}\n</code></pre> then we can start the application and see the magic happen:</p> <p></p> <p>we can try the API as below:</p> <ul> <li>profile API</li> </ul> <p></p> <ul> <li>post entity</li> </ul> <p></p>","tags":["Java","OpenFeign","Refactoring"]},{"location":"Software%20Engineering%20Blog/2023/04/04/say-goodbye-to-meaningless-code-for-controller-and-service-with-spring-data-rest/#2-customize-endpoint","title":"2. Customize Endpoint","text":"<p>The Spring-Data-Rest default setting exposes all the endpoints (POST, GET, PUT, DELETE \u2026) on the path the same as the entity name i.e. POST <code>http://localhost:8083/orderRecords</code>. Sometimes, we may want to</p> <ol> <li>hide some operations,</li> <li>expose some specific search API</li> <li>change the path of APIs</li> </ol> <p>Here we will introduce how to do the mentioned customization, there are lots of other configurations that can be found in the official document.</p>","tags":["Java","OpenFeign","Refactoring"]},{"location":"Software%20Engineering%20Blog/2023/04/04/say-goodbye-to-meaningless-code-for-controller-and-service-with-spring-data-rest/#hide-some-operations","title":"Hide some operations","text":"<p>In some cases, i.g. a pure query side server in a CQRS system, will only need to expose the GET API and hide the other operation APIs. There are two ways to do that and related to some strategies to set by properties file (or .yaml) <code>spring.data.rest.detection-strategy</code>.</p> <ol> <li>Use the <code>DEFAULT</code> strategy (no need to set properties), and add <code>@RepositoryRestResource(exported = false)</code> /<code>@RestResource(exported = false)</code> to the repositories/ methods that don\u2019t need to be exposed.</li> <li>Use the <code>ANNOTATED</code> strategy and add <code>@RepositoryRestResource</code> to the entity that wants to be exposed, and also add <code>@RestResource(exported = false)</code> to methods in the repository that don\u2019t need to be exposed. There are two strategies, <code>ALL</code> and <code>VISIBILITY</code> which I think are not so useful, so we skip them here.</li> </ol> <p>In my case, I want to disable the create, update, and delete operations for my API, so I just add the annotation to my repository and override the basic method as the documents suggest.</p> OrderRepository.java<pre><code>public interface OrderRepository extends JpaRepository&lt;OrderRecord, String&gt; {\n\n    @Override\n    @RestResource(exported = false)\n    void deleteById(String id);\n\n    @Override\n    @RestResource(exported = false)\n    OrderRecord save(OrderRecord orderRecord);\n\n}\n</code></pre> <p>In this way, we can disable the API endpoint that we don\u2019t need.</p> <p></p>","tags":["Java","OpenFeign","Refactoring"]},{"location":"Software%20Engineering%20Blog/2023/04/04/say-goodbye-to-meaningless-code-for-controller-and-service-with-spring-data-rest/#specific-search-api","title":"Specific search API","text":"<p>Sometimes, we will need some APIs that access the data by other fields instead of the primary key (PK). Then we need to implement some APIs like a search API. In Spring-Date-Rest, that is also covered. we can write a method in the repository class:</p> OrderEventRepository.java<pre><code>public interface OrderEventRepository extends JpaRepository&lt;OrderEventRecord, Long&gt; {\n...\nList&lt;OrderEventRecord&gt; findByOrderId(String orderId);\n...\n}\n</code></pre> <p>and the corresponding API will be generated (a GET API and the method arguments become query parameters in the request):</p> <p></p>","tags":["Java","OpenFeign","Refactoring"]},{"location":"Software%20Engineering%20Blog/2023/04/04/say-goodbye-to-meaningless-code-for-controller-and-service-with-spring-data-rest/#change-the-path-of-apis","title":"Change the path of APIs","text":"<p>The first thing we can do is change the base path of all API exposed by Spring-Date-Rest. The easiest way is to set it in the properties file (<code>.yaml</code> or <code>.properties</code>) like below:</p> application.yaml<pre><code>spring:\n  data:\n    rest:\n      basePath: /api\n</code></pre> <p>or  application.properties<pre><code>spring.data.rest.basePath=/api\n</code></pre></p> <p>or we can do it in a configuration class as the document do.</p> <p>The second config is to change the path from the entity class name to our preference. The library provides an annotation <code>@RepositoryRestResource</code> to override all the API-path in the repository like below:</p> OrderRepository.java<pre><code>@RepositoryRestResource(path = \"v1-orders\", collectionResourceRel = \"v1-orders\", itemResourceRel = \"v1-orders\")\npublic interface OrderRepository extends JpaRepository&lt;OrderRecord, String&gt; {\n...\n}\n</code></pre> <p>As we can see in default, the search method will be exposed and the path of the API will be the same as the method name. we can change it by and the property in the annotation RestResource like below:</p> OrderEventRepository.java<pre><code>@RepositoryRestResource(path = \"v1-orders-log\", collectionResourceRel = \"v1-orders-log\", itemResourceRel = \"v1-orders-log\")\npublic interface OrderEventRepository extends JpaRepository&lt;OrderEventRecord, Long&gt; {\n    ...\n    @RestResource(path = \"order-id\")\n    List&lt;OrderEventRecord&gt; findByOrderId(String orderId);\n    ...\n}\n</code></pre> <p></p> <p>One found limitation is that the path in both <code>@RepositoryRestResource</code> and <code>@RestResource</code> is only supporting one segment (can contain <code>-</code>, <code>_</code>). In other words, we can only expose APIs on <code>some/base/api-path/repository-path/method-path</code>.</p>","tags":["Java","OpenFeign","Refactoring"]},{"location":"Software%20Engineering%20Blog/2023/04/04/say-goodbye-to-meaningless-code-for-controller-and-service-with-spring-data-rest/#3-client-module-for-spring-data-rest-api","title":"3. Client Module for Spring-Data-Rest API","text":"<p>Building an API client module is an important key to reducing efforts in a microservice system since there are a lot of scenarios in that servers will need data from other services.</p> <p>Since the APIs of Spring-Date-Rest are supported by the HATEOAS format, we have to add the related dependency:</p> build.gradle<pre><code>dependencies {\n    ...\n    implementation 'org.springframework.boot:spring-boot-starter-hateoas'\n    ...\n}\n</code></pre> <p>And add the annotation <code>@EnableHypermediaSupport</code> to enable it on the client config class:</p> OrderQueryClientConfig.java<pre><code>@AutoConfiguration\n@EnableHypermediaSupport(type = EnableHypermediaSupport.HypermediaType.HAL)\npublic class OrderQueryClientConfig {\n\n// @Observed, ObservationRegistry, MeterRegistry, MicrometerObservationCapability\n// MicrometerCapability is for Observability. can ignore if you didn't use it \n    @Bean\n    @Observed\n    public OrderQueryClient orderQueryClient(ObservationRegistry observationRegistry, MeterRegistry meterRegistry) {\n        Feign.Builder builder = Feign.builder()\n                .logLevel(Logger.Level.FULL)\n                .logger(new Slf4jLogger())\n                .encoder(new JacksonEncoder(List.of(new JavaTimeModule())))\n                .decoder(new JacksonDecoder(List.of(new JavaTimeModule())))\n                .addCapability(new MicrometerObservationCapability(observationRegistry))\n                .addCapability(new MicrometerCapability(meterRegistry));\n        return new OrderQueryClient(builder);\n    }\n\n}\n</code></pre> <p>and then we can implement the client class (the model class can be found here, I will skip their code) :</p> OrderQueryClient.java<pre><code>public class OrderQueryClient {\n\n    private final OrderQueryStub orderQueryStub;\n\n    public OrderQueryClient(Feign.Builder builder) {\n        OrderQueryStub feign = builder.target(OrderQueryStub.class, \"http://localhost:8083\");\n        this.orderQueryStub = feign;\n    }\n\n    public V1Order get(String id) {\n        return orderQueryStub.get(id).getContent();\n    }\n\n    private interface OrderQueryStub {\n\n        String BASE_PATH = \"/api/v1-orders\";\n\n        @RequestLine(\"GET \" + BASE_PATH + \"/{id}\")\n        @Headers(\"Content-Type: application/json\")\n        EntityModel&lt;V1Order&gt; get(@Param(\"id\") String id);\n\n    }\n\n}\n</code></pre> <p>In my case, I use OpenFeign to build my API client. the differences between pure Json OpenFeign clients are:</p> <p>The returns class type should be wrapped by EntityModel (or CollectionModel for List, Map, etc. reference) Should call getContent() to use the entity data. Here we use the field variable <code>OrderQueryStub</code> to encapsulate the Feign client and hide the complex response format and only return the entity in normal usage. It\u2019s a simple way, but ignoring the HATEOAS format, I might need a little more research on how to use it elegantly.</p> <p>In this way, the client can be used by other modules. But if you\u2019re interested in how to make it easier to use, please refer to my article on how to auto-configuration Spring Boot component. Besides, there is also an issue I encounter when consuming <code>java.time.Instant</code>:</p> <ul> <li>https://stackoverflow.com/questions/55028766/feign-jackson-datetime-jsonmappingexception</li> <li>https://stackoverflow.com/questions/74974924/how-to-deserialize-java-time-instant-in-jackson2jsonredisserializer</li> </ul> <p>the final usage in other modules will be like below:</p> OrderService<pre><code>@Service\n@RequiredArgsConstructor\n@Slf4j\n@LogInfo\npublic class OrderService {\n\n    private final OrderQueryClient orderQueryClient;\n    private final OrderEventProducer orderEventProducer;\n\n    public String completeOrder(String id) {\n        V1Order result = orderQueryClient.get(id);\n        if (result.status() == V1OrderStatus.CREATED) {\n            orderEventProducer.create(new OrderEvent(id, COMPLETED, Instant.now()));\n            return \"OK\";\n        } else {\n            throw new RuntimeException(\"order(id = {}) is not in right status.\");\n        }\n    }\n\n}\n</code></pre>","tags":["Java","OpenFeign","Refactoring"]},{"location":"Software%20Engineering%20Blog/2023/04/04/say-goodbye-to-meaningless-code-for-controller-and-service-with-spring-data-rest/#summary","title":"Summary","text":"<p>This article introduces Spring-Data-Rest, a library that can help eliminate meaningless code, such as Controller and Service, for a pure DB access API in a Spring Boot application. It also covers how to customize the exposed endpoints, including hiding some operations, exposing specific search APIs, and changing the path of APIs. These customizations can be achieved by adding annotations and setting properties in the properties file. The article also explains how to integrate Spring-HATEOAS to wrap the API client and provide other microservices with an easier way to use those APIs.</p> <p>I\u2019ve opened the related Pull Request (PR) in my personal repository, feel free to get more details and the complete code here.</p>","tags":["Java","OpenFeign","Refactoring"]},{"location":"Software%20Engineering%20Blog/2023/04/04/say-goodbye-to-meaningless-code-for-controller-and-service-with-spring-data-rest/#reference","title":"Reference","text":"<ul> <li>https://docs.spring.io/spring-data/rest/docs/current/reference/html/</li> <li>https://docs.spring.io/spring-hateoas/docs/current/reference/html/</li> <li>https://stackoverflow.com/questions/30396953/how-to-customize-spring-data-rest-to-use-a-multi-segment-path-for-a-repository-r</li> <li>https://stackoverflow.com/questions/25352764/hateoas-methods-not-found</li> </ul>","tags":["Java","OpenFeign","Refactoring"]},{"location":"Software%20Engineering%20Blog/2023/02/28/spring-boot-3-observability-monitor-application-on-the-method-level/","title":"Spring Boot 3 Observability: monitor Application on the method level","text":"","tags":["Observability","Java"]},{"location":"Software%20Engineering%20Blog/2023/02/28/spring-boot-3-observability-monitor-application-on-the-method-level/#observe-using-observedaspect-with-logging-method-arguments","title":"Observe using ObservedAspect with logging method arguments","text":"<p>Observability is one of the significant improvements in the Spring Boot 3 (also see how I migrate to spring boot 3), they now provide an effortless, pluggable way to use annotation to observe all the logs, trace, and metrics data for our spring beans.</p> <p>In this article, we will go through below topics:</p> <ul> <li>Recap on how observability works</li> <li>Run monitoring component (Grafana, Loki, Tempo, Prometheus) by docker-compose</li> <li>How to implement code to observe the Application</li> <li>How to log method arguments in ObservedAspect</li> <li>How would data be displayed on the Grafana</li> </ul> <p>This Article is based on the guide posted in the Spring Blog, adding some of my opinion and how I solve some topics that were not covered in the original post. Please refer to the code change on my GitHub project PR. Let\u2019s begin!</p>","tags":["Observability","Java"]},{"location":"Software%20Engineering%20Blog/2023/02/28/spring-boot-3-observability-monitor-application-on-the-method-level/#recap-on-how-observability-works","title":"Recap on how observability works","text":"<p>First, let\u2019s do a quick recap on observability. We have four components in a normal architecture:</p> <ol> <li>Log aggregation system     e.g. Loki, Fluentd, Logstash\u2026 (reference).</li> <li>Distributed tracing backend     e.g. Tempo, Jaeger, Zipkin\u2026 (reference).</li> <li>Time series metrics and monitoring system/database  Prometheus is kind of dominant, but there are still some alternatives.</li> <li>Data query, visualize, alerting platform     e.g. Grafana, kibana... (see more alternatives)</li> </ol> <p>And the whole architected processed like below:</p> <ol> <li>Applications keep producing logs while running </li> <li>Logs are sent to or pulled from the Log aggregation system </li> <li>Data of trace and span are sent to the Distributed tracing backend </li> <li>Prometheus will scrape metrics from Applications periodically. (In some cases we can also push metrics to Prometheus. how to)</li> <li>Grafan provides a GUI for us to easily access data in others components. Then display data in a dashboard, check alerting rule is matched, and respond to our exploring query, etc.</li> </ol>","tags":["Observability","Java"]},{"location":"Software%20Engineering%20Blog/2023/02/28/spring-boot-3-observability-monitor-application-on-the-method-level/#run-monitoring-components-by-docker-compose","title":"Run monitoring components by docker-compose","text":"<p>I basically follow the <code>docker-compose.yml</code> mentioned in this post. And then add the dependency config files under this folder. Some notable things are:</p> <ul> <li>We can see the Prometheus scrape metrics form itself with the default metrics path, <code>/metrics</code> like below (see more in the official config document). This is because we can also monitor the status of Prometheus servers.</li> </ul> prometheus/prometheus.yml<pre><code>scrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets:\n        - 'localhost:9090'\n</code></pre> <ul> <li>We have to change the scrape setting for our application. Since we have set <code>extra_hosts: [\u2018host.docker.internal:host-gateway\u2019]</code> in the <code>docker-compose.yml</code> , The Prometheus use <code>host.docker.internal</code> to access the application either run on localhost or docker (need to expose port).</li> </ul> prometheus/prometheus.yml<pre><code>scrape_configs:\n  ...\n  - job_name: 'cluster-api'\n  metrics_path: '/actuator/prometheus'\n  static_configs:\n    - targets: [ 'host.docker.internal:8081' ]\n      labels:\n        namespace: 'event-sourcing'\n        app: 'order-command'\n</code></pre> <ul> <li>We can download many dashboard template JSON files from Grafana Labs. Then put it in the folder that will be mounted in Prometheus by setting volume in <code>docker-compose.yml</code>. This is because we set the following to provide dashboards to Grafana.</li> </ul> grafana/provisioning/dashboards/dashboard.yml<pre><code>providers:\n  - name: dashboards\n    type: file\n    disableDeletion: true\n    editable: true\n    options:\n      path: /etc/grafana/provisioning/dashboards\n      foldersFromFilesStructure: true\n</code></pre> <p>After preparing all the config files, we can finally use docker-compose to start all the components (Grafana, Loki, Tempo, Prometheus): <pre><code>docker compose -f observe-docker-compose.yaml -p observability up\n</code></pre></p> <p>Then we can see them running well in the docker dashboard (see my commit for all the files in detail).</p> <p></p>","tags":["Observability","Java"]},{"location":"Software%20Engineering%20Blog/2023/02/28/spring-boot-3-observability-monitor-application-on-the-method-level/#how-to-implement-code-to-observe-the-application","title":"How to implement code to observe the Application","text":"<p>Next, we are going to make our application expose metrics and send out traceable logs.</p>","tags":["Observability","Java"]},{"location":"Software%20Engineering%20Blog/2023/02/28/spring-boot-3-observability-monitor-application-on-the-method-level/#dependencies-and-configs","title":"Dependencies and Configs","text":"<p>First, add the following dependency:</p> build.gradle<pre><code>dependencies {\n // using new @Observed on class and enaabled @ObservedAspect\n implementation \"org.springframework.boot:spring-boot-starter-aop\"\n // enabled endpoint and expose metrics\n implementation \"org.springframework.boot:spring-boot-starter-actuator\"\n implementation \"io.micrometer:micrometer-registry-prometheus\"\n // handleing lifecycle of a span\n implementation \"io.micrometer:micrometer-tracing-bridge-brave\"\n // send span and trace data \n // endpoint is default to \"http://locahost:9411/api/v2/spans\" by actuator\n // we could setting by management.zipkin.tracing.endpoint \n implementation \"io.zipkin.reporter2:zipkin-reporter-brave\"\n // send logs by log Appender through URL\n implementation \"com.github.loki4j:loki-logback-appender:1.4.0-rc2\"\n }\n</code></pre> <p>Second, add an <code>application.yaml</code> in the project resources to set up the related properties:</p> application.yaml<pre><code>management:\n  tracing:\n    sampling:\n      probability: 1.0 # sampling all in dev, reduce it in prod to save loading\n  endpoints:\n    web:\n      exposure:\n        include: prometheus\n  metrics:\n    distribution:\n      percentiles-histogram:\n        http:\n          server:\n            requests: true\n\nlogging:\n  pattern:\n    level: \"%5p [${spring.application.name:},%X{traceId:-},%X{spanId:-}]\"\n</code></pre> <p>Third, setting the log appender to auto-send logs to the Loki server.</p> <p>``` xml title=\"logback.xml </p> <p> <pre><code>&lt;appender name=\"LOKI\" class=\"com.github.loki4j.logback.Loki4jAppender\"&gt;\n    &lt;http&gt;\n        &lt;url&gt;http://${LOKI_HOST:-localhost}:3100/loki/api/v1/push&lt;/url&gt;\n    &lt;/http&gt;\n    &lt;format&gt;\n        &lt;label&gt;\n            &lt;pattern&gt;app=${appName},host=${HOSTNAME},traceID=%X{traceId:-NONE},level=%level&lt;/pattern&gt;\n        &lt;/label&gt;\n        &lt;message&gt;\n            &lt;pattern&gt;${FILE_LOG_PATTERN}&lt;/pattern&gt;\n        &lt;/message&gt;\n        &lt;sortByTime&gt;true&lt;/sortByTime&gt;\n    &lt;/format&gt;\n&lt;/appender&gt;\n\n&lt;root level=\"INFO\"&gt;\n    &lt;appender-ref ref=\"LOKI\"/&gt;\n&lt;/root&gt;\n&lt;logger name=\"feign\" level=\"DEBUG\"/&gt; &lt;!-- you can set your own level rule --&gt;\n</code></pre> <p> ``` </p>","tags":["Observability","Java"]},{"location":"Software%20Engineering%20Blog/2023/02/28/spring-boot-3-observability-monitor-application-on-the-method-level/#code-implementation","title":"Code Implementation","text":"<p>First, add a configuration to enable spring scan for the annotation <code>@Observed</code> , which will make the class/method also handled by the related class of Zipkin and Prometheus to send trace data or prepare metrics data.  <code>java title=\"ObserveConfiguration.java\" import io.micrometer.observation.ObservationRegistry; import io.micrometer.observation.aop.ObservedAspect; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration;  @Configuration(proxyBeanMethods = false) public class ObserveConfiguration {      @Bean     ObservedAspect observedAspect(ObservationRegistry observationRegistry) {         return new ObservedAspect(observationRegistry);     }  }</code></p> <p>Then, we need to add <code>@Observedon</code> the classes/methods, which we consider as a new span start. Also, we can add some logs in the method so that it can be found by Loki, not only the trace and span data in Tempo. Besides, the class didn\u2019t annotate with <code>@Observed</code>, The logs produced by them will also have the <code>traceId</code> and <code>spanId</code> as we defined in the log pattern but stay in the same span.</p> OrderV2Controller.java<pre><code>import io.micrometer.observation.annotation.Observed;\n...\n\n@RestController\n@RequiredArgsConstructor\n@RequestMapping(value = \"api/v2/orders\")\n@Slf4j\n@Observed\npublic class OrderV2Controller {\n\n    private final OrderService orderService;\n\n    @PostMapping\n    @ResponseStatus(HttpStatus.OK)\n    public Order createOrder(@RequestBody Order order) {\n        log.info(\"recieve create order command, order = {}.\", order);\n        return orderService.createOrder(order);\n    }\n\n}\n</code></pre> <p>Basically, observability is enough for application with the above setting and implementation. But I encounter some issues:</p> <ul> <li>The self-config <code>OpenFeignClient</code> (not auto-configure by <code>spring-cloud-starter-feign</code>) won\u2019t send the <code>traceId</code> with the request to the target server, so the trace will be lost. (Solved)</li> <li>The asynchronous methods (i.e. CompletableFuture) don\u2019t have both the <code>traceId</code> &amp; <code>spanId</code>. (Didn\u2019t solve)</li> </ul> <p>The solution for OpenFeignClient is very easy, we only need to add two Capability while constructing the client instance like:</p> OrderCommandClientConfig.java<pre><code>import feign.Feign;\nimport feign.micrometer.MicrometerCapability;\nimport feign.micrometer.MicrometerObservationCapability;\nimport io.micrometer.core.instrument.MeterRegistry;\nimport io.micrometer.observation.ObservationRegistry;\nimport io.micrometer.observation.annotation.Observed;\n...\n\n@Configuration(proxyBeanMethods = false)\npublic class OrderCommandClientConfig {\n\n    @Bean\n    @Observed\n    public OrderCommandClient orderCommandClient(ObservationRegistry observationRegistry,\n                                                 MeterRegistry meterRegistry) {\n        return Feign.builder()\n                .logLevel(Logger.Level.FULL)\n                .logger(new Slf4jLogger())\n                .encoder(new JacksonEncoder())\n                .decoder(new JacksonDecoder())\n                // let the ObservationRegistry and MeterRegistry autowired \n                // by the Bean constructor, and use in these two Capability\n                .addCapability(new MicrometerObservationCapability(observationRegistry))\n                .addCapability(new MicrometerCapability(meterRegistry))\n                .target(OrderCommandClient.class, \"http://localhost:8081\");\n    }\n\n}\n</code></pre> <p>For the second issue, I still thinking about whether it is a good idea to put an asynchronous method in the same trace or span of the original request, let's talk about it in the future and ignore it now.</p>","tags":["Observability","Java"]},{"location":"Software%20Engineering%20Blog/2023/02/28/spring-boot-3-observability-monitor-application-on-the-method-level/#how-to-log-method-arguments-in-observedaspect","title":"How to log method arguments in <code>ObservedAspect</code>","text":"<p>In the previous section, we need to add codes manually into each method for logging. It doesn\u2019t sound right for any software developer. Although the post in Spring Blog does this by implementing the <code>ObservationHandler&lt;Observation.Context&gt;</code>, in this way, we can not get arguments of the method in our observation. Then I find this paragraph in the source code:</p> <p>According to the javadoc of <code>ObservedAspect.java</code> from <code>io.micrometer-micrometer.observation-1.10.2</code>: \u2026You might want to add io.micrometer.common.KeyValues programmatically to the Observation. In this case, the ObservationConvention can help. It receives an ObservedAspect.ObservedAspectContext that also contains the ProceedingJoinPoint and returns the <code>io.micrometer.common.KeyValues</code> that will be attached to the Observation.</p> <p>So I wrote this implementation class to log around the observed method with arguments:</p> AbstractObserveAroundMethodHandler.java<pre><code>import io.micrometer.observation.Observation;\nimport io.micrometer.observation.ObservationHandler;\nimport io.micrometer.observation.aop.ObservedAspect;\nimport org.aspectj.lang.ProceedingJoinPoint;\n\npublic class AbstractObserveAroundMethodHandler extends AbstractLogAspect\n        implements ObservationHandler&lt;ObservedAspect.ObservedAspectContext&gt; {\n\n    @Override\n    public void onStart(ObservedAspect.ObservedAspectContext context) {\n        /* we can get many information (including class, arguments...) \n        form ProceedingJoinPoint. */\n        ProceedingJoinPoint joinPoint = context.getProceedingJoinPoint();\n        super.logBefore(joinPoint);\n    }\n\n    @Override\n    public void onStop(ObservedAspect.ObservedAspectContext context) {\n        ProceedingJoinPoint joinPoint = context.getProceedingJoinPoint();\n        super.logAfter(joinPoint);\n    }\n\n    @Override\n    public boolean supportsContext(Observation.Context context) {\n        /* required, otherwise the here will handle the \n        non-spring bean method (e.g. handling http.server.requests) \n        and throw a class cast exception. */\n        return context instanceof ObservedAspect.ObservedAspectContext;\n    }\n}\n</code></pre> <p>The extended class <code>AbstractLogAspect</code> is for the classes that I didn\u2019t put the <code>@Observed</code> on it, but still want to log around them. So I extract the logic for others <code>@Aspect</code> to use.</p> AbstractLogAspect.java<pre><code>import org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.Signature;\n...\n\npublic class AbstractLogAspect {\n\n    public void logBefore(ProceedingJoinPoint joinPoint) {\n        LogInfo logInfo = getLogInfo(joinPoint);\n        // this make the logger print the right classType\n        Logger log = LoggerFactory.getLogger(logInfo.declaringType);\n        log.info(\"[{}.{}] start ({})\", logInfo.className, \n                  logInfo.annotatedMethodName, logInfo.args);\n    }\n\n    private static LogInfo getLogInfo(ProceedingJoinPoint joinPoint) {\n        Signature signature = joinPoint.getSignature();\n        Class declaringType = signature.getDeclaringType();\n        String className = declaringType.getSimpleName();\n        String annotatedMethodName = signature.getName();\n        Object[] args = joinPoint.getArgs();\n        return new LogInfo(declaringType, className, annotatedMethodName, args);\n    }\n\n    public void logAfter(ProceedingJoinPoint joinPoint) {\n        LogInfo logInfo = getLogInfo(joinPoint);\n        Logger log = LoggerFactory.getLogger(logInfo.declaringType);\n        log.info(\"[{}.{}] end\", logInfo.className, logInfo.annotatedMethodName);\n    }\n\n    private record LogInfo(\n            @NotNull\n            Class declaringType,\n            @NotNull\n            String className,\n            @NotNull\n            String annotatedMethodName,\n            @Nullable\n            Object[] args) {\n    }\n\n}\n</code></pre> <p>Please see more detail on my Pull Request in my POC project.</p>","tags":["Observability","Java"]},{"location":"Software%20Engineering%20Blog/2023/02/28/spring-boot-3-observability-monitor-application-on-the-method-level/#how-would-data-be-displayed-on-the-grafana","title":"How would data be displayed on the Grafana","text":"<p>After all the settings and implementation above, we can finally look at the result through Grafana UI on http://localhost:3000. First, we can use explore tab to query the logs we are interested in (see how to query by LogQL).</p> <p></p> <p>Now we can see all the logs match our query between different servers. Then we can click one of them and the detailed information will appear. Click the Tempo button right next to the <code>traceId</code>, the correlated trace and span data will be displayed in a new panel.</p> <p></p> <p>The data in Tempo is very useful when we want to find out the bottleneck of requests in our micro-service cluster. Now we can get more insight from tempo since the spring boot 3 observability can easily show the time cost for each method in one application.</p> <p>After exploring, we can save the queries to form a dashboard, or we can download the well-designed dashboard from Grafana Labs. Here I choose one, which very matches our use case.</p> <p></p> <p>There are too many things to discuss on Grafana dashboards and alerting rules, so I will just stop here and maybe do some research in the future.</p>","tags":["Observability","Java"]},{"location":"Software%20Engineering%20Blog/2023/02/28/spring-boot-3-observability-monitor-application-on-the-method-level/#summary","title":"Summary","text":"<p>In this article, we implement the spring boot 3 observability and test locally and run all components (Grafana, Loki, Tempo, and Prometheus) by docker-compose. The spring boot 3 observability provides a very straightforward way to achieve it by AOP and annotation. Moreover, it can now observe detail down to the method level.</p> <p>Besides, I also do a little study on the source code and find out the easy way to combine the <code>@Observed</code> annotation and the spring <code>aspect</code> with <code>ProceedingJoinPoint</code>, which provides the argument and right class name of the target method. In this way, the log will be more clear and accurate.</p> <p>There are some useful features I haven\u2019t covered yet, like <code>lowCardinalityTags</code>, <code>highCardinalityTags</code>, custom metrics, etc. I will keep studying those topics and share them in the future.</p>","tags":["Observability","Java"]},{"location":"Software%20Engineering%20Blog/2023/02/28/spring-boot-3-observability-monitor-application-on-the-method-level/#reference","title":"Reference","text":"<ul> <li>Observability with Spring Boot 3</li> <li>Integration with Micrometer Observation</li> <li>Spring Cloud OpenFeign</li> </ul>","tags":["Observability","Java"]},{"location":"Software%20Engineering%20Blog/2024/01/26/spring-boot-3-build-the-efficiency-idempotent-api-by-redis/","title":"Spring Boot 3: build the efficiency Idempotent API by Redis","text":"<p>Idempotency API means that the data / system state will be the same no matter how many times the API is successfully called with the same request body/parameter.</p> <p>We've described why we need and how to design an idempotency API mechanism in the article How to design an efficient Idempotency API, If you haven't read it before, please refer to it.</p> <p>This article will focus on implementing it in an existing project, which is my event-sourcing POC project. Here are the implementing steps:</p> <ol> <li>create and use the shared module</li> <li>implement the idempotency mechanism</li> <li>modify the original service logic</li> <li>demonstrate the result</li> </ol>","tags":["Java","Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2024/01/26/spring-boot-3-build-the-efficiency-idempotent-api-by-redis/#create-and-use-the-shared-module","title":"Create and use the shared module","text":"<p>Since my project is like a mono-repo, and for the convenience of reusing in different application modules, we need to create a shared module named <code>idempotency</code> and then we can also make use of the technique of spring autoconfiguration (please refer to my other article for more detail Auto-configure your common module in the \u201cSpring-Boot-Way\u201d).</p> <p>First, we create a module, <code>idempotency</code> under the <code>modules</code> with this <code>build.gradle</code>:</p> modules/idempotency/build.gradle<pre><code>dependencies {\n    implementation 'org.springframework.boot:spring-boot-starter'\n    implementation \"org.springframework.boot:spring-boot-starter-aop\"\n    implementation 'org.springframework.boot:spring-boot-starter-web'\n    implementation 'org.springframework.boot:spring-boot-starter-data-redis'\n\n    testImplementation 'org.springframework.boot:spring-boot-starter-test'\n    testImplementation 'org.junit.jupiter:junit-jupiter-api'\n    testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine'\n}\n\nbootJar {\n    enabled = false\n}\n\njar {\n    enabled = true\n}\n\ntasks.findAll { it.name.startsWith(\"jib\") }.forEach { it.enabled = false }\n</code></pre> <p>Then, create the IdempotencyFilter and IdempotencyConfig ( ignore some code here, please use the link to check the complete code.).</p> IdempotencyFilter.java<pre><code>@Slf4j\n@RequiredArgsConstructor\npublic class IdempotenceFilter extends OncePerRequestFilter {\n    // implement this class in second step\n}\n</code></pre> IdempotencyConfig.java<pre><code>package org.example.event.sourcing.order.poc.modules.idempotency.config;\n\n...\n\n@AutoConfiguration\npublic class IdempotencyConfig {\n\n    @Value(\"${espoc.idempotency.paths}\")\n    private List&lt;String&gt; idempotencyApiPaths;\n\n    @Value(\"${espoc.idempotency.ttlInMinutes:60}\")\n    private Long ttlInMinutes;\n\n    @Bean\n    RedisTemplate&lt;String, IdempotenceFilter.IdempotencyValue&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) {\n        StringRedisSerializer stringRedisSerializer = new StringRedisSerializer();\n        Jackson2JsonRedisSerializer jackson2JsonRedisSerializer =\n                new Jackson2JsonRedisSerializer(IdempotenceFilter.IdempotencyValue.class);\n\n        RedisTemplate&lt;String, IdempotenceFilter.IdempotencyValue&gt; template = new RedisTemplate&lt;&gt;();\n        template.setConnectionFactory(redisConnectionFactory);\n\n        template.setKeySerializer(stringRedisSerializer);\n        template.setValueSerializer(jackson2JsonRedisSerializer);\n\n        template.setHashKeySerializer(stringRedisSerializer);\n        template.setHashValueSerializer(jackson2JsonRedisSerializer);\n\n        return template;\n    }\n\n    @Bean\n    public FilterRegistrationBean&lt;IdempotenceFilter&gt; idempotenceFilterRegistrationBean(\n            RedisTemplate&lt;String, IdempotenceFilter.IdempotencyValue&gt; redisTemplate) {\n\n        FilterRegistrationBean&lt;IdempotenceFilter&gt; registrationBean = new FilterRegistrationBean();\n\n        IdempotenceFilter idempotenceFilter = new IdempotenceFilter(redisTemplate, ttlInMinutes);\n\n        registrationBean.setFilter(idempotenceFilter);\n        registrationBean.addUrlPatterns(idempotencyApiPaths.toArray(String[]::new));\n        registrationBean.setOrder(1); //make sure the idempotency-filter is after all auth-related filter\n        return registrationBean;\n    }\n\n}\n</code></pre> <p>Second, add a file named <code>org.springframework.boot.autoconfigure.AutoConfiguration.imports</code> under the <code>modules/idempotency/src/main/resources/META-INF/spring</code> as below:</p> <pre><code>org.example.event.sourcing.order.poc.modules.idempotency.config.IdempotencyConfig\n</code></pre> <p>This setting can let the spring boot auto-scan the <code>@AutoConfiguration</code> class we specify in the file. The only thing to do is to import this module in the target application module (in our case is the order-command-side app):</p> order/command-side/build.gradle<pre><code>...\n\ndependencies {\n    ...\n    implementation project(\":modules:idempotency\")\n\n    ...\n}\n</code></pre> <p>Finally, we use the profile mechanism to include the related config, we should add two config files, <code>application-redis.yaml</code> and <code>application-idempotency.yaml</code>:</p> applicatio-redis.yaml<pre><code>spring:\n  data:\n    redis:\n      host: ${REDIS_HOST:127.0.0.1}\n      port: ${REDIS_PORT:6379}\n</code></pre> application-idempotency.yaml<pre><code>espoc:\n  idempotency:\n    paths: &gt;\n      /api/v1/orders,\n      /api/v1/orders/complete\n    ttlInMinutes: 120\n</code></pre> <p>Then, please make sure these yaml are included in the config location, then we can add it in the main/resources/application.yaml for order/command-side module as below:</p> <pre><code>spring:\n  ...\n  profiles:\n    include:\n      ...\n      - redis\n      - idempotency\n</code></pre> <p>After all these settings, the filter is added to the filter chain, we are ready to implement the logic of idempotency filter.</p>","tags":["Java","Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2024/01/26/spring-boot-3-build-the-efficiency-idempotent-api-by-redis/#implement-the-idempotency-mechanism","title":"Implement the idempotency mechanism","text":"<p>In this step, we have some challenges such as:</p> <ul> <li>Avoid cache-key conflicting</li> <li>Avoid a race condition when initializing the cache as in progress</li> <li>How to cache the response body from an OutputStream</li> <li>How to properly respond when cache exists</li> </ul> <p>we will go through them one by one. (get a glance first at the complete code in here)</p> <p>before the idempotency logic truly starts, we should first pass some checks, (all logic is implemented in the <code>doFilterInternal</code> method)</p> <pre><code>@Slf4j\n@RequiredArgsConstructor\npublic class IdempotenceFilter extends OncePerRequestFilter {\n\n    @Override\n    protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain)\n            throws ServletException, IOException {\n        log.debug(\"start IdempotenceFilter\");\n\n        String method = request.getMethod();\n        String requestId = request.getHeader(REQUEST_ID_KEY);\n        String clientId = request.getHeader(CLIENT_ID_KEY);\n\n        if (isNotTargetMethod(method)) {\n            log.info(\"Request method {} didn't match the target idempotency https method.\", method);\n            filterChain.doFilter(request, response);\n        } else if (StringUtils.isBlank(requestId)\n                || StringUtils.isBlank(clientId)) {\n            log.warn(\"Request should bring a RequestId and ClientId in header, but no. get rid = {}, cid = {}.\", requestId, clientId);\n            filterChain.doFilter(request, response);\n        } else {\n            // idempotency logic \n        }\n    }\n\n    private boolean isNotTargetMethod(String method) {\n        return !HttpMethod.POST.matches(method);\n    }\n}\n</code></pre>","tags":["Java","Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2024/01/26/spring-boot-3-build-the-efficiency-idempotent-api-by-redis/#avoid-cache-key-conflicting","title":"Avoid cache-key conflicting","text":"<p>As a filter that covers different endpoints of our service. The last thing we want is to miss-place the cached response to the wrong request (e.g. response of <code>POST /order/create</code> is returned to a request to <code>POST /order/cancel</code> only since they have the same requestId).</p> <p>so we should also consider the <code>request method</code> (if we want to cover not only the <code>POST</code> method), <code>request URI</code>, <code>client id</code> ( which is the id of the client or service that integrates our API), and the <code>request-id</code> (which is generated by the client itself to denote its retry call).</p> <pre><code>@Slf4j\n@RequiredArgsConstructor\npublic class IdempotenceFilter extends OncePerRequestFilter {\n\n    @Override\n    protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain)\n            throws ServletException, IOException {\n        // define other variables\n        String cacheKey = join(DELIMITER, method, request.getRequestURI(), clientId, requestId);\n\n        if (isNotTargetMethod(method)) {\n            // invalid http method\n        } else if (StringUtils.isBlank(requestId)\n                || StringUtils.isBlank(clientId)) {\n            // invalid header\n        } else {\n            BoundValueOperations&lt;String, IdempotencyValue&gt; keyOperation = redisTemplate.boundValueOps(cacheKey);\n            // idempotency logic\n        }\n    }\n}\n</code></pre> <p>Here, we simply combine all four parts as a cacheKey for later use.</p>","tags":["Java","Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2024/01/26/spring-boot-3-build-the-efficiency-idempotent-api-by-redis/#avoid-a-race-condition-when-initializing-the-cache-as-in-progress","title":"Avoid a race condition when initializing the cache as in-progress","text":"<p>The first challenge is to make sure the first request will create a cache noted as in-progress, so the following retry request will get an in-progress response instead of executing the business logic. so we design the Redis value data structure as follows:</p> <pre><code>public record IdempotencyValue(Map&lt;String, Object&gt; header, int status, String cacheValue, boolean isDone) {\n\n    protected static IdempotencyValue init() {\n        return new IdempotencyValue(Collections.emptyMap(), 0, \"\", false);\n    }\n\n    protected static IdempotencyValue done(Map&lt;String, Object&gt; header, Integer status, String cacheValue) {\n        return new IdempotencyValue(header, status, cacheValue, true);\n    }\n\n}\n</code></pre> <p>Then, we use the <code>BoundValueOperations::setIfAbsent</code> in <code>spring-data-redis</code>, which makes use of the <code>SETNX key value</code> in Redis.</p> <pre><code>@Slf4j\n@RequiredArgsConstructor\npublic class IdempotenceFilter extends OncePerRequestFilter {\n\n    @Override\n    protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain)\n            throws ServletException, IOException {\n        // define variables\n\n        if (isNotTargetMethod(method)) {\n            // invalid http method\n        } else if (StringUtils.isBlank(requestId)\n                || StringUtils.isBlank(clientId)) {\n            // invalid header\n        } else {\n            log.info(\"requestId and clientId not empty, rid = {}, cid = {}\", requestId, clientId);\n            BoundValueOperations&lt;String, IdempotencyValue&gt; keyOperation = redisTemplate.boundValueOps(cacheKey);\n            boolean isAbsent = keyOperation.setIfAbsent(IdempotencyValue.init(), ttl, TimeUnit.MINUTES);\n            if (isAbsent) {\n                // if cache is not exist\n            } else {\n                // if cache is exist\n            }\n        }\n    }\n}\n</code></pre> <p>The function is designed to initialize the value when the query indicates that the key does not exist in a single thread. This process occurs sequentially and is synchronized. Meanwhile, any other queries for the same key will be temporarily blocked by Redis, given its single-threaded nature, ensuring a consistent and ordered execution of operations.</p> <p>combining the two traits above, we can make sure no race condition would happen in the key creation phase.</p>","tags":["Java","Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2024/01/26/spring-boot-3-build-the-efficiency-idempotent-api-by-redis/#how-to-cache-the-response-body-from-an-outputstream","title":"How to cache the response body from an OutputStream","text":"<p>The default <code>HttpServletResponse</code> only allows a one-time read of the response body. Fortunately, Spring Boot provides a <code>ContentCachingResponseWrapper</code>, that makes it easy to read the response body multiple times. We only need to wrap the original response into it and pass it to the next filters and controller.</p> <pre><code>@Slf4j\n@RequiredArgsConstructor\npublic class IdempotenceFilter extends OncePerRequestFilter {\n\n    @Override\n    protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain)\n            throws ServletException, IOException {\n        // define variables\n\n        if (isNotTargetMethod(method)) {\n            // invalid http method\n        } else if (StringUtils.isBlank(requestId)\n                || StringUtils.isBlank(clientId)) {\n            // invalid header\n        } else {\n            // ... skip some code here\n            boolean isAbsent = keyOperation.setIfAbsent(IdempotencyValue.init(), ttl, TimeUnit.MINUTES);\n            if (isAbsent) {\n                log.info(\"cache {} not exist \", cacheKey);\n                ContentCachingResponseWrapper responseCopier = new ContentCachingResponseWrapper(response);\n\n                filterChain.doFilter(request, responseCopier); // execute the original business logic\n\n                updateResultInCache(request, responseCopier, keyOperation);\n                responseCopier.copyBodyToResponse();\n            } else {\n                // if cache is exist\n            }\n        }\n    }\n\n    private void updateResultInCache(HttpServletRequest request, ContentCachingResponseWrapper responseCopier,\n                                     BoundValueOperations&lt;String, IdempotencyValue&gt; keyOperation)\n            throws UnsupportedEncodingException {\n        if (needCache(responseCopier)) {\n            log.info(\"process result need to be cached\");\n            String responseBody = new String(responseCopier.getContentAsByteArray(), responseCopier.getCharacterEncoding());\n            IdempotencyValue result = IdempotencyValue.done(Collections.emptyMap(), responseCopier.getStatus(), responseBody);\n\n            log.info(\"save {} to redis\", result);\n            keyOperation.set(result, ttl, TimeUnit.MINUTES);\n        } else {\n            log.info(\"process result don't need to be cached\");\n            redisTemplate.delete(keyOperation.getKey());\n        }\n    }\n\n    private boolean needCache(ContentCachingResponseWrapper responseCopier) {\n        int statusCode = responseCopier.getStatus();\n        return statusCode &gt;= 200\n                &amp;&amp; statusCode &lt; 300;\n    }\n\n}\n</code></pre> <p>Dive into the logic here, after the original business logic, we will get a response. In certain scenarios, such as network errors or temporary failures, it becomes necessary to permit the client to retry our API. To achieve this, the needCache method is introduced. It ensures that we only cache the response body when the HTTP status is in the 2xx range. If caching is unnecessary, we also take the step of deleting the in-progress cache record in Redis. This prevents all subsequent retry calls from consistently encountering an in-progress error.</p> <p>In the case of caching the response, we would need to cache all the needed headers (omitted for simplicity in this instance), the status code, and configure a Time-to-Live (TTL) setting. The TTL is crucial to ensure the periodic cleanup of Redis, maintaining a tidy and efficient storage environment.</p>","tags":["Java","Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2024/01/26/spring-boot-3-build-the-efficiency-idempotent-api-by-redis/#how-to-properly-respond-when-cache-exists","title":"How to properly respond when cache exists","text":"<p>In this scenario, it is imperative to abstain from executing the original business logic. Instead, when a prior request is still in progress, the appropriate course is to return an in-progress error. Conversely, if the previous request has successfully finished and a cached response is available, it should be rapidly returned. For both cases, we need to build the response in the <code>IdempotenceFilter</code>.</p> <pre><code>@Slf4j\n@RequiredArgsConstructor\npublic class IdempotenceFilter extends OncePerRequestFilter {\n\n    @Override\n    protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain)\n            throws ServletException, IOException {\n        // define variables\n\n        if (isNotTargetMethod(method)) {\n            // invalid http method\n        } else if (StringUtils.isBlank(requestId)\n                || StringUtils.isBlank(clientId)) {\n            // invalid header\n        } else {\n            log.info(\"requestId and clientId not empty, rid = {}, cid = {}\", requestId, clientId);\n            BoundValueOperations&lt;String, IdempotencyValue&gt; keyOperation = redisTemplate.boundValueOps(cacheKey);\n            boolean isAbsent = keyOperation.setIfAbsent(IdempotencyValue.init(), ttl, TimeUnit.MINUTES);\n            if (isAbsent) {\n                // if cache is not exist\n            } else {\n                log.info(\"cache {} already exist \", cacheKey);\n                handleWhenCacheExist(request, response, keyOperation);\n            }\n        }\n    }\n\n    private void handleWhenCacheExist(HttpServletRequest request, HttpServletResponse response,\n                                      BoundValueOperations&lt;String, IdempotencyValue&gt; keyOperation)\n            throws IOException {\n        IdempotencyValue cachedResponse = keyOperation.get();\n        log.info(\"cached content = {} \", cachedResponse);\n        String responseBody;\n        Integer status;\n\n        if (cachedResponse.isDone) {\n            log.info(\"cache {} exist, and is done.\");\n            status = cachedResponse.status;\n            responseBody = cachedResponse.cacheValue;\n        } else {\n            log.info(\"cache exist, and is still in processing, please retry later\");\n            status = TOO_EARLY.value();\n            ProblemDetail pd = ProblemDetail.forStatus(TOO_EARLY);\n            pd.setType(URI.create(request.getRequestURI()));\n            pd.setDetail(\"request is now processing, please try again later\");\n            responseBody = OBJECT_MAPPER.writeValueAsString(pd);\n        }\n        response.setStatus(status);\n        response.setContentType(MediaType.APPLICATION_JSON_VALUE);\n\n        PrintWriter responseWriter = response.getWriter();\n        responseWriter.write(responseBody);\n\n        response.flushBuffer();\n\n    }\n}\n</code></pre> <p>The key to doing this is to write something into the response writer, and then do <code>flushBuffer()</code>. In this way, we can deal with both the in-progress error response and the cached successful response.</p> <p>Combine all the snippets above we get a comprehensive mechanism in IdempotenceFilter (please get the complete code in my GitHub Repository)</p>","tags":["Java","Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2024/01/26/spring-boot-3-build-the-efficiency-idempotent-api-by-redis/#modify-original-service-logic","title":"Modify original service logic","text":"<p>Within the original service (OrderService), our primary concern is to verify, through a database check, whether the request has been previously executed. This step is crucial due to the existence of a TTL constraint imposed on the idempotency cache, a measure taken to optimize performance.</p> <p>In my project, I've implemented this check as follows (with the request body's ID serving as the unique identifier for the same request within the context of my business logic):</p> <pre><code>public class OrderService {\n\n    public Order createOrder(Order order) {\n        Optional&lt;V1Order&gt; queryResult = queryOrder(order.id());\n        if (queryResult.isPresent()) {\n            return toOrder(queryResult);\n        } else {\n            boolean isSuccess = orderEventProducer.create(new OrderEvent(order.id(), CREATED, Instant.now()));\n            if (isSuccess) {\n                return order;\n            } else {\n                log.warn(\"create Order event fail\", order);\n                throw new ResponseStatusException(HttpStatus.INTERNAL_SERVER_ERROR, \"send event fail.\");\n            }\n        }\n    }\n}\n</code></pre>","tags":["Java","Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2024/01/26/spring-boot-3-build-the-efficiency-idempotent-api-by-redis/#demonstrate-the-result","title":"Demonstrate the result","text":"<p>Finally, we complete the whole idempotency mechanism, let see how would it work.</p> <p>I prepare four command-line terminal, three of them would send exactly the same request to server (especially the same <code>rid</code> and <code>cid</code>), Meanwhile, the fourth terminal utilizes rdcli to inspect the cache values in Redis.</p> <ul> <li>Left-Top: The first one to send the request.</li> <li>Right-Top: Send request after the left-top send, but before it receive response.</li> <li>Left-Bottom: Send request after the left-top receive response.</li> <li>Right-Bottom: Query the key-value during these process.</li> </ul> <p></p> <p>We can see that, during the Left-Top request is processing, the request in Right-Top quickly get a <code>425 Too Early</code> response, and the cache in redis is marked as <code>isDone:false</code>. Then, after the response of the Left-Top terminal is back, the cache in redis will be marked as <code>isDone:true</code> and with a response body <code>{\\\"id\\\":\\\"22222\\\"}</code>.</p> <p>Finally, sending the request again in the Left-Bottom terminal quickly yields the same response as the Left-Top, indicating the successful implementation of the idempotency mechanism.</p>","tags":["Java","Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2024/01/26/spring-boot-3-build-the-efficiency-idempotent-api-by-redis/#summary","title":"Summary","text":"<p>In this article, we delve into the implementation of the idempotency mechanism proposed in my earlier article, how-to-design-an-efficient-idempotency-api within a Spring Boot 3 Application.</p> <p>Our approach leverages Spring's autoconfiguration feature, transforming it into a plug-and-use module. Specifically, we make use of various Spring Boot components to streamline development, avoiding the reinvention of the wheel. Notable components include <code>ContentCachingResponseWrapper</code> for reading response bodies multiple times, <code>BoundValueOperations.setIfAbsent</code> to prevent race conditions, and the utilization of <code>ProblemDetail</code> and <code>HttpServletResponse Writer</code> to ensure proper responses to clients. Additionally, a simple command-line demonstration showcases how the mechanism operates in practice.</p> <p>For those interested in exploring the complete code changes, please refer to the associated Pull Request. Any feedback is greatly appreciated, and a minor fix is also addressed in the second Pull Request. Feel free to explore and share your thoughts.</p>","tags":["Java","Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2024/01/26/spring-boot-3-build-the-efficiency-idempotent-api-by-redis/#reference","title":"Reference:","text":"<ul> <li>https://medium.com/gitconnected/how-to-design-an-efficient-idempotency-api-e664fa2954bb</li> <li>https://docs.spring.io/spring-data/data-redis/docs/3.1.5/reference/html/</li> <li>https://github.com/spring-projects/spring-data-examples/tree/main/redis</li> <li>https://www.baeldung.com/spring-mvc-handlerinterceptor</li> <li>https://stackoverflow.com/questions/26699385/spring-boot-yaml-configuration-for-a-list-of-strings</li> </ul>","tags":["Java","Idempotent API","Distributed Systems","Data Consistency"]},{"location":"Software%20Engineering%20Blog/2023/05/13/the-first-step-toward-true-continuous-delivery/","title":"The First Step Toward True Continuous Delivery","text":"","tags":["DevOps","GitHub Action","CD"]},{"location":"Software%20Engineering%20Blog/2023/05/13/the-first-step-toward-true-continuous-delivery/#run-end-to-end-test-in-pr-check-by-github-action","title":"Run End to End Test in PR check by GitHub Action","text":"<p>This article will cover the first step for achieving True Continuous Delivery: How to gain more confidence when merging code into the main branch. The answer is always more tests, more specifically, more automated tests. Unlike the unit test, the E2E (End to End) test is more approximate to the real environment, which gives more confidence when it is passed.</p> <p>So I want to add an E2E test when opening a PR (Pull Request) as a PR check. that can make sure my code change will not break the existing business logic.</p> <p>How to implement it? Here is my proposal for the PR-triggered workflow:</p> <ol> <li>Checkout code</li> <li>Build and Run The Cluster    using docker-compose to run all the project applications and dependency components (e.g. DB, Kafka, Redis, etc.)</li> <li>Run the E2E test    (either by running Postman collection or other tools)</li> <li>Clean up the Cluster</li> </ol> <p>If you want to give it a try but don\u2019t want to trigger on GitHub, please refer to my article about testing GitHub Action locally.</p>","tags":["DevOps","GitHub Action","CD"]},{"location":"Software%20Engineering%20Blog/2023/05/13/the-first-step-toward-true-continuous-delivery/#background","title":"Background","text":"<p>CD (Continuous Delivery) is the highest pursuit for software development. In a narrow sense to say CD is to achieve that: once we merge the PR into the main branch of the repo, after running through a lot of automation pipeline/test, the code/artifact should be deployed in the production environment and facing the real user.</p> <p>No manual tests, no code freeze, and no one need to sign the release. It also can faster the release and feedback cycle. plenty of advantages I didn\u2019t mention yet. Sound fancy right?</p> <p>Lots of advantages come with lots of work and issues: automated test, security test, rollback, and revert\u2026 there are a lot of aspects that should be discussed. There is one awesome presentation talk about these topics, which was given by Roy Osherove. This article is inspired by his talk also.</p> <p>The Pipeline-Driven Organization \u2022 Roy Osherove \u2022 GOTO 2022</p>","tags":["DevOps","GitHub Action","CD"]},{"location":"Software%20Engineering%20Blog/2023/05/13/the-first-step-toward-true-continuous-delivery/#implementation","title":"Implementation","text":"<p>I plan to use Github Action to implement the workflow I proposed since it\u2019s a modern, community-supported, and well-integrated (with Github) CI (Continuous Integration) tool around the market.</p>","tags":["DevOps","GitHub Action","CD"]},{"location":"Software%20Engineering%20Blog/2023/05/13/the-first-step-toward-true-continuous-delivery/#_1","title":"The First Step Toward True Continuous Delivery","text":"<p>Workflow setting and Checkout Code Easy peasy, Just reference to the official action and modify like below:</p> <p><pre><code>name: E2E Test\n\non:\n  pull_request:\n    branches: [ \"master\" ]\n\npermissions:\n  contents: read\n\njobs:\n  e2e-test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n</code></pre> we set the workflow name \u201cE2E Test\u201d will be triggered and run on a Ubuntu runner when a PR to the \u201cmaster\u201d branch is open</p>","tags":["DevOps","GitHub Action","CD"]},{"location":"Software%20Engineering%20Blog/2023/05/13/the-first-step-toward-true-continuous-delivery/#build-and-run-the-cluster","title":"Build and Run The Cluster","text":"<p>The detail in this step may differ from the language we use. But the Whole concept is to build the artifact (in my grade spring boot project is the .jar file). Then use the docker-compose to start the application. Besides, we can use also run the dependency component like DB, Kafka, or Redis before we run the applications.</p> <p>The steps .yaml will be like the below: <pre><code>...\n- name: Set up JDK 17\n  uses: actions/setup-java@v3\n  with:\n    java-version: '17'\n    distribution: 'temurin'\n    cache: 'gradle'\n\n- name: Build with Gradle\n  run: ./gradlew build -x test --no-daemon\n\n- name: Run The Cluster\n  run: docker compose -f Docker/boot-run-apps-docker-compose.yml -f Docker/kafka-docker-compose.yml --env-file Docker/config/.env.docker -p event-sourcing up -d\n...\n</code></pre></p> <p>Here, I use the official setup action and then build the project (ignore tests by <code>-x test</code>, since we focus on the E2E test here, we will run the unit test in another workflow)</p> <p>Two notable things are:</p> <ol> <li>we must run the <code>docker compose up</code> in background mode (with -d option). Otherwise, the action will be stuck in this command( since the process is occupied by <code>docker-compose up</code>)</li> <li>If using the GitHub free plan, then we can only run on a 2-core runner, so we have to reduce the number of containers started in docker-compose (I passed the test until reducing it to 3 containers ).</li> </ol> <p>Then we can take a look at the docker-compose .yaml for application and Kafka (the only dependency component in my project).</p> <p>The <code>boot-run-apps-docker-compose.yml</code> like below: <pre><code>version: \"3.9\"\nservices:\n  order-command:\n    image: openjdk:19-slim\n    extra_hosts: [ 'host.docker.internal:host-gateway' ]\n    depends_on:\n      - kafka\n    env_file:\n      - ./config/.env.docker\n    volumes:\n      - \"../:/app\"\n    ports:\n      - \"8081:8081\"\n    working_dir: /app\n    command: java -jar order/command-side/build/libs/command-side-0.0.1-SNAPSHOT.jar\n  order-query:\n    extra_hosts: [ 'host.docker.internal:host-gateway' ]\n    depends_on:\n      - kafka\n    image: openjdk:19-slim\n    ports:\n      - \"8083:8083\"\n    env_file:\n      - ./config/.env.docker\n    volumes:\n      - \"../:/app\"\n    working_dir: /app\n    command: java -jar order/query-side/build/libs/query-side-0.0.1-SNAPSHOT.jar\n</code></pre></p> <p>And the <code>kafka-docker-compose.yml</code> like below:</p> <pre><code>version: \"3\"\nservices:\n  kafka:\n    image: 'bitnami/kafka:${KAFKA_TAG:-latest}'\n    extra_hosts: [ 'host.docker.internal:host-gateway' ]\n    ports:\n      - '9092:9092'\n    environment:\n      - ALLOW_PLAINTEXT_LISTENER=yes\n      - LOG_RETENTION_HOURS=26280\n</code></pre> <p>In this way, we can run all application and dependency components on the GitHub action runner.</p>","tags":["DevOps","GitHub Action","CD"]},{"location":"Software%20Engineering%20Blog/2023/05/13/the-first-step-toward-true-continuous-delivery/#run-the-e2e-test","title":"Run The E2E test","text":"<p>Before executing the E2E test, we should make sure our target server is on and ready to respond to requests. Since we run the docker-compose up -d in background mode, we won\u2019t know when the containers are running (some dependency images needs to be pulled first). And after the container is running, we also need to wait for our Spring Boot applications to initialize. So here I use jtalk/url-health-check-action to make sure the target server is ready.</p> <pre><code>- name: Check the order-query\n  uses: jtalk/url-health-check-action@v3\n  with:\n    url: http://localhost:8083/actuator/health\n    max-attempts: 5\n    retry-delay: 12s\n    retry-all: true\n\n- name: Check the order-command\n  uses: jtalk/url-health-check-action@v3\n  with:\n    url: http://localhost:8081/actuator/health\n    max-attempts: 5\n    retry-delay: 12s\n    retry-all: true\n</code></pre> <p>The action will run like the below picture. I think it\u2019s very convenient and straightforward. </p> <p>After making sure the server is running, we can now execute our E2E test. There are plenty of ways to do so. Here I choose to use matt-ball/newman-action. It\u2019s an action for running \u201cnewman\u201d, a CLI command for running the postman collection.</p> <pre><code>...\n- name: run Postman Collection\n  uses: matt-ball/newman-action@master\n  with:\n    collection: postman/OrderCommand.postman_collection.json\n...\n</code></pre> <p>So, all I need to do is to prepare a Postman collection with tests in it. Then export as <code>.json</code> file and add it to my repository (it\u2019s a long JSON file, so I provide the link here).</p> <p> </p> <p>The action will run like the below picture. It\u2019s much easier than installing node.js and newman CLI manually as this article suggested.</p> <p></p> <p>Here we can see that the E2E test is executing successfully and there is no fail test. So I can have more confidence that my code change in PR is good enough and ready to go live.</p>","tags":["DevOps","GitHub Action","CD"]},{"location":"Software%20Engineering%20Blog/2023/05/13/the-first-step-toward-true-continuous-delivery/#clean-up-the-cluster","title":"Clean up the Cluster","text":"<p>The final important thing is to clean up what we have done. since we run the cluster up in background mode, we should make sure that we stop it (otherwise the free limit of GitHub Action runner minutes might be gone in just a few days, not sure, but I don\u2019t want to try).</p> <pre><code>- name: clean docker compose\n  if: always()\n  run: |\n    docker compose -f Docker/boot-run-apps-docker-compose.yml -f Docker/kafka-docker-compose.yml --env-file Docker/config/.env.docker -p event-sourcing down\n</code></pre> <p>the <code>if: always()</code> ensure this step will always be executed, no matter whether the previous steps succeed or failed.</p>","tags":["DevOps","GitHub Action","CD"]},{"location":"Software%20Engineering%20Blog/2023/05/13/the-first-step-toward-true-continuous-delivery/#summary","title":"Summary","text":"<p>This article wants to share the first step in achieving continuous delivery, which is to gain more confidence when merging code. This can be accomplished by adding more automated tests, particularly the E2E test, which are more approximate to the real environment and give more confidence when they pass.</p> <p>I propose to implement E2E tests as a PR check triggered by Github Actions, using docker-compose to build and run the cluster, running E2E tests (by running Postman collection), and cleaning up the cluster. Here is the complete <code>workflow.yaml</code> file:</p> <pre><code>name: E2E Test\n\non:\n  pull_request:\n    branches: [ \"master\" ]\n\npermissions:\n  contents: read\n\njobs:\n  e2e-test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Set up JDK 17\n        uses: actions/setup-java@v3\n        with:\n          java-version: '17'\n          distribution: 'temurin'\n          cache: 'gradle'\n\n      - name: Build with Gradle\n        run: ./gradlew build -x test --no-daemon\n\n      - name: Run The Cluster\n        run: docker compose -f Docker/boot-run-apps-docker-compose.yml -f Docker/kafka-docker-compose.yml --env-file Docker/config/.env.docker -p event-sourcing up -d\n\n      # test for estimate the health check parameters, can be ignore\n      - name: Check docker compose status\n        run: |\n          sleep 20\n          docker ps\n          docker logs event-sourcing-order-query-1\n\n      - name: Check the order-query\n        uses: jtalk/url-health-check-action@v3\n        with:\n          url: http://localhost:8083/actuator/health\n          max-attempts: 5\n          retry-delay: 12s\n          retry-all: true\n\n      - name: Check the order-command\n        uses: jtalk/url-health-check-action@v3\n        with:\n          url: http://localhost:8081/actuator/health\n          max-attempts: 5\n          retry-delay: 12s\n          retry-all: true\n\n      - name: run Postman Collection\n        uses: matt-ball/newman-action@master\n        with:\n          collection: postman/OrderCommand.postman_collection.json\n\n      - name: clean docker compose\n        if: always()\n        run: |\n          docker compose -f Docker/boot-run-apps-docker-compose.yml -f Docker/kafka-docker-compose.yml --env-file Docker/config/.env.docker -p event-sourcing down\n</code></pre> <p>I\u2019ve opened the related Pull Request (PR) in my personal repository, feel free to get more details and the complete code here.</p>","tags":["DevOps","GitHub Action","CD"]},{"location":"Software%20Engineering%20Blog/2023/05/13/the-first-step-toward-true-continuous-delivery/#reference","title":"Reference","text":"<p>https://www.youtube.com/watch?v=zmA5fhV-FGk https://www.linkedin.com/pulse/running-postman-collections-via-github-action-nirmala-jayasanka/</p>","tags":["DevOps","GitHub Action","CD"]},{"location":"Software%20Engineering%20Blog/2023/10/23/three-ways-to-solve-cors-issue-in-the-embed-swagger-page-in-backstage/","title":"Three Ways to Solve CORS Issue in the Embed Swagger Page in Backstage","text":"<p>As we mentioned in this article before, centralizing all the needed knowledge in one developer portal is a big improvement in daily working experience and convenience.</p> <p>But we face a CORS problem when sending requests by an embedded swagger page in the API definition. this problem will significantly reduce the functionality of the Swagger page, so this article proposes three ways to solve it:</p> <ol> <li>allow the App to cross-origin for your Backstage domain</li> <li>provide modified plain-text open API JSON and add proxy</li> <li>change the server URL when rendering pages and add proxy</li> </ol> <p>Let's go through them in detail!</p>","tags":["Backstage","CORS","Developer Portal","Problem Solve"]},{"location":"Software%20Engineering%20Blog/2023/10/23/three-ways-to-solve-cors-issue-in-the-embed-swagger-page-in-backstage/#problem-reproduce-as-is","title":"Problem reproduce (As-Is)","text":"<p>After our basic setup in previous article, when sending a request from the Swagger page will look like this:</p> <p></p> <p>here we can discover two problems: 1. the current URL is <code>localhost:3000</code>, while the target server URL is <code>localhost:8081</code> 2. the first problem led to the server response with a CORS error    </p> <p>So by default, we can not send requests through the embedded Swagger page in the Backstage.</p>","tags":["Backstage","CORS","Developer Portal","Problem Solve"]},{"location":"Software%20Engineering%20Blog/2023/10/23/three-ways-to-solve-cors-issue-in-the-embed-swagger-page-in-backstage/#solution-to-be","title":"Solution (To-Be)","text":"<p>Here I propose three lightweight (little code modified) solutions, we can choose one of them according to the security concern, and existing CI/CD process.  </p>","tags":["Backstage","CORS","Developer Portal","Problem Solve"]},{"location":"Software%20Engineering%20Blog/2023/10/23/three-ways-to-solve-cors-issue-in-the-embed-swagger-page-in-backstage/#1-allow-cors-in-the-app","title":"1. Allow CORS in The APP","text":"<p>The easiest way is to allow the App to cross-origin for your Backstage domain, if it's OK to modify your app setting, which might have these side effects:  1. your app in test env (and add logic to disable it in prod env). 2. left some code in your codebase (take my spring boot application for example).    OrderController.java<pre><code>@RestController\n// to allow only Backstage (domain= http://localhost:3000) to send request\n@CrossOrigin(origins = \"http://localhost:3000\") \n...\npublic class OrderController {...}\n</code></pre> In this way, the swagger page can successfully send requests directly to the app.</p>","tags":["Backstage","CORS","Developer Portal","Problem Solve"]},{"location":"Software%20Engineering%20Blog/2023/10/23/three-ways-to-solve-cors-issue-in-the-embed-swagger-page-in-backstage/#2-provide-modified-plain-text-json-and-add-proxy","title":"2. provide modified plain-text JSON, and add proxy","text":"","tags":["Backstage","CORS","Developer Portal","Problem Solve"]},{"location":"Software%20Engineering%20Blog/2023/10/23/three-ways-to-solve-cors-issue-in-the-embed-swagger-page-in-backstage/#needed-modification","title":"Needed Modification","text":"<p>If your open API spec is provided by a static file generated in a CI/CD process, then it is a good way to add a customized step to modify the original URL to Backstage's proxy endpoint, and the Backstage backend will proxy the request and send to your app without a CORS error.</p> <p>to enable the proxy setting, we have to rewrite/ add the <code>servers.url</code> string with a specific key (i.e. <code>/order-command</code> in my example).  api-docs.json<pre><code>{\n  \"openapi\": \"3.0.1\",\n  \"info\": {\n    \"title\": \"OpenAPI definition\",\n    \"version\": \"v0\"\n  },\n  \"servers\": [\n    { // origin URL is http://localhost:8081\n      \"url\": \"http://localhost:7007/api/proxy/order-command\",\n      \"description\": \"Generated server URL\"\n    }\n  ],\n  ...\n}\n</code></pre></p> <p>and add the following setting in the <code>app-config.yaml</code> in the Backstage project</p> app-config.yaml<pre><code>proxy:\n  '/order-command':\n    target: 'http://localhost:8081'\n    changeOrigin: true\n    pathRewrite: \n      '^/api/proxy/order-command': '/'\n</code></pre>","tags":["Backstage","CORS","Developer Portal","Problem Solve"]},{"location":"Software%20Engineering%20Blog/2023/10/23/three-ways-to-solve-cors-issue-in-the-embed-swagger-page-in-backstage/#result","title":"Result","text":"<p>after starting the Backstage, we can first see these logs show the Proxy is created. <pre><code>[1] 2023-10-22T09:14:37.849Z proxy info [HPM] Proxy created: /order-command  -&gt; http://localhost:8081 type=plugin\n[1] 2023-10-22T09:14:37.849Z proxy info [HPM] Proxy rewrite rule created: \"^/api/proxy/order-command\" ~&gt; \"/\" type=plugin\n</code></pre> then the URL on Swagger will change to the proxy endpoint of the Backstage backend.</p> <p></p> <p>In this case, the request will successfully be sent to the Backstage backend and be proxy to the correct App endpoint and respond normally.</p> <p></p>","tags":["Backstage","CORS","Developer Portal","Problem Solve"]},{"location":"Software%20Engineering%20Blog/2023/10/23/three-ways-to-solve-cors-issue-in-the-embed-swagger-page-in-backstage/#3-change-the-server-url-when-render-page-and-add-proxy","title":"3. change the server URL when render page and add proxy","text":"<p>If you have concerns about allowing CORS on the App and don't have an existing CI/CD process to generate a static open API file. Then we should use a customized API entity renderer to do the URL modification task in real time.</p>","tags":["Backstage","CORS","Developer Portal","Problem Solve"]},{"location":"Software%20Engineering%20Blog/2023/10/23/three-ways-to-solve-cors-issue-in-the-embed-swagger-page-in-backstage/#needed-modification_1","title":"Needed Modification","text":"<p>Refer to this Custom API Renderings tutorial. we can first add <code>@types/swagger-ui-react</code> and <code>swagger-ui-react</code> to the package/app, then change the <code>packages/app/src/apis.ts</code> to a <code>.tsx</code> file and add the following (see the diff in my commit): packages/app/src/apis.tsx<pre><code>import {ApiEntity } from '@backstage/catalog-model';\nimport {\napiDocsConfigRef,\ndefaultDefinitionWidgets\n} from '@backstage/plugin-api-docs';\n\nexport const apis: AnyApiFactory[] = [\n  createApiFactory({\n    ...\n  }),\n  ScmAuth.createDefaultApiFactory(),\n  // add the below code snippet\n  createApiFactory({\n      api: apiDocsConfigRef,\n      deps: {},\n      factory: () =&gt; {\n        // load the default widgets\n        const definitionWidgets = defaultDefinitionWidgets();\n        return {\n          getApiDefinitionWidget: (apiEntity: ApiEntity) =&gt; {\n            // custom rendering for solve cors issue\n            if (apiEntity.spec.type === 'cors-openapi') {\n              let regex = /\"servers\":\\[{\"url\":\"([a-z]+:\\/\\/[a-zA-Z-.:0-9]+)\"/g;\n              let matches = regex.exec(apiEntity.spec.definition);\n              let targetString = matches ? matches[1] : \"\";\n\n              apiEntity.spec.definition = apiEntity.spec.definition.replaceAll(\n               regex,\n               \"\\\"servers\\\":[{\\\"url\\\":\\\"http://localhost:7007/api/proxy/\" + targetString + \"\\\"\");\n\n               apiEntity.spec.type='openapi';\n            }\n            // fallback to the defaults\n            return definitionWidgets.find(d =&gt; d.type === apiEntity.spec.type);\n          },\n        };\n      },\n    }),\n    // add the above code snippet\n];\n</code></pre></p> <p>finally, add the corresponding proxy setting in <code>app-config.yaml</code> like:  app-config.yaml<pre><code>proxy:\n  '/http://localhost:8081':\n    target: 'http://localhost:8081'\n    changeOrigin: true\n    pathRewrite:\n      '^/api/proxy/http://localhost:8081': 'http://localhost:8081'\n</code></pre></p> <p>and the only thing we need to modify the <code>spec.type</code> to <code>cors-openapi</code> in our API definition yaml. </p> order-command-side-api.yaml<pre><code>apiVersion: backstage.io/v1alpha1\nkind: API\nmetadata:\n  ...\nspec:\n  type: cors-openapi\n  lifecycle: experimental\n  owner: guests\n  definition:\n    $text: http://localhost:8081/v3/api-docs\n</code></pre>","tags":["Backstage","CORS","Developer Portal","Problem Solve"]},{"location":"Software%20Engineering%20Blog/2023/10/23/three-ways-to-solve-cors-issue-in-the-embed-swagger-page-in-backstage/#result_1","title":"Result","text":"<p>then the URL on Swagger will change to the proxy endpoint of the Backstage backend with the original URL.</p> <p></p> <p>In this case, the request will also successfully be sent to the Backstage backend and be proxy to the correct App endpoint and respond normally.</p> <p></p>","tags":["Backstage","CORS","Developer Portal","Problem Solve"]},{"location":"Software%20Engineering%20Blog/2023/10/23/three-ways-to-solve-cors-issue-in-the-embed-swagger-page-in-backstage/#summary","title":"Summary","text":"<p>Using three ways proposed in this article can solve the CORS issue with slight changes in the project or the Backstage app. This will bring more convenience when others integrate/try our API by reading on the Backstage.</p> <p>The developer portal is a very powerful tool to improve developers' experience,  but it also needs some effort to build some guidelines, plugins, or mechanisms on the portal App (i.e. Backstage), which can be done by a platform engineering team or task force. After the hard work, you will find it very worthy to have a well-done developer portal.</p>","tags":["Backstage","CORS","Developer Portal","Problem Solve"]},{"location":"Software%20Engineering%20Blog/2023/10/23/three-ways-to-solve-cors-issue-in-the-embed-swagger-page-in-backstage/#reference","title":"reference","text":"<ul> <li>CORS problem: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS/Errors</li> <li>Custom API Renderings in Backstage: https://github.com/backstage/backstage/tree/master/plugins/api-docs#customizations</li> </ul>","tags":["Backstage","CORS","Developer Portal","Problem Solve"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/","title":"Spring Boot 3.1 Docker Compose Support: The Most Intuitive and Consistent Way to Develop and Run Tests with Containers","text":"<p>Spring-Boot-Docker-Compose is a very powerful tool to help developer in their daily job. Help the Spring Boot app start with <code>docker-compose up</code> automatically. In Both development and running tests. This tiny automation actually brings huge benefits to both developer experiment and engineering practice</p> <p>In this article, we will cover:</p> <ul> <li>The challenges faced before adoption.</li> <li>How to implement it.</li> <li>Setup tips for optimal usage.</li> <li>A summary of the advantages.</li> </ul>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#as-is","title":"As-Is","text":"","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#in-development","title":"In Development","text":"<p>Developers often rely on Docker to create a consistent and reproducible environment when developing Spring Boot applications. When the number of dependency components grows, two key tools used in this process are Docker Compose and Makefile.</p> <p>However, in a multi-module project, each app might need different dependencies. It comes to a difficult situation. Every time we need to put different profiles with <code>docker-compose up</code>, or wrap them into different <code>make</code> commands. Both need extra effort to execute and remember (engineers are extremely lazy). Don't even mention the integration difficulty in unit tests.</p>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#in-testing","title":"In Testing","text":"<p>When a test needs to cover the dependency, it first comes up with the mock framework in a programmatic way. But it turns out to lose so many details because of the lack of real integration. such as</p> <ol> <li>The dialect and SQL execution difference between in-memory h2 DB and a real MySQL DB.</li> <li>Provide a mock Java object to make an API Client test lose the coverage of the JSON parsing process.</li> </ol> <p>so some of them start to use the TestContainer technic, to provide a real instance during unit tests. However, TestContainer suffers from some points too, like:</p> <ol> <li>need support library to gain more interaction.</li> <li>a programmatic way to set up, which is different from the development time (docker-compose way).</li> <li>extra setting to share containers in different tests</li> </ol>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#introducing-the-spring-boot-docker-compose","title":"Introducing the Spring-Boot-Docker-Compose","text":"<p>In conclusion, the way of providing dependency containers isn't a flawless practice for now. So Spring community provides a new way to seamlessly integrate with Spring Boot App and docker-compose called Spring-Boot-Docker-Compose, follow the starting guide could easily run the app with the dependency container together in one command.</p> <ul> <li>Adding the dependency to the build.gradle of the app module:</li> </ul> <pre><code>developmentOnly 'org.springframework.boot:spring-boot-docker-compose'\n</code></pre> <ul> <li>Provide a <code>compose.yaml</code> file in the root folder can do the trick. If the <code>compose.yaml</code> has a different name or in a different folder, <code>spring.docker.compose.file</code> can solve the problem, refer to Using a Specific Compose File.</li> </ul> <p></p> <p></p> <p>From now on, every project member can run the app without manually starting the dependency component container cluster.</p>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#little-more-setup-to-make-it-better","title":"Little More Setup to Make It Better","text":"<p>To enhance the development and testing experience, consider the following improvements.</p>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#for-development","title":"For Development","text":"","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#docker-profile-connected-to-spring-profile","title":"Docker Profile Connected to Spring Profile:","text":"<p>Since the practice of spring profiles is usually used as a dependency management method. For example, we add a profile <code>redis</code> in <code>spring.profiles.include</code>, to include <code>application-redis.yaml</code>, which contains the related config for using Redis.</p> <p>So we want to link docker compose profiles to Spring profiles. In this way, developers can ensure that the correct configuration is used and the corresponding container is up also.</p> <p>To do that, we need some modifications:</p> <ol> <li>use comma separate format for <code>spring.profiles.include</code></li> </ol> application.yaml<pre><code>spring:\n  application:\n    name: order-command-side\n  profiles:\n    include: \"observe, cluster, mock, kafka, kafka-producer, redis, idempotency, docker-support\"\n</code></pre> <ol> <li>add profile linking property 'spring.docker.compose.profiles.active'</li> </ol> application-docker-support.yaml<pre><code>spring:\n  docker:\n    compose:\n      enabled: true\n      profiles:\n        active: ${spring.profiles.include}\n</code></pre> <ol> <li>add profiles for each service in compose.yaml</li> </ol> compose.yaml<pre><code>version: \"3\"\n\nservices:\n  kafka:\n    image: 'bitnami/kafka:3.7'\n    profiles: [\"kafka\", \"all\"]\n    ports:\n      - '9092:9092'\n    environment:\n      - KAFKA_CFG_NODE_ID=0\n      - KAFKA_CFG_PROCESS_ROLES=controller,broker\n      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093\n      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\n      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://${KAFKA_HOST:-localhost}:9092\n      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093\n      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER\n\n  redis:\n    image: 'redis:7.2'\n    profiles: [\"redis\", \"all\"]\n    ports:\n      - '6379:6379'\n</code></pre>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#ignore-when-building-in-image","title":"Ignore when Building in Image:","text":"<p>JIB is now an out-of-the-box technic of building image for a Spring Boot Application. For now, JIB still includes the spring-boot-docker-compose library in the built image (we can track this issue for new solutions). However, in most cases, we have existed dependency components (e.g. DB service, Redis cluster...) in the Test, Staging, or Production environment. So we need to manually exclude it by setting up by jib-layer-filter-extension for gradle or maven.</p> <ol> <li>add buildscript at the start of the root <code>build.gradle</code> <pre><code>buildscript {\n    dependencies {\n        classpath('com.google.cloud.tools:jib-layer-filter-extension-gradle:0.3.0')\n    }\n}\n... other settings\n</code></pre></li> <li>add plugin usage in jib.gradle   <pre><code>jib {\n  // ... other settings\n  pluginExtensions {\n      pluginExtension {\n          implementation = 'com.google.cloud.tools.jib.gradle.extension.layerfilter.JibLayerFilterExtension'\n          configuration {\n              filters {\n                  filter {\n                      glob = '**/spring-boot-docker-compose-*.jar'\n                  }\n              }\n          }\n      }\n  }\n}\n</code></pre></li> </ol> <p>In this way, the image built by JIB will not include the <code>spring-boot-docker-compose</code> library and can run like a normal Spring Boot Application. </p>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#for-testing","title":"For Testing","text":"","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#activate-docker-in-tests","title":"Activate Docker in Tests:","text":"<p>In default, the <code>spring-boot-docker-compose</code> is disabled when running tests. So we need to activate it by ourselves via property: <code>spring.docker.compose.skip.in-tests</code>. Besides, the root folder when running a test is the module folder, so we need to provide the file path to the <code>root/compose.yaml</code>. In result, we will write a <code>test/resources/application.yaml</code> like:</p> <pre><code>spring:\n  docker:\n    compose:\n      enabled: true\n      file: ../../compose.yml  # my test class is in root/modules/client\n      skip:\n        in-tests: false\n      profiles:\n        active: ${spring.profiles.include}\n</code></pre> <p>Then, in our test class, just add the <code>@SpringBootTest</code>, so that the docker-compose will trigger by spring boot app.</p>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#shared-containers-across-modules","title":"Shared Containers Across Modules","text":"<p>If your application consists of multiple modules, sharing Docker containers across tests can significantly speed up the testing process. Instead of spinning up new containers for each test module, containers can be shared, reducing the startup time and resource usage.</p> <p>If we set the <code>spring.docker.compose.lifecycle-management</code> as <code>start-only</code> (reference to document), then the docker cluster won't stop after each test. In the second test, which needs to start the docker-compose, will find the docker cluster is already up and ignore that.</p> <p></p>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#summary-of-advantages","title":"Summary of Advantages","text":"<p>Using Spring-Boot-Docker-Compose in both development and testing environments provides several key benefits:</p>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#no-prior-knowledge-required","title":"No Prior Knowledge Required:","text":"<p>Developers don't need to know and execute <code>docker-compose</code> or <code>makefile</code> to run dependency components before running the Spring Boot app. The setup can be standardized and automatic, allowing even newcomers to the project to start the application quickly.</p>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#decoupling-testcontainer-java-library","title":"Decoupling TestContainer Java Library","text":"<p>By using this technic, we can be free from finding the supported TestContainer library for each component, when we need to do extra initialize setup, such as creating schema and data in Mysql-TestContainer, adding mock-rules for MockServer TestContainer, \u2026etc. We can use the same setting method as we used in developing.</p>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#test-case-consistency","title":"Test Case Consistency:","text":"<p>By using Spring-Boot-Docker-Compose for tests, we ensure that all test data is consistent between development and test. Reducing the effort to try to make extra test data in developing features.</p>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#improved-test-coverage","title":"Improved Test Coverage:","text":"<p>In this way, We can easily write some Integration-Test (In a style that the connection didn't go out of one machine, (Otherwise would be called an E2E-Test)) by <code>@SpringBootTest</code> (with images of DB, Kafka, Redis, Mock-Server, etc. ). So that the PR-check can provide more confidence than a normal unit-test.</p> <p>In conclusion, integrating Spring-Boot-Docker-Compose support into your Spring Boot development and testing offers an intuitive, consistent, and efficient process that can significantly enhance productivity and reliability. With just a few additional configurations, you can leverage the full power of Docker to create a development environment that's both easy to use and highly effective.</p>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/2024/09/01/spring-boot-31-docker-compose-support-the-most-intuitive-and-consistent-way-to-develop-and-run-tests-with-containers/#reference","title":"Reference","text":"<ul> <li>https://docs.spring.io/spring-boot/reference/features/dev-services.html#features.dev-services.docker-compose</li> <li>https://github.com/GoogleContainerTools/jib-extensions/issues/158</li> </ul>","tags":["Java - Development - Unit Test - Integration Test - Docker"]},{"location":"Software%20Engineering%20Blog/archive/2024/","title":"2024","text":""},{"location":"Software%20Engineering%20Blog/archive/2023/","title":"2023","text":""},{"location":"Software%20Engineering%20Blog/category/spring-boot/","title":"Spring Boot","text":""},{"location":"Software%20Engineering%20Blog/category/system-design/","title":"System Design","text":""},{"location":"Software%20Engineering%20Blog/category/devops/","title":"DevOps","text":""},{"location":"Software%20Engineering%20Blog/category/developer-experience/","title":"Developer Experience","text":""},{"location":"Software%20Engineering%20Blog/page/2/","title":"Software Engineering Blog","text":""},{"location":"Software%20Engineering%20Blog/page/3/","title":"Software Engineering Blog","text":""},{"location":"Software%20Engineering%20Blog/page/4/","title":"Software Engineering Blog","text":""},{"location":"Software%20Engineering%20Blog/archive/2024/page/2/","title":"2024","text":""},{"location":"Software%20Engineering%20Blog/archive/2023/page/2/","title":"2023","text":""},{"location":"Software%20Engineering%20Blog/category/spring-boot/page/2/","title":"Spring Boot","text":""},{"location":"Software%20Engineering%20Blog/tags/","title":"Tags","text":"<p>Following is a list of technical blog relevant tags:</p>"},{"location":"Software%20Engineering%20Blog/tags/#backstage","title":"Backstage","text":"<ul> <li>Centralize All Needed Knowledge in One Developer Portals Through Spotify Backstage</li> <li>Three Ways to Solve CORS Issue in the Embed Swagger Page in Backstage</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#cd","title":"CD","text":"<ul> <li>The First Step Toward True Continuous Delivery</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#cors","title":"CORS","text":"<ul> <li>Three Ways to Solve CORS Issue in the Embed Swagger Page in Backstage</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#data-consistency","title":"Data Consistency","text":"<ul> <li>How to design an efficient Idempotent API</li> <li>Spring Boot 3: build the efficiency Idempotent API by Redis</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#database","title":"Database","text":"<ul> <li>Mysql Trap when DB Read-Write Separate and Parent-Child Relationship Schema without Transaction</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#deployment-strategy","title":"Deployment Strategy","text":"<ul> <li>Easier, Flexible, and Lower Resource Cost Deployment Strategies by Feature Toggle</li> <li>Safely Deliver Large Scale Feature Migration With Feature Toggle</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#devops","title":"DevOps","text":"<ul> <li>Easier, Flexible, and Lower Resource Cost Deployment Strategies by Feature Toggle</li> <li>The First Step Toward True Continuous Delivery</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#developer-portal","title":"Developer Portal","text":"<ul> <li>Centralize All Needed Knowledge in One Developer Portals Through Spotify Backstage</li> <li>Three Ways to Solve CORS Issue in the Embed Swagger Page in Backstage</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#devops_1","title":"Devops","text":"<ul> <li>Design an Easy-to-Test, Flexible Application with Config Server, Toggle System, and Mock Server</li> <li>High Availability Deployment by Pod Topology Spread Constraints in K8s Cluster</li> <li>Safely Deliver Large Scale Feature Migration With Feature Toggle</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#distributed-systems","title":"Distributed Systems","text":"<ul> <li>3 Times Performance Improvement for Generative AI within a Kafka Pipeline System</li> <li>Get Kafka in prod-ready, 2 decisions to make and 3 implementation details</li> <li>How to design an efficient Idempotent API</li> <li>Mysql Trap when DB Read-Write Separate and Parent-Child Relationship Schema without Transaction</li> <li>Robust Kafka Consumer Error Handling on a Spring Boot 3 Application</li> <li>Spring Boot 3: build the efficiency Idempotent API by Redis</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#feature-toggle","title":"Feature Toggle","text":"<ul> <li>Easier, Flexible, and Lower Resource Cost Deployment Strategies by Feature Toggle</li> <li>Safely Deliver Large Scale Feature Migration With Feature Toggle</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#github-action","title":"GitHub Action","text":"<ul> <li>The First Step Toward True Continuous Delivery</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#gradle","title":"Gradle","text":"<ul> <li>5 Steps to Make Gradle Configuration Extreme Clean in a Multi-Module Project</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#high-availability","title":"High-Availability","text":"<ul> <li>High Availability Deployment by Pod Topology Spread Constraints in K8s Cluster</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#idempotent-api","title":"Idempotent API","text":"<ul> <li>How to design an efficient Idempotent API</li> <li>Spring Boot 3: build the efficiency Idempotent API by Redis</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#java","title":"Java","text":"<ul> <li>5 Steps to Make Gradle Configuration Extreme Clean in a Multi-Module Project</li> <li>Say Goodbye to meaningless code for Controller and Service with Spring-Data-Rest</li> <li>Spring Boot 3 Observability: monitor Application on the method level</li> <li>Spring Boot 3: build the efficiency Idempotent API by Redis</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#java-development-unit-test-integration-test-docker","title":"Java - Development - Unit Test - Integration Test - Docker","text":"<ul> <li>Spring Boot 3.1 Docker Compose Support: The Most Intuitive and Consistent Way to Develop and Run Tests with Containers</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#kafka","title":"Kafka","text":"<ul> <li>3 Times Performance Improvement for Generative AI within a Kafka Pipeline System</li> <li>Get Kafka in prod-ready, 2 decisions to make and 3 implementation details</li> <li>Robust Kafka Consumer Error Handling on a Spring Boot 3 Application</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#kubernetes","title":"Kubernetes","text":"<ul> <li>High Availability Deployment by Pod Topology Spread Constraints in K8s Cluster</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#observability","title":"Observability","text":"<ul> <li>Spring Boot 3 Observability: monitor Application on the method level</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#openfeign","title":"OpenFeign","text":"<ul> <li>Say Goodbye to meaningless code for Controller and Service with Spring-Data-Rest</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#problem-solve","title":"Problem Solve","text":"<ul> <li>Three Ways to Solve CORS Issue in the Embed Swagger Page in Backstage</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#read-write-separate","title":"Read-Write Separate","text":"<ul> <li>Mysql Trap when DB Read-Write Separate and Parent-Child Relationship Schema without Transaction</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#refactoring","title":"Refactoring","text":"<ul> <li>5 Steps to Make Gradle Configuration Extreme Clean in a Multi-Module Project</li> <li>Say Goodbye to meaningless code for Controller and Service with Spring-Data-Rest</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#system-design","title":"System Design","text":"<ul> <li>Design an Easy-to-Test, Flexible Application with Config Server, Toggle System, and Mock Server</li> <li>Safely Deliver Large Scale Feature Migration With Feature Toggle</li> </ul>"},{"location":"Software%20Engineering%20Blog/tags/#trunk-based","title":"Trunk-Based","text":"<ul> <li>Design an Easy-to-Test, Flexible Application with Config Server, Toggle System, and Mock Server</li> <li>Safely Deliver Large Scale Feature Migration With Feature Toggle</li> </ul>"}]}